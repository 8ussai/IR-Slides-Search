doc_id,page_number,text
IR_ch1,1,introduction to information retrieval introducing information retrieval and web search
IR_ch1,2,information retrieval information retrieval ir is finding material usually documents of an unstructured nature usually text that satisfies an information need from within large collections usually stored on computers these days we frequently think first of web search but there are many other cases e mail search searching your laptop corporate knowledge bases legal information retrieval
IR_ch1,3,unstructured text vs structured database data in the mid nineties
IR_ch1,4,unstructured text vs structured database data today
IR_ch1,5,sec basic assumptions of information retrieval collection a set of documents assume it is a static collection for the moment goal retrieve documents with information that is relevant to the user s information need and helps the user complete a task
IR_ch1,6,the classic search model get rid of mice in a user task politically correct way misconception info about removing mice info need without killing them misformulation query searc how trap mice alive h search engine query results collection refinement
IR_ch1,7,sec how good are the retrieved docs precision fraction of retrieved docs that are relevant to the user s information need recall fraction of relevant docs in collection that are retrieved more precise definitions and measurements to follow later
IR_ch1,8,introduction to information retrieval term document incidence matrices
IR_ch1,9,sec unstructured data in which plays of shakespeare contain the words brutus and caesar but not calpurnia one could grep all of shakespeare s plays for brutus and caesar then strip out lines containing calpurnia why is that not the answer slow for large corpora not calpurnia is non trivial other operations e g find the word romans near countrymen not feasible ranked retrieval best documents to return later lectures
IR_ch1,10,sec term document incidence matrices antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser if play contains brutus and caesar but not word otherwise calpurnia
IR_ch1,11,sec incidence vectors so we have a vector for each term to answer query take the vectors for brutus caesar and calpurnia complemented bitwise and and and antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch1,12,sec answers to query antony and cleopatra act iii scene ii agrippa aside to domitius enobarbus why enobarbus when antony found julius caesar dead he cried almost to roaring and he wept when at philippi he found brutus slain hamlet act iii scene ii lord polonius i did enact julius caesar i was killed i the capitol brutus killed me
IR_ch1,13,sec bigger collections consider n million documents each with about words avg bytes word including spaces punctuation gb of data in the documents say there are m k distinct terms among these
IR_ch1,14,sec can t build the matrix k x m matrix has half a trillion s and s but it has no more than one billion s why matrix is extremely sparse what s a better representation we only record the positions
IR_ch1,15,introduction to information retrieval the inverted index the key data structure underlying modern ir
IR_ch1,16,sec inverted index for each term t we must store a list of all documents that contain t identify each doc by a docid a document serial number can we used fixed size arrays for this brutus caesar calpurnia what happens if the word caesar is added to document
IR_ch1,17,sec inverted index we need variable size postings lists on disk a continuous run of postings is normal and best posting in memory can use linked lists or variable length arrays brutus some tradeoffs in size ease of insertion caesar calpurnia postings dictionary sorted by docid more later on why
IR_ch1,18,sec inverted index construction documents to friends romans countrymen be indexed tokenizer token stream friends romans countrymen linguistic modules friend roman countryman modified tokens indexer friend roman inverted index countryman
IR_ch1,19,initial stages of text processing tokenization cut character sequence into word tokens deal with john s a state of the art solution normalization map text and query term to same form you want u s a and usa to match stemming we may wish different forms of a root to match authorize authorization stop words we may omit very common words or not the a to of
IR_ch1,20,sec indexer steps token sequence sequence of modified token document id pairs doc doc i did enact julius so let it be with caesar i was killed caesar the noble i the capitol brutus hath told you brutus killed me caesar was ambitious
IR_ch1,21,sec indexer steps sort sort by terms and then docid core indexing step
IR_ch1,22,sec indexer steps dictionary postings multiple term entries in a single document are merged split into dictionary and postings doc frequency information is added why frequency will discuss later
IR_ch1,23,sec where do we pay in storage lists of docids terms and counts ir system implementation how do we index efficiently how much storage do we need pointers
IR_ch1,24,introduction to information retrieval query processing with an inverted index
IR_ch1,25,sec the index we just built how do we process a query our focus later what kinds of queries can we process
IR_ch1,26,sec query processing and consider processing the query brutus and caesar locate brutus in the dictionary retrieve its postings locate caesar in the dictionary retrieve its postings merge the two postings intersect the document sets brutus caesar
IR_ch1,27,sec the merge walk through the two postings simultaneously in time linear in the total number of postings entries brutus caesar if the list lengths are x and y the merge takes o x y operations crucial postings sorted by docid
IR_ch1,28,intersecting two postings lists a merge algorithm
IR_ch1,29,introduction to information retrieval the boolean retrieval model extended boolean models
IR_ch1,30,sec boolean queries exact match the boolean retrieval model is being able to ask a query that is a boolean expression boolean queries are queries using and or and not to join query terms views each document as a set of words is precise document matches condition or not perhaps the simplest model to build an ir system on primary commercial retrieval tool for decades many search systems you still use are boolean email library catalog mac os x spotlight
IR_ch1,31,sec example westlaw http www westlaw com largest commercial paying subscribers legal search service started ranking added new federated search added tens of terabytes of data users majority of users still use boolean queries example query what is the statute of limitations in cases involving the federal tort claims act limit statute action s federal tort claim within words s in same sentence
IR_ch1,32,sec example westlaw http www westlaw com another example query requirements for disabled people to be able to access a workplace disabl p access s work site work place employment place note that space is disjunction not conjunction long precise queries proximity operators incrementally developed not like web search many professional searchers still like boolean search you know exactly what you are getting but that doesn t mean it actually works better
IR_ch1,33,sec boolean queries more general merges exercise adapt the merge for the queries brutus and not caesar brutus or not caesar can we still run through the merge in time o x y what can we achieve
IR_ch1,34,sec merging what about an arbitrary boolean formula brutus or caesar and not antony or cleopatra can we always merge in linear time linear in what can we do better
IR_ch1,35,sec query optimization what is the best order for query processing consider a query that is an and of n terms for each of the n terms get its postings brututshen and them tog ethe r caesar calpurnia query brutus and calpurnia and caesar
IR_ch1,36,sec query optimization example process in order of increasing freq start with smallest set then keep cutting further this is why we kept document freq in dictionary brutus caesar calpurnia execute the query as calpurnia and brutus and caesar
IR_ch1,37,sec more general optimization e g madding or crowd and ignoble or strife get doc freq s for all terms estimate the size of each or by the sum of its doc freq s conservative process in increasing order of or sizes
IR_ch1,38,exercise recommend a query processing order for which two terms should we process first t e k m s t t e a r y a k r a n e e l i m s e r e g e i m s s d e o a r i s l a n c d e o e p e f r e q tangerine or trees and marmalade or skies and kaleidoscope or eyes
IR_ch1,39,query processing exercises exercise if the query is friends and romans and not countrymen how could we use the freq of countrymen exercise extend the merge to an arbitrary boolean query can we always guarantee execution in time linear in the total postings size hint begin with the case of a boolean formula query in this each query term appears only once in the query
IR_ch1,40,exercise try the search feature at http www rhymezone com shakespeare write down five search features you think it could do better
IR_ch1,41,introduction to information retrieval phrase queries and positional indexes
IR_ch1,42,sec phrase queries we want to be able to answer queries such as stanford university as a phrase thus the sentence i went to university at stanford is not a match the concept of phrase queries has proven easily understood by users one of the few advanced search ideas that works many more queries are implicit phrase queries for this it no longer suffices to store only term docs entries
IR_ch1,43,sec a first attempt biword indexes index every consecutive pair of terms in the text as a phrase for example the text friends romans countrymen would generate the biwords friends romans romans countrymen each of these biwords is now a dictionary term two word phrase query processing is now immediate
IR_ch1,44,sec longer phrase queries longer phrases can be processed by breaking them down stanford university palo alto can be broken into the boolean query on biwords stanford university and university palo and palo alto without the docs we cannot verify that the docs matching the above boolean query do contain the phrase can have false positives
IR_ch1,45,sec issues for biword indexes false positives as noted before index blowup due to bigger dictionary infeasible for more than biwords big even for them biword indexes are not the standard solution for all biwords but can be part of a compound strategy
IR_ch1,46,sec solution positional indexes in the postings store for each term the position s in which tokens of it appear term number of docs containing term doc position position doc position position etc
IR_ch1,47,sec positional index example be which of docs could contain to be or not to be for phrase queries we use a merge algorithm recursively at the document level but we now need to deal with more than just equality
IR_ch1,48,sec processing a phrase query extract inverted index entries for each distinct term to be or not merge their doc position lists to enumerate all positions with to be or not to be to be same general method for proximity searches
IR_ch1,49,sec proximity queries limit statute federal tort again here k means within k words of clearly positional indexes can be used for such queries biword indexes cannot exercise adapt the linear merge of postings to handle proximity queries can you make it work for any value of k this is a little tricky to do correctly and efficiently see figure of iir
IR_ch1,50,sec positional index size a positional index expands postings storage substantially even though indices can be compressed nevertheless a positional index is now standardly used because of the power and usefulness of phrase and proximity queries whether used explicitly or implicitly in a ranking retrieval system
IR_ch1,51,sec positional index size need an entry for each occurrence not just once per document index size depends on average document sizewhy average web page has terms sec filings books even some epic poems easily terms consider a term with frequency document size postings positional postings
IR_ch1,52,sec rules of thumb a positional index is as large as a non positional index positional index size of volume of original text caveat all of this holds for english like languages
IR_ch1,53,sec combination schemes these two approaches can be profitably combined for particular phrases michael jackson britney spears it is inefficient to keep on merging positional postings lists even more so for phrases like the who williams et al evaluate a more sophisticated mixed indexing scheme a typical web query mixture was executed in ¼ of the time of using just a positional index it required more space than having a positional index alone
IR_ch1,54,introduction to information retrieval structured vs unstructured data
IR_ch1,55,ir vs databases structured vs unstructured data structured data tends to refer to information in tables employee manager salary smith jones chang smith ivy smith typically allows numerical range and exact match for text queries e g salary and manager smith
IR_ch1,56,unstructured data typically refers to free text allows keyword queries including operators more sophisticated concept queries e g find all web pages dealing with drug abuse classic model for searching text documents
IR_ch1,57,semi structured data in fact almost no data is unstructured e g this slide has distinctly identified zones such as the title and bullets to say nothing of linguistic structure facilitates semi structured search such as title contains data and bullets contain search or even title is about object oriented programming and author something like stro rup where is the wild card operator
IR_ch2,1,introduction to information retrieval introduction to information retrieval introducing information retrieval and web search
IR_ch2,2,introduction to information retrieval information retrieval information retrieval ir is finding material usually documents of an unstructured nature usually text that satisfies an information need from within large collections usually stored on computers these days we frequently think first of web search but there are many other cases e mail search searching your laptop corporate knowledge bases legal information retrieval
IR_ch2,3,introduction to information retrieval unstructured text vs structured database data in the mid nineties unstructured structured data volume market cap
IR_ch2,4,introduction to information retrieval unstructured text vs structured database data today unstructured structured data volume market cap
IR_ch2,5,introduction to information retrieval sec basic assumptions of information retrieval collection a set of documents assume it is a static collection for the moment goal retrieve documents with information that is relevant to the user s information need and helps the user complete a task
IR_ch2,6,introduction to information retrieval the classic search model get rid of mice in a user task politically correct way misconception info about removing mice info need without killing them misformulation query how trap mice alive search search engine query results collection refinement
IR_ch2,7,introduction to information retrieval sec how good are the retrieved docs precision fraction of retrieved docs that are relevant to the user s information need recall fraction of relevant docs in collection that are retrieved more precise definitions and measurements to follow later
IR_ch2,8,introduction to information retrieval introduction to information retrieval term document incidence matrices
IR_ch2,9,introduction to information retrieval sec unstructured data in which plays of shakespeare contain the words brutus and caesar but not calpurnia one could grep all of shakespeare s plays for brutus and caesar then strip out lines containing calpurnia why is that not the answer slow for large corpora not calpurnia is non trivial other operations e g find the word romans near countrymen not feasible ranked retrieval best documents to return later lectures
IR_ch2,10,introduction to information retrieval sec term document incidence matrices antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser if play contains brutus and caesar but not word otherwise calpurnia
IR_ch2,11,introduction to information retrieval sec incidence vectors so we have a vector for each term to answer query take the vectors for brutus caesar and calpurnia complemented bitwise and and and antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch2,12,introduction to information retrieval sec answers to query antony and cleopatra act iii scene ii agrippa aside to domitius enobarbus why enobarbus when antony found julius caesar dead he cried almost to roaring and he wept when at philippi he found brutus slain hamlet act iii scene ii lord polonius i did enact julius caesar i was killed i the capitol brutus killed me
IR_ch2,13,introduction to information retrieval sec bigger collections consider n million documents each with about words avg bytes word including spaces punctuation gb of data in the documents say there are m k distinct terms among these
IR_ch2,14,introduction to information retrieval sec can t build the matrix k x m matrix has half a trillion s and s but it has no more than one billion s why matrix is extremely sparse what s a better representation we only record the positions
IR_ch2,15,introduction to information retrieval introduction to information retrieval the inverted index the key data structure underlying modern ir
IR_ch2,16,introduction to information retrieval sec inverted index for each term t we must store a list of all documents that contain t identify each doc by a docid a document serial number can we used fixed size arrays for this brutus caesar calpurnia what happens if the word caesar is added to document
IR_ch2,17,introduction to information retrieval sec inverted index we need variable size postings lists on disk a continuous run of postings is normal and best in memory can use linked lists or variable length arrays some tradeoffs in size ease of insertion posting brutus caesar calpurnia postings dictionary sorted by docid more later on why
IR_ch2,18,introduction to information retrieval sec inverted index construction documents to friends romans countrymen be indexed tokenizer token stream friends romans countrymen linguistic modules friend roman countryman modified tokens indexer friend roman inverted index countryman
IR_ch2,19,introduction to information retrieval initial stages of text processing tokenization cut character sequence into word tokens deal with john s a state of the art solution normalization map text and query term to same form you want u s a and usa to match stemming we may wish different forms of a root to match authorize authorization stop words we may omit very common words or not the a to of
IR_ch2,20,introduction to information retrieval sec indexer steps token sequence sequence of modified token document id pairs doc doc i did enact julius so let it be with caesar i was killed caesar the noble i the capitol brutus hath told you brutus killed me caesar was ambitious
IR_ch2,21,introduction to information retrieval sec indexer steps sort sort by terms at least conceptually and then docid core indexing step
IR_ch2,22,introduction to information retrieval sec indexer steps dictionary postings multiple term entries in a single document are merged split into dictionary and postings doc frequency information is added why frequency will discuss later
IR_ch2,23,introduction to information retrieval sec where do we pay in storage lists of docids terms and counts ir system implementation how do we index efficiently how much storage do we need pointers
IR_ch2,24,introduction to information retrieval introduction to information retrieval query processing with an inverted index
IR_ch2,25,introduction to information retrieval sec the index we just built how do we process a query our focus later what kinds of queries can we process
IR_ch2,26,introduction to information retrieval sec query processing and consider processing the query brutus and caesar locate brutus in the dictionary retrieve its postings locate caesar in the dictionary retrieve its postings merge the two postings intersect the document sets brutus caesar
IR_ch2,27,introduction to information retrieval sec the merge walk through the two postings simultaneously in time linear in the total number of postings entries brutus caesar if the list lengths are x and y the merge takes o x y operations crucial postings sorted by docid
IR_ch2,28,introduction to information retrieval intersecting two postings lists a merge algorithm
IR_ch2,29,introduction to information retrieval introduction to information retrieval the boolean retrieval model extended boolean models
IR_ch2,30,introduction to information retrieval sec boolean queries exact match the boolean retrieval model is being able to ask a query that is a boolean expression boolean queries are queries using and or and not to join query terms views each document as a set of words is precise document matches condition or not perhaps the simplest model to build an ir system on primary commercial retrieval tool for decades many search systems you still use are boolean email library catalog macos spotlight
IR_ch2,31,introduction to information retrieval sec example westlaw http www westlaw com largest commercial paying subscribers legal search service started ranking added new federated search added tens of terabytes of data users majority of users still use boolean queries example query what is the statute of limitations in cases involving the federal tort claims act limit statute action s federal tort claim within words s in same sentence
IR_ch2,32,introduction to information retrieval sec example westlaw http www westlaw com another example query requirements for disabled people to be able to access a workplace disabl p access s work site work place employment place note that space is disjunction not conjunction long precise queries proximity operators incrementally developed not like web search many professional searchers still like boolean search you know exactly what you are getting but that doesn t mean it actually works better
IR_ch2,33,introduction to information retrieval sec boolean queries more general merges exercise adapt the merge for the queries brutus and not caesar brutus or not caesar can we still run through the merge in time o x y what can we achieve
IR_ch2,34,introduction to information retrieval sec merging what about an arbitrary boolean formula brutus or caesar and not antony or cleopatra can we always merge in linear time linear in what can we do better
IR_ch2,35,introduction to information retrieval sec query optimization what is the best order for query processing consider a query that is an and of n terms for each of the n terms get its postings then and them together brutus caesar calpurnia query brutus and calpurnia and caesar
IR_ch2,36,introduction to information retrieval sec query optimization example process in order of increasing freq start with smallest set then keep cutting further this is why we kept document freq in dictionary brutus caesar calpurnia execute the query as calpurnia and brutus and caesar
IR_ch2,37,introduction to information retrieval exercise recommend a query processing order for which two terms should we process first t e k m s t t e a r y a k r a n e e l i m s e r e g e i m s s d e o a r i s l a n c d e o e p e f r e q tangerine or trees and marmalade or skies and kaleidoscope or eyes
IR_ch2,38,introduction to information retrieval sec more general optimization e g madding or crowd and ignoble or strife get doc freq s for all terms estimate the size of each or by the sum of its doc freq s conservative process in increasing order of or sizes
IR_ch2,39,introduction to information retrieval query processing exercises exercise if the query is friends and romans and not countrymen how could we use the freq of countrymen exercise extend the merge to an arbitrary boolean query can we always guarantee execution in time linear in the total postings size hint begin with the case of a boolean formula query in this each query term appears only once in the query
IR_ch2,40,introduction to information retrieval exercise try the search feature at http www rhymezone com shakespeare write down five search features you think it could do better
IR_ch2,41,introduction to information retrieval introduction to information retrieval phrase queries and positional indexes
IR_ch2,42,introduction to information retrieval sec phrase queries we want to be able to answer queries such as stanford university as a phrase thus the sentence i went to university at stanford is not a match the concept of phrase queries has proven easily understood by users one of the few advanced search ideas that works many more queries are implicit phrase queries for this it no longer suffices to store only term docs entries
IR_ch2,43,introduction to information retrieval sec a first attempt biword indexes index every consecutive pair of terms in the text as a phrase for example the text friends romans countrymen would generate the biwords friends romans romans countrymen each of these biwords is now a dictionary term two word phrase query processing is now immediate
IR_ch2,44,introduction to information retrieval sec longer phrase queries longer phrases can be processed by breaking them down stanford university palo alto can be broken into the boolean query on biwords stanford university and university palo and palo alto without the docs we cannot verify that the docs matching the above boolean query do contain the phrase can have false positives
IR_ch2,45,introduction to information retrieval sec issues for biword indexes false positives as noted before index blowup due to bigger dictionary infeasible for more than biwords big even for them biword indexes are not the standard solution for all biwords but can be part of a compound strategy
IR_ch2,46,introduction to information retrieval sec solution positional indexes in the postings store for each term the position s in which tokens of it appear term number of docs containing term doc position position doc position position etc
IR_ch2,47,introduction to information retrieval sec positional index example be which of docs could contain to be or not to be for phrase queries we use a merge algorithm recursively at the document level but we now need to deal with more than just equality
IR_ch2,48,introduction to information retrieval sec processing a phrase query extract inverted index entries for each distinct term to be or not merge their doc position lists to enumerate all positions with to be or not to be to be same general method for proximity searches
IR_ch2,49,introduction to information retrieval sec proximity queries limit statute federal tort again here k means within k words of clearly positional indexes can be used for such queries biword indexes cannot exercise adapt the linear merge of postings to handle proximity queries can you make it work for any value of k this is a little tricky to do correctly and efficiently see figure of iir
IR_ch2,50,introduction to information retrieval sec positional index size a positional index expands postings storage substantially even though indices can be compressed nevertheless a positional index is now standardly used because of the power and usefulness of phrase and proximity queries whether used explicitly or implicitly in a ranking retrieval system
IR_ch2,51,introduction to information retrieval sec positional index size need an entry for each occurrence not just once per document index size depends on average document size why average web page has terms sec filings books even some epic poems easily terms consider a term with frequency document size postings positional postings
IR_ch2,52,introduction to information retrieval sec rules of thumb a positional index is as large as a non positional index positional index size of volume of original text caveat all of this holds for english like languages
IR_ch2,53,introduction to information retrieval sec combination schemes these two approaches can be profitably combined for particular phrases michael jackson britney spears it is inefficient to keep on merging positional postings lists even more so for phrases like the who williams et al evaluate a more sophisticated mixed indexing scheme a typical web query mixture was executed in ¼ of the time of using just a positional index it required more space than having a positional index alone
IR_ch3,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search basic inverted index construction
IR_ch3,2,introduction to information retrieval ch index construction how do we construct an index what strategies can we use with limited main memory
IR_ch3,3,introduction to information retrieval sec recall index construction term doc i did documents are parsed to extract words and these enact julius are saved with the document id caesar i was killed i the capitol brutus killed me doc doc so let it be i did enact julius with so let it be with caesar the caesar i was killed caesar the noble noble brutus i the capitol brutus hath told you hath told brutus killed me you caesar was ambitious caesar was ambitious
IR_ch3,4,introduction to information retrieval t e r m id id e n a c t ju liu s c a e s a r iw a s k ille d i t h e c a p it o l b r u t u s k ille d m e s o le t itb e w it h c a e s a r t h e n o b le b r u t u s h a t h t o ld y o u c a e s a r w a s a m b it io u s d o c t e r m a m b it io b e b r u t u s b r u t u s c a p it o l c a e s a r c a e s a r c a e s a r d id e n a c t h a t h ii i itju liu s k ille d k ille d le t m e n o b le s o t h e t h e t o ld y o u w a s w a s w it h u s d o c sec key step after all documents have been parsed the inverted file is sorted by terms we focus on this sort step
IR_ch3,5,introduction to information retrieval sec rcv our collection for this lecture as an example for applying scalable index construction algorithms we will use the reuters rcv collection this is one year of reuters newswire part of and the collection isn t really large enough but it s publicly available and is a plausible example
IR_ch3,6,introduction to information retrieval sec a reuters rcv document
IR_ch3,7,introduction to information retrieval sec reuters rcv statistics symbol statistic value n documents l avg tokens per doc m terms word types avg bytes per token incl spaces punct avg bytes per token without spaces punct avg bytes per term non positional postings bytes per word token vs bytes per word type why
IR_ch3,8,introduction to information retrieval sec sort based index construction as we build the index we parse docs one at a time the final postings for any term are incomplete until the end at bytes per termid docid demands a lot of space for large collections t in the case of rcv so we can do this in memory today but typical collections are much larger e g the new york times provides an index of years of newswire thus we need to store intermediate results on disk
IR_ch3,9,introduction to information retrieval sec scaling index construction in memory index construction does not scale can t stuff entire collection into memory sort then write back how can we construct an index for very large collections taking into account hardware constraints memory disk speed etc let s review some hardware basics
IR_ch3,10,introduction to information retrieval sec hardware basics servers used in ir systems now typically have several gb of main memory sometimes tens of gb available disk space is several orders of magnitude larger fault tolerance is very expensive it s much cheaper to use many regular machines rather than one fault tolerant machine
IR_ch3,11,introduction to information retrieval sec hardware basics access to data in memory is much faster than access to data on disk disk seeks no data is transferred from disk while the disk head is being positioned therefore transferring one large chunk of data from disk to memory is faster than transferring many small chunks disk i o is block based reading and writing of entire blocks as opposed to smaller chunks block sizes kb to kb
IR_ch3,12,introduction to information retrieval sec hardware assumptions circa symbol statistic value s average seek time ms x s b transfer time per byte μs x s processor s clock rate s p low level operation μs s e g compare swap a word size of main memory several gb size of disk space tb or more
IR_ch3,13,introduction to information retrieval sec sort using disk as memory can we use the same index construction algorithm for larger collections but by using disk instead of memory no sorting t records on disk is too slow too many disk seeks we need an external sorting algorithm
IR_ch3,14,introduction to information retrieval introduction to information retrieval cs information retrieval and web search external memory indexing
IR_ch3,15,introduction to information retrieval sec bsbi blocked sort based indexing sorting with fewer disk seeks byte records termid docid these are generated as we parse docs must now sort m such byte records by termid define a block m such records can easily fit a couple into memory will have such blocks to start with basic idea of algorithm accumulate postings for each block sort write to disk then merge the blocks into one long sorted order
IR_ch3,16,introduction to information retrieval sec
IR_ch3,17,introduction to information retrieval sec sorting blocks of m records first read each block and sort within quicksort takes o n ln n expected steps in our case n m times this estimate gives us sorted runs of m records each done straightforwardly need copies of data on disk but can optimize this
IR_ch3,18,introduction to information retrieval sec how to merge the sorted runs can do binary merges with a merge tree of log layers during each layer read into memory runs in blocks of m merge write back brutus d d d d brutus d d brutus d d caesar d d d d d caesar d d d caesar d d julius d noble d julius d killed d with d d d d killed d noble d with d d d d postings lists merged to be merged postings list disk
IR_ch3,19,introduction to information retrieval sec how to merge the sorted runs but it is more efficient to do a multi way merge where you are reading from all blocks simultaneously open all block files simultaneously and maintain a read buffer for each one and a write buffer for the output file in each iteration pick the lowest termid that hasn t been processed using a priority queue merge all postings lists for that termid and write it out providing you read decent sized chunks of each block into memory and then write out a decent sized output chunk then you re not killed by disk seeks
IR_ch3,20,introduction to information retrieval sec remaining problem with sort based algorithm our assumption was we can keep the dictionary in memory we need the dictionary which grows dynamically in order to implement a term to termid mapping
IR_ch3,21,introduction to information retrieval sec spimi single pass in memory indexing key idea generate separate dictionaries for each block no need to maintain term termid mapping across blocks key idea don t sort accumulate postings in postings lists as they occur with these two ideas we can generate a complete inverted index for each block these separate indexes can then be merged into one big index
IR_ch3,22,introduction to information retrieval sec spimi invert merging of blocks is analogous to bsbi
IR_ch3,23,introduction to information retrieval spimi in action sorted input token dictionary dictionary brutus d d caesar d brutus d d with d caesar d d d with d d d d brutus d noble d noble d caesar d with d d d d with d caesar d d d brutus d with d caesar d noble d with d
IR_ch3,24,introduction to information retrieval sec spimi compression compression makes spimi even more efficient compression of terms compression of postings more on this later original publication on spimi heinz and zobel
IR_ch3,25,introduction to information retrieval introduction to information retrieval cs information retrieval and web search distributed indexing
IR_ch3,26,introduction to information retrieval sec distributed indexing for web scale indexing don t try this at home must use a distributed computing cluster individual machines are fault prone can unpredictably slow down or fail how do we exploit such a pool of machines
IR_ch3,27,introduction to information retrieval sec web search engine data centers web search data centers google bing baidu mainly contain commodity machines data centers are distributed around the world estimate google million servers million processors cores gartner
IR_ch3,28,introduction to information retrieval sec massive data centers if in a non fault tolerant system with nodes each node has uptime what is the uptime of the entire system answer meaning of the time one or more servers is down exercise calculate the number of servers failing per minute for an installation of million servers
IR_ch3,29,introduction to information retrieval sec distributed indexing maintain a master machine directing the indexing job considered safe break up indexing into sets of parallel tasks master machine assigns each task to an idle machine from a pool
IR_ch3,30,introduction to information retrieval sec parallel tasks we will use two sets of parallel tasks parsers inverters break the input document collection into splits each split is a subset of documents corresponding to blocks in bsbi spimi
IR_ch3,31,introduction to information retrieval sec data flow master assign assign postings parser a f g p q z inverter a f parser a f g p q z inverter g p splits inverter q z parser a f g p q z map reduce segment files phase phase
IR_ch3,32,introduction to information retrieval sec parsers master assigns a split to an idle parser machine parser reads a document at a time and emits term doc pairs parser writes pairs into j partitions example each partition is for a range of terms first letters e g a f g p q z here j now to complete the index inversion
IR_ch3,33,introduction to information retrieval sec inverters an inverter collects all term doc pairs postings for one term partition sorts and writes to postings lists
IR_ch3,34,introduction to information retrieval example for index construction map caesar conquered d c came c c ed d c died c d came d c d c ed d c d died d reduce c d d d died d came d c ed d c d d died d came d c ed d
IR_ch3,35,introduction to information retrieval sec index construction index construction was just one phase another phase transforming a term partitioned index into a document partitioned index term partitioned one machine handles a subrange of terms document partitioned one machine handles a subrange of documents as we ll discuss in the web part of the course most search engines use a document partitioned index better load balancing etc
IR_ch3,36,introduction to information retrieval sec mapreduce the index construction algorithm we just described is an instance of mapreduce mapreduce dean and ghemawat is a robust and conceptually simple framework for distributed computing without having to write code for the distribution part they describe the google indexing system ca as consisting of a number of phases each implemented in mapreduce
IR_ch3,37,introduction to information retrieval sec schema for index construction in mapreduce schema of map and reduce functions map input list k v reduce k list v output instantiation of the schema for index construction map collection list termid docid reduce termid list docid termid list docid postings list postings list
IR_ch3,38,introduction to information retrieval introduction to information retrieval cs information retrieval and web search dynamic indexing
IR_ch3,39,introduction to information retrieval sec dynamic indexing up to now we have assumed that collections are static they rarely are documents come in over time and need to be inserted documents are deleted and modified this means that the dictionary and postings lists have to be modified postings updates for terms already in dictionary new terms added to dictionary
IR_ch3,40,introduction to information retrieval sec simplest approach maintain big main index new docs go into small auxiliary index search across both merge results deletions invalidation bit vector for deleted docs filter docs output on a search result by this invalidation bit vector periodically re index into one main index
IR_ch3,41,introduction to information retrieval sec issues with main and auxiliary indexes problem of frequent merges you touch stuff a lot poor performance during merge actually merging of the auxiliary index into the main index is efficient if we keep a separate file for each postings list merge is the same as a simple append but then we would need a lot of files inefficient for os assumption for the rest of the lecture the index is one big file in reality use a scheme somewhere in between e g split very large postings lists collect postings lists of length in one file etc
IR_ch3,42,introduction to information retrieval sec logarithmic merge maintain a series of indexes each twice as large as the previous one at any time some of these powers of are instantiated keep smallest z in memory larger ones i i on disk if z gets too big n write to disk as i or merge with i if i already exists as z either write merge z to disk as i if no i or merge with i to form z
IR_ch3,43,introduction to information retrieval logarithmic merge in action n z z z z n i i i i n i i i i n i n n
IR_ch3,44,introduction to information retrieval sec
IR_ch3,45,introduction to information retrieval sec logarithmic merge auxiliary and main index t n merges where t is of postings and n is size of auxiliary index construction time is o t n as in the worst case a posting is touched t n times logarithmic merge each posting is merged at most o log t n times so complexity is o t log t n so logarithmic merge is much more efficient for index construction but query processing now requires the merging of o log t n indexes whereas it is o if you just have a main and auxiliary index
IR_ch3,46,introduction to information retrieval sec further issues with multiple indexes collection wide statistics are hard to maintain e g when we speak of spell correction which of several corrected alternatives do we present to the user we may want to pick the one with the most hits how do we maintain the top ones with multiple indexes and invalidation bit vectors one possibility ignore everything but the main index for such ordering will see more such statistics used in results ranking
IR_ch3,47,introduction to information retrieval sec dynamic indexing at search engines all the large search engines now do dynamic indexing their indices have frequent incremental changes news items blogs new topical web pages but sometimes typically they also periodically reconstruct the index from scratch query processing is then switched to the new index and the old index is deleted
IR_ch3,48,introduction to information retrieval earlybird real time search at twitter requirements for real time search low latency high throughput query evaluation high ingestion rate and immediate data availability concurrent reads and writes of the index dominance of temporal signal
IR_ch3,49,introduction to information retrieval earlybird index organization earlybird consists of multiple index segments each segment is relatively small holding up to tweets each posting in a segment is a bit word bits for the tweet id and bits for the position in the tweet only one segment can be written to at any given time small enough to be in memory new postings are simply appended to the postings list but the postings list is traversed backwards to prioritize newer tweets the remaining segments are optimized for read only postings sorted in reverse chronological order newest first
IR_ch3,50,introduction to information retrieval sec other sorts of indexes positional indexes same sort of sorting problem just larger why building character n gram indexes as text is parsed enumerate n grams for each n gram need pointers to all dictionary terms containing it the postings
IR_ch3,51,introduction to information retrieval ch resources for today s lecture chapter of iir mg chapter original publication on mapreduce dean and ghemawat original publication on spimi heinz and zobel earlybird busch et al icde
IR_ch4,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search christopher manning and pandu nayak lecture index compression
IR_ch4,2,introduction to information retrieval last lecture index construction sort based indexing naïve in memory inversion blocked sort based indexing bsbi merge sort is effective for hard disk based sorting avoid seeks single pass in memory indexing spimi no global dictionary generate separate dictionary for each block don t sort postings accumulate postings in postings lists as they occur distributed indexing using mapreduce dynamic indexing multiple indices logarithmic merge
IR_ch4,3,introduction to information retrieval ch today collection statistics in more detail with rcv how big will the dictionary and postings be dictionary compression postings compression
IR_ch4,4,introduction to information retrieval ch why compression in general use less disk space save a little money give users more space keep more stuff in memory increases speed increase speed of data transfer from disk to memory read compressed data decompress is faster than read uncompressed data premise decompression algorithms are fast true of the decompression algorithms we use
IR_ch4,5,introduction to information retrieval ch why compression for inverted indexes dictionary make it small enough to keep in main memory make it so small that you can keep some postings lists in main memory too postings file s reduce disk space needed decrease time needed to read postings lists from disk large search engines keep a significant part of the postings in memory compression lets you keep more in memory we will devise various ir specific compression schemes
IR_ch4,6,introduction to information retrieval sec recall reuters rcv symbol statistic value n documents l avg tokens per doc m terms word types avg bytes per token incl spaces punct avg bytes per token without spaces punct avg bytes per term non positional postings
IR_ch4,7,introduction to information retrieval sec index parameters vs what we index details iir table p size of word types terms non positional positional postings postings dictionary non positional index positional index size cumul size k cumul size k cumul k unfiltered no numbers case folding stopwords stopwords stemming exercise give intuitions for all the entries why do some zero entries correspond to big deltas in other columns
IR_ch4,8,introduction to information retrieval sec lossless vs lossy compression lossless compression all information is preserved what we mostly do in ir lossy compression discard some information several of the preprocessing steps can be viewed as lossy compression case folding stop words stemming number elimination chapter prune postings entries that are unlikely to turn up in the top k list for any query almost no loss of quality in top k list
IR_ch4,9,introduction to information retrieval sec vocabulary size vs collection size how big is the term vocabulary that is how many distinct words are there can we assume an upper bound not really at least different words of length in practice the vocabulary will keep growing with the collection size especially with unicode
IR_ch4,10,introduction to information retrieval sec vocabulary size vs collection size heaps law m ktb m is the size of the vocabulary t is the number of tokens in the collection typical values k and b in a log log plot of vocabulary size m vs t heaps law predicts a line with slope about ½ it is the simplest possible linear relationship between the two in log log space log m log k b log t an empirical finding empirical law
IR_ch4,11,introduction to information retrieval sec heaps law fig p for rcv the dashed line log m log t is the best least squares fit thus m t so k and b good empirical fit for reuters rcv for first tokens law predicts terms actually terms
IR_ch4,12,introduction to information retrieval sec exercises what is the effect of including spelling errors vs automatically correcting spelling errors on heaps law compute the vocabulary size m for this scenario looking at a collection of web pages you find that there are different terms in the first tokens and different terms in the first tokens assume a search engine indexes a total of pages containing tokens on average what is the size of the vocabulary of the indexed collection as predicted by heaps law
IR_ch4,13,introduction to information retrieval sec zipf s law heaps law gives the vocabulary size in collections we also study the relative frequencies of terms in natural language there are a few very frequent terms and very many very rare terms zipf s law the ith most frequent term has frequency proportional to i cf i k i where k is a normalizing constant i cf is collection frequency the number of i occurrences of the term t in the collection i
IR_ch4,14,introduction to information retrieval sec zipf consequences if the most frequent term the occurs cf times then the second most frequent term of occurs cf times the third most frequent term and occurs cf times equivalent cf k i where k is a normalizing factor i so log cf log k log i i linear relationship between log cf and log i i another power law relationship
IR_ch4,15,introduction to information retrieval sec zipf s law for reuters rcv
IR_ch4,16,introduction to information retrieval ch compression now we will consider compressing the space for the dictionary and postings we ll do basic boolean index only no study of positional indexes etc but these ideas can be extended we will consider compression schemes
IR_ch4,17,introduction to information retrieval sec dictionary compression
IR_ch4,18,introduction to information retrieval sec why compress the dictionary search begins with the dictionary we want to keep it in memory memory footprint competition with other applications embedded mobile devices may have very little memory even if the dictionary isn t in memory we want it to be small for a fast search startup time so compressing the dictionary is important
IR_ch4,19,introduction to information retrieval dictionary storage naïve version array of fixed width entries terms bytes term mb t a a z a u e r c h l u m e s n f r e q p o s t i n g s p t r sec bytes bytes each dictionary search structure
IR_ch4,20,introduction to information retrieval sec fixed width terms are wasteful most of the bytes in the term column are wasted we allot bytes for letter terms and we still can t handle supercalifragilisticexpialidocious or hydrochlorofluorocarbons written english averages characters word exercise why is isn t this the number to use for estimating the dictionary size ave dictionary word in english characters how do we use characters per dictionary term short words dominate token counts but not type average
IR_ch4,21,introduction to information retrieval compressing the term list dictionary as a string systilesyzygeticsyzygialsyzygyszaibelyiteszczecinszomo f r e q p o s t i n g s p t r t e r m p t r sec store dictionary as a long string of characters pointer to next word shows end of current word hope to save up to of dictionary space total string length k x b mb pointers resolve m positions log m bits bytes
IR_ch4,22,introduction to information retrieval sec space for dictionary as a string bytes per term for freq now avg bytes per term for pointer to postings bytes term not bytes per term pointer avg bytes per term in term string k terms x mb against mb for fixed width
IR_ch4,23,introduction to information retrieval sec blocking store pointers to every kth term string example below k need to store term lengths extra byte systile syzygetic syzygial syzygy szaibelyite szczecin szomo freq postings ptr term ptr save bytes lose bytes on on term lengths pointers
IR_ch4,24,introduction to information retrieval sec blocking net gains example for block size k where we used bytes pointer without blocking x bytes now we use bytes shaved another mb this reduces the size of the dictionary from mb to mb we can save more with larger k question why not go with larger k
IR_ch4,25,introduction to information retrieval sec dictionary search without blocking assuming each dictionary term equally likely in query not really so in practice average number of comparisons exercise what if the frequencies of query terms were non uniform but known how would you structure the dictionary search tree
IR_ch4,26,introduction to information retrieval sec dictionary search with blocking binary search down to term block then linear search through terms in block blocks of binary tree avg compares
IR_ch4,27,introduction to information retrieval sec exercises estimate the space usage and savings compared to mb with blocking for block sizes of k and estimate the impact on search performance and slowdown compared to k with blocking for block sizes of k and
IR_ch4,28,introduction to information retrieval sec front coding front coding sorted words commonly have long common prefix store differences only for last k in a block of k automata automate automatic automation automat a e ic ion extra length encodes prefix automat beyond automat begins to resemble general string compression
IR_ch4,29,introduction to information retrieval sec rcv dictionary compression summary technique size in mb fixed width dictionary as string with pointers to every term blocking k blocking front coding
IR_ch4,30,introduction to information retrieval sec postings compression
IR_ch4,31,introduction to information retrieval sec postings compression the postings file is much larger than the dictionary factor of at least often over times larger key desideratum store each posting compactly a posting for our purposes is a docid for reuters documents we would use bits per docid when using byte integers alternatively we can use log bits per docid our goal use far fewer than bits per docid
IR_ch4,32,introduction to information retrieval sec postings two conflicting forces a term like arachnocentric occurs in maybe one doc out of a million we would like to store this posting using log m bits a term like the occurs in virtually every doc so bits posting mb is too expensive prefer bitmap vector in this case k
IR_ch4,33,introduction to information retrieval sec gap encoding of postings file entries we store the list of docs containing a term in increasing order of docid computer consequence it suffices to store gaps hope most gaps can be encoded stored with far fewer than bits especially for common words
IR_ch4,34,introduction to information retrieval sec three postings entries
IR_ch4,35,introduction to information retrieval sec variable length encoding aim for arachnocentric we will use bits gap entry for the we will use bit gap entry if the average gap for a term is g we want to use log g bits gap entry key challenge encode every integer gap with about as few bits as needed for that integer this requires a variable length encoding variable length codes achieve this by using short codes for small numbers
IR_ch4,36,introduction to information retrieval unary code represent n as n s with a final unary code for is unary code for is unary code for is this doesn t look promising but optimal if p n n we can use it as part of our solution
IR_ch4,37,introduction to information retrieval sec gamma codes we can compress better with bit level codes the gamma code is the best known of these represent a gap g as a pair length and offset offset is g in binary with the leading bit cut off for example length is the length of offset for offset this is we encode length with unary code gamma code of is the concatenation of length and offset
IR_ch4,38,introduction to information retrieval sec gamma code examples number length offset code none
IR_ch4,39,introduction to information retrieval reminder bitwise operations for compression you need to use bitwise operators python and most everything else bitwise and bitwise or bitwise xor ones complement left shift bits right shift lacks zero fill right shift recipes extract bits a x f if take high order bit add x f combine bit numbers a b c lookup tables rather than decoding can be faster yet still small
IR_ch4,40,introduction to information retrieval sec gamma code properties g is encoded using log g bits length of offset is log g bits length of length is log g bits all gamma codes have an odd number of bits almost within a factor of of best possible log g gamma code is uniquely prefix decodable like vb gamma code can be used for any distribution optimal for p n n gamma code is parameter free
IR_ch4,41,introduction to information retrieval sec gamma seldom used in practice machines have word boundaries bits operations that cross word boundaries are slower compressing and manipulating at the granularity of bits can be too slow all modern practice is to use byte or word aligned codes variable byte encoding is a faster conceptually simpler compression scheme with decent compression
IR_ch4,42,introduction to information retrieval sec variable byte vb codes for a gap value g we want to use close to the fewest bytes needed to hold log g bits begin with one byte to store g and dedicate bit in it to be a continuation bit c if g binary encode it in the available bits and set c else encode g s lower order bits and then use additional bytes to encode the higher order bits using the same algorithm at the end set the continuation bit of the last byte to c and for the other bytes c
IR_ch4,43,introduction to information retrieval sec example docids gaps vb code postings stored as the byte concatenation key property vb encoded postings are uniquely prefix decodable for a small gap vb uses a whole byte
IR_ch4,44,introduction to information retrieval sec rcv compression data structure size in mb dictionary fixed width dictionary term pointers into string with blocking k with blocking front coding collection text xml markup etc collection text term doc incidence matrix postings uncompressed bit words postings uncompressed bits postings variable byte encoded postings encoded
IR_ch4,45,introduction to information retrieval sec other variable unit codes variable byte codes are used by many real systems good low tech blend of variable length coding and sensitivity to computer memory alignment matches byte alignment wastes space if you have many small gaps as gap encoding often makes more modern work mainly uses the ideas be word aligned or bits even faster encode several gaps at the same time often assume a maximum gap size perhaps with an escape
IR_ch4,46,introduction to information retrieval group variable integer code used by google around turn of millennium jeff dean keynote at wsdm and presentations at cs encodes integers in blocks of size bytes first byte four bit binary length fields l l l l l j then l l l l bytes between hold numbers each number can use bits max gap length billion it was suggested that this was about twice as fast as vb encoding decoding gaps is much simpler no bit masking first byte can be decoded with lookup table or switch
IR_ch4,47,introduction to information retrieval simple anh moffat a word aligned multiple number encoding scheme how can we store several numbers in bits with a format selector bit numbers bit numbers bit numbers bit numbers selectors total ways
IR_ch4,48,introduction to information retrieval simple encoding scheme anh moffat encoding block bytes bits most significant nibble bits describe the layout of the other bits as follows layout n numbers of b bits each a single bit number bits n b two bit numbers three bit numbers and one spare bit four bit numbers five bit numbers and three spare bits seven bit numbers nine bit numbers and one spare bit fourteen two bit numbers twenty eight one bit numbers simple is a variant with additional uneven configurations efficiently decoded with hand coded decoder using bit masks extended simple family idea applies to bit words etc
IR_ch4,49,introduction to information retrieval sec index compression summary we can now create an index for highly efficient boolean retrieval that is very space efficient only of the total size of the collection only of the total size of the text in the collection we ve ignored positional information hence space savings are less for indexes used in practice but techniques substantially the same
IR_ch4,50,introduction to information retrieval ch resources for today s lecture iir mg f scholer h e williams and j zobel compression of inverted indexes for fast query evaluation proc acm sigir variable byte codes v n anh and a moffat inverted index compression using word aligned binary codes information retrieval word aligned codes
IR_ch5,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search christopher manning and pandu nayak wildcard queries and spelling correction
IR_ch5,2,introduction to information retrieval wild card queries
IR_ch5,3,introduction to information retrieval sec wild card queries mon find all docs containing any word beginning with mon easy with binary tree or b tree dictionary retrieve all words in range mon w moo mon find words ending in mon harder maintain an additional b tree for terms backwards can retrieve all words in range nom w non from this how can we enumerate all terms meeting the wild card query pro cent
IR_ch5,4,introduction to information retrieval sec query processing at this point we have an enumeration of all terms in the dictionary that match the wild card query we still have to look up the postings for each enumerated term e g consider the query se ate and fil er this may result in the execution of many boolean and queries
IR_ch5,5,introduction to information retrieval sec b trees handle s at the end of a query term how can we handle s in the middle of query term co tion we could look up co and tion in a b tree and intersect the two term sets expensive the solution transform wild card queries so that the s occur at the end this gives rise to the permuterm index
IR_ch5,6,introduction to information retrieval sec permuterm index add a to the end of each term rotate the resulting term and index them in a b tree for term hello index under hello ello h llo he lo hel o hell hello where is a special symbol hello ello h llo he empirically dictionary hello quadruples in size lo hel o hell hello
IR_ch5,7,introduction to information retrieval sec permuterm query processing add rotate to end lookup in permuterm index queries x lookup on x hello for hello x lookup on x hel for hel x lookup on x llo for llo x lookup on x ell for ell x y lookup on y x lo h for h lo x y z treat as a search for x z and post filter for h a o search for h o by looking up o h and post filter hello and retain halo
IR_ch5,8,introduction to information retrieval sec bigram k gram indexes enumerate all k grams sequence of k chars occurring in any term e g from text april is the cruelest month we get the grams bigrams a ap pr ri il l i is s t th he e c cr ru ue el le es st t m mo on nt h is a special word boundary symbol maintain a second inverted index from bigrams to dictionary terms that match each bigram
IR_ch5,9,introduction to information retrieval sec bigram index example the k gram index finds terms based on a query consisting of k grams here k m mace madden mo among amortize on along among
IR_ch5,10,introduction to information retrieval sec processing wild cards query mon can now be run as m and mo and on gets terms that match and version of our wildcard query but we d enumerate moon must post filter these terms against query surviving enumerated terms are then looked up in the term document inverted index fast space efficient compared to permuterm
IR_ch5,11,introduction to information retrieval sec processing wild card queries as before we must execute a boolean query for each enumerated filtered term wild cards can result in expensive query execution very large disjunctions pyth and prog if you encourage laziness people will respond search type your search terms use if you need to e g alex will match alexander
IR_ch5,12,introduction to information retrieval spelling correction
IR_ch5,13,introduction to information retrieval applications for spelling correction word processing phones web search
IR_ch5,14,introduction to information retrieval rates of spelling errors depending on the application error rates web queries wang et al retyping no backspace whitelaw et al english german words corrected retyping on phone sized organizer words uncorrected on organizer soukoreff mackenzie retyping kane and wobbrock gruden et al
IR_ch5,15,introduction to information retrieval spelling tasks spelling error detection spelling error correction autocorrect hte the suggest a correction suggestion lists
IR_ch5,16,introduction to information retrieval types of spelling errors non word errors graffe giraffe real word errors typographical errors three there cognitive errors homophones piece peace too two your you re non word correction was historically mainly context insensitive real word correction almost needs to be context sensitive
IR_ch5,17,introduction to information retrieval non word spelling errors non word spelling error detection any word not in a dictionary is an error the larger the dictionary the better up to a point the web is full of mis spellings so the web isn t necessarily a great dictionary non word spelling error correction generate candidates real words that are similar to error choose the one which is best shortest weighted edit distance highest noisy channel probability
IR_ch5,18,introduction to information retrieval real word non word spelling errors for each word w generate candidate set find candidate words with similar pronunciations find candidate words with similar spellings include w in candidate set choose best candidate noisy channel view of spell errors context sensitive so have to consider whether the surrounding words make sense flying form heathrow to lax flying from heathrow to lax
IR_ch5,19,introduction to information retrieval terminology we just discussed character bigrams and k grams st pr an we can also have word bigrams and n grams palo alto flying from road repairs
IR_ch5,20,introduction to information retrieval the noisy channel model of spelling independent word spelling correction
IR_ch5,21,introduction to information retrieval noisy channel intuition
IR_ch5,22,introduction to information retrieval noisy channel bayes rule we see an observation x of a misspelled word find the correct word ŵ w ˆ a a r r gw gw mî mî v v a a x x p p w x p w x x p w bayes argmax p x w p w wîv prior noisy channel model
IR_ch5,23,introduction to information retrieval history noisy channel for spelling proposed around ibm mays eric fred j damerau and robert l mercer context based spelling correction information processing and management at t bell labs kernighan mark d kenneth w church and william a gale a spelling correction program based on a noisy channel model proceedings of coling
IR_ch5,24,introduction to information retrieval non word spelling error example acress
IR_ch5,25,introduction to information retrieval candidate generation words with similar spelling small edit distance to error words with similar pronunciation small distance of pronunciation to error
IR_ch5,26,introduction to information retrieval candidate testing damerau levenshtein edit distance minimal edit distance between two strings where edits are insertion deletion substitution transposition of two adjacent letters see iir sec for edit distance
IR_ch5,27,introduction to information retrieval words within of acress error candidate correct error type correction letter letter acress actress t deletion acress cress a insertion acress caress ca ac transposition acress access c r substitution acress across o e substitution acress acres s insertion
IR_ch5,28,introduction to information retrieval candidate generation of errors are within edit distance almost all errors within edit distance also allow insertion of space or hyphen thisidea this idea inlaw in law can also allow merging words data base database for short texts like a query can just regard whole string as one item from which to produce edits
IR_ch5,29,introduction to information retrieval how do you generate the candidates run through dictionary check edit distance with each word generate all words within edit distance k e g k or and then intersect them with dictionary use a character k gram index and find dictionary words that share most k grams with word e g by jaccard coefficient see iir sec compute them fast with a levenshtein finite state transducer have a precomputed map of words to possible corrections
IR_ch5,30,introduction to information retrieval a paradigm we want the best spell corrections instead of finding the very best we find a subset of pretty good corrections say edit distance at most find the best amongst them these may not be the actual best this is a recurring paradigm in ir including finding the best docs for a query best answers best ads find a good candidate set find the top k amongst them and return them as the best
IR_ch5,31,introduction to information retrieval let s say we ve generated candidates now back to bayes rule we see an observation x of a misspelled word find the correct word ŵ w ˆ a a r r gw gw mî mî v v a a x x p p w x p x w x p w argmax p x w p w what s p w wîv
IR_ch5,32,introduction to information retrieval language model take a big supply of words your document collection with t tokens let c w occurrences of w in other applications you can take the supply to be typed queries suitably filtered when a static dictionary is inadequate p w c t w
IR_ch5,33,introduction to information retrieval unigram prior probability counts from words in corpus of contemporary english coca word frequency of p w word actress cress caress access across acres
IR_ch5,34,introduction to information retrieval channel model probability error model probability edit probability kernighan church gale misspelled word x x x x x m correct word w w w w w n p x w probability of the edit deletion insertion substitution transposition
IR_ch5,35,introduction to information retrieval computing error probability confusion matrix del x y count xy typed as x ins x y count x typed as xy sub x y count y typed as x trans x y count xy typed as yx insertion and deletion conditioned on previous character
IR_ch5,36,introduction to information retrieval confusion matrix for substitution
IR_ch5,37,introduction to information retrieval nearby keys
IR_ch5,38,introduction to information retrieval generating the confusion matrix peter norvig s list of errors peter norvig s list of counts of single edit errors all peter norvig s ngrams data links http norvig com ngrams
IR_ch5,39,introduction to information retrieval channel model kernighan church gale
IR_ch5,40,introduction to information retrieval smoothing probabilities add smoothing but if we use the confusion matrix example unseen errors are impossible they ll make the overall probability that seems too harsh e g in kernighan s chart q a and a q are both even though they re adjacent on the keyboard a simple solution is to add to all counts and then if there is a a character alphabet to normalize appropriately sub x w if substitution p x w count w a
IR_ch5,41,introduction to information retrieval channel model for acress candidate correct error x w p x w correction letter letter actress t c ct cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
IR_ch5,42,introduction to information retrieval candidate correct error x w p x w p w corrnectoionisy lcethteranlnetteerl probability for acress p x w p w actress t c ct cress a a caress ca ac ac c a access c r r c across o e e o acres s es e acres s ss s
IR_ch5,43,introduction to information retrieval candidate correct error x w p x w p w noisy channel probability for acress correction letter letter p x w p w actress t c c t cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
IR_ch5,44,introduction to information retrieval evaluation some spelling error test sets wikipedia s list of common english misspelling aspell filtered version of that list birkbeck spelling error corpus peter norvig s list of errors includes wikipedia and birkbeck for training or testing
IR_ch5,45,introduction to information retrieval context sensitive spelling correction spelling correction with the noisy channel
IR_ch5,46,introduction to information retrieval real word spelling errors leaving in about fifteen minuets to go to her house the design an construction of the system can they lave him my messages the study was conducted mainly be john black of spelling errors are real words kukich
IR_ch5,47,introduction to information retrieval context sensitive spelling error fixing for each word in sentence phrase query generate candidate set the word itself all single letter edits that are english words words that are homophones all of this can be pre computed choose best candidates noisy channel model
IR_ch5,48,introduction to information retrieval noisy channel for real word spell correction given a sentence x x x x n generate a set of candidates for each word x i candidate x x w w w candidate x x w w w candidate x x w w w n n n n n choose the sequence w that maximizes p w x x n wˆ argmax p w x wîv argmax p x w p w wîv
IR_ch5,49,introduction to information retrieval incorporating context words context sensitive spelling correction determining whether actress or across is appropriate will require looking at the context of use we can do this with a better language model you learned can learn a lot about language models in cs or cs n here we present just enough to be dangerous do the assignment a bigram language model conditions the probability of a word on just the previous word p w w p w p w w p w w n n n
IR_ch5,50,introduction to information retrieval incorporating context words for unigram counts p w is always non zero if our dictionary is derived from the document collection this won t be true of p w w we need to smooth k k we could use add smoothing on this conditional distribution but here s a better way interpolate a unigram and a bigram p w w λp w λ p w w li k k uni k bi k k p w w c w w c w bi k k k k k
IR_ch5,51,introduction to information retrieval all the important fine points note that we have several probability distributions for words keep them straight you might want need to work with log probabilities log p w w log p w log p w w log p w w n n n otherwise be very careful about floating point underflow our query may be words anywhere in a document we ll start the bigram estimate of a sequence with a unigram estimate often people instead condition on a start of sequence symbol but not good here because of this the unigram and bigram counts have different totals not a problem
IR_ch5,52,introduction to information retrieval using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
IR_ch5,53,introduction to information retrieval using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
IR_ch5,54,introduction to information retrieval noisy channel for real word spell correction two of thew to threw tao off thaw too on the two of thaw
IR_ch5,55,introduction to information retrieval noisy channel for real word spell correction two of thew to threw tao off thaw too on the two of thaw
IR_ch5,56,introduction to information retrieval simplification one error per sentence out of all possible sentences with one word replaced w w w w two off thew w w w w two of the w w w w too of thew choose the sequence w that maximizes p w
IR_ch5,57,introduction to information retrieval where to get the probabilities language model unigram bigram etc channel model same as for non word spelling correction plus need probability for no error p w w
IR_ch5,58,introduction to information retrieval probability of no error what is the channel probability for a correctly typed word p the the if you have a big corpus you can estimate this percent correct but this value depends strongly on the application error in words error in words error in words
IR_ch5,59,introduction to information retrieval peter norvig s thew example x w x w p x w p w p x w p w thew the ew e thew thew thew thaw e a thew threw h hr ew w thew thwe e
IR_ch5,60,introduction to information retrieval state of the art noisy channel we never just multiply the prior and the error model independence assumptions probabilities not commensurate instead weight them learn λ from a development test set w ˆ a r gw mî v a x p x w p w l
IR_ch5,61,introduction to information retrieval improvements to channel model allow richer edits brill and moore ent ant ph f le al incorporate pronunciation into channel toutanova and moore incorporate device into channel not all android phones need have the same error model but spell correction may be done at the system level
IR_ch6,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search pandu nayak and prabhakar raghavan lecture scoring term weighting and the vector space model
IR_ch6,2,introduction to information retrieval recap of lecture collection and vocabulary statistics heaps and zipf s laws dictionary compression for boolean indexes dictionary string blocks front coding postings compression gap encoding prefix unique codes variable byte and gamma codes collection text xml markup etc mb collection text term doc incidence matrix postings uncompressed bit words postings uncompressed bits postings variable byte encoded postings encoded
IR_ch6,3,introduction to information retrieval this lecture iir sections ranked retrieval scoring documents term frequency collection statistics weighting schemes vector space scoring
IR_ch6,4,introduction to information retrieval ch ranked retrieval thus far our queries have all been boolean documents either match or don t good for expert users with precise understanding of their needs and the collection also good for applications applications can easily consume s of results not good for the majority of users most users incapable of writing boolean queries or they are but they think it s too much work most users don t want to wade through s of results this is particularly true of web search
IR_ch6,5,introduction to information retrieval ch problem with boolean search feast or famine boolean queries often result in either too few or too many s results query standard user dlink hits query standard user dlink no card found hits it takes a lot of skill to come up with a query that produces a manageable number of hits and gives too few or gives too many
IR_ch6,6,introduction to information retrieval ranked retrieval models rather than a set of documents satisfying a query expression in ranked retrieval the system returns an ordering over the top documents in the collection for a query free text queries rather than a query language of operators and expressions the user s query is just one or more words in a human language in principle there are two separate choices here but in practice ranked retrieval has normally been associated with free text queries and vice versa
IR_ch6,7,introduction to information retrieval ch feast or famine not a problem in ranked retrieval when a system produces a ranked result set large result sets are not an issue indeed the size of the result set is not an issue we just show the top k results we don t overwhelm the user premise the ranking algorithm works
IR_ch6,8,introduction to information retrieval ch scoring as the basis of ranked retrieval we wish to return in order the documents most likely to be useful to the searcher how can we rank order the documents in the collection with respect to a query assign a score say in to each document this score measures how well document and query match
IR_ch6,9,introduction to information retrieval ch query document matching scores we need a way of assigning a score to a query document pair let s start with a one term query if the query term does not occur in the document score should be the more frequent the query term in the document the higher the score should be we will look at a number of alternatives for this
IR_ch6,10,introduction to information retrieval ch take jaccard coefficient recall from lecture a commonly used measure of overlap of two sets a and b jaccard a b a b a b jaccard a a jaccard a b if a b a and b don t have to be the same size always assigns a number between and
IR_ch6,11,introduction to information retrieval ch jaccard coefficient scoring example what is the query document match score that the jaccard coefficient computes for each of the two documents below query ides of march document caesar died in march document the long march
IR_ch6,12,introduction to information retrieval issues with jaccard for scoring it doesn t consider term frequency how many times a term occurs in a document rare terms in a collection are more informative than frequent terms jaccard doesn t consider this information we need a more sophisticated way of normalizing for length later in this lecture we ll use instead of a b a b jaccard for length normalization a b a b ch
IR_ch6,13,introduction to information retrieval sec recall lecture binary term document incidence matrix antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser each document is represented by a binary vector v
IR_ch6,14,introduction to information retrieval sec term document count matrices consider the number of occurrences of a term in a document each document is a count vector in ℕv a column below antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch6,15,introduction to information retrieval bag of words model vector representation doesn t consider the ordering of words in a document john is quicker than mary and mary is quicker than john have the same vectors this is called the bag of words model in a sense this is a step back the positional index was able to distinguish these two documents we will look at recovering positional information later in this course for now bag of words model
IR_ch6,16,introduction to information retrieval term frequency tf the term frequency tf of term t in document d is t d defined as the number of times that t occurs in d we want to use tf when computing query document match scores but how raw term frequency is not what we want a document with occurrences of the term is more relevant than a document with occurrence of the term but not times more relevant relevance does not increase proportionally with term frequency nb frequency count in ir
IR_ch6,17,introduction to information retrieval log frequency weighting the log frequency weight of term t in d is etc score for a document query pair sum over terms t in both q and d score the score is if none of the query terms is present in the document w t d t q l o d g t f t l d o g t f t i o d f t t h f e t d r w i s e sec
IR_ch6,18,introduction to information retrieval sec document frequency rare terms are more informative than frequent terms recall stop words consider a term in the query that is rare in the collection e g arachnocentric a document containing this term is very likely to be relevant to the query arachnocentric we want a high weight for rare terms like arachnocentric
IR_ch6,19,introduction to information retrieval sec document frequency continued frequent terms are less informative than rare terms consider a query term that is frequent in the collection e g high increase line a document containing such a term is more likely to be relevant than a document that doesn t but it s not a sure indicator of relevance for frequent terms we want high positive weights for words like high increase and line but lower weights than for rare terms we will use document frequency df to capture this
IR_ch6,20,introduction to information retrieval idf weight df is the document frequency of t the number of t documents that contain t df is an inverse measure of the informativeness of t t df n t we define the idf inverse document frequency of t by we use log n df instead of n df to dampen the effect t t of idf i d f t l o g n d f t sec will turn out the base of the log is immaterial
IR_ch6,21,introduction to information retrieval sec idf example suppose n million term df idf t t calpurnia animal sunday fly under the idf log n df t t there is one idf value for each term t in a collection
IR_ch6,22,introduction to information retrieval effect of idf on ranking does idf have an effect on ranking for one term queries like iphone idf has no effect on ranking one term queries idf affects the ranking of documents for queries with at least two terms for the query capricious person idf weighting makes occurrences of capricious count for much more in the final document ranking than occurrences of person
IR_ch6,23,introduction to information retrieval sec collection vs document frequency the collection frequency of t is the number of occurrences of t in the collection counting multiple occurrences example word collection frequency document frequency insurance try which word is a better search term and should get a higher weight
IR_ch6,24,introduction to information retrieval sec tf idf weighting the tf idf weight of a term is the product of its tf weight and its idf weight w log tf log n df t d t t d best known weighting scheme in information retrieval note the in tf idf is a hyphen not a minus sign alternative names tf idf tf x idf increases with the number of occurrences within a document increases with the rarity of the term in the collection
IR_ch6,25,introduction to information retrieval score for a document given a query there are many variants how tf is computed with without logs whether the terms in the query are also weighted s c o r e q d t q d t f i d f t d sec
IR_ch6,26,introduction to information retrieval binary count weight matrix c c a b c a l m w n t o n y r u t u s a e s a r l p u r n i e o p a t r e r c y o r s e r a a a n t o n y a n d c l e o p a t r a j u l i u s c a e s a r t h e t e m p e s t h a m l e t o t h e l l o m a c b e t h sec each document is now represented by a real valued vector of tf idf weights r v
IR_ch6,27,introduction to information retrieval sec documents as vectors so we have a v dimensional vector space terms are axes of the space documents are points or vectors in this space very high dimensional tens of millions of dimensions when you apply this to a web search engine these are very sparse vectors most entries are zero
IR_ch6,28,introduction to information retrieval sec queries as vectors key idea do the same for queries represent them as vectors in the space key idea rank documents according to their proximity to the query in this space proximity similarity of vectors proximity inverse of distance recall we do this because we want to get away from the you re either in or out boolean model instead rank more relevant documents higher than less relevant documents
IR_ch6,29,introduction to information retrieval sec formalizing vector space proximity first cut distance between two points distance between the end points of the two vectors euclidean distance euclidean distance is a bad idea because euclidean distance is large for vectors of different lengths
IR_ch6,30,introduction to information retrieval sec why distance is a bad idea the euclidean distance between q and d is large even though the distribution of terms in the query q and the distribution of terms in the document d are very similar
IR_ch6,31,introduction to information retrieval sec use angle instead of distance thought experiment take a document d and append it to itself call this document d semantically d and d have the same content the euclidean distance between the two documents can be quite large the angle between the two documents is corresponding to maximal similarity key idea rank documents according to angle with query
IR_ch6,32,introduction to information retrieval sec from angles to cosines the following two notions are equivalent rank documents in decreasing order of the angle between query and document rank documents in increasing order of cosine query document cosine is a monotonically decreasing function for the interval o o
IR_ch6,33,introduction to information retrieval sec from angles to cosines but how and why should we be computing cosines
IR_ch6,34,introduction to information retrieval sec length normalization a vector can be length normalized by dividing each of its components by its length for this we use the l norm x x i i dividing a vector by its l norm makes it a unit length vector on surface of unit hypersphere effect on the two documents d and d d appended to itself from earlier slide they have identical vectors after length normalization long and short documents now have comparable weights
IR_ch6,35,introduction to information retrieval cosine query document c o s q d q q d d q q d d v i q v i i q i d i v i d i sec dot product unit vectors q is the tf idf weight of term i in the query i d is the tf idf weight of term i in the document i cos q d is the cosine similarity of q and d or equivalently the cosine of the angle between q and d
IR_ch6,36,introduction to information retrieval cosine for length normalized vectors for length normalized vectors cosine similarity is simply the dot product or scalar product v cos q d q d q d i i i for q d length normalized
IR_ch6,37,introduction to information retrieval cosine similarity illustrated
IR_ch6,38,introduction to information retrieval sec cosine similarity amongst documents how similar are the novels term sas pap wh sas sense and affection sensibility jealous pap pride and gossip wuthering prejudice and wh wuthering term frequencies counts heights note to simplify this example we don t do idf weighting
IR_ch6,39,introduction to information retrieval sec documents example contd log frequency weighting after length normalization term sas pap wh term sas pap wh affection affection jealous jealous gossip gossip wuthering wuthering cos sas pap cos sas wh cos pap wh why do we have cos sas pap cos sas wh
IR_ch6,40,introduction to information retrieval sec computing cosine scores
IR_ch6,41,introduction to information retrieval sec tf idf weighting has many variants columns headed n are acronyms for weight schemes why is the base of the log in idf immaterial
IR_ch6,42,introduction to information retrieval sec weighting may differ in queries vs documents many search engines allow for different weightings for queries vs documents smart notation denotes the combination in use in an engine with the notation ddd qqq using the acronyms from the previous table a very standard weighting scheme is lnc ltc document logarithmic tf l as first character no idf and cosine normalization a bad idea query logarithmic tf l in leftmost column idf t in second column no normalization
IR_ch6,43,introduction to information retrieval sec tf idf example lnc ltc document car insurance auto insurance query best car insurance term query document pro d tf tf wt df idf wt n liz tf raw tf wt wt n liz raw e e auto best car insurance exercise what is n the number of docs doc length score
IR_ch6,44,introduction to information retrieval summary vector space ranking represent the query as a weighted tf idf vector represent each document as a weighted tf idf vector compute the cosine similarity score for the query vector and each document vector rank documents with respect to the query by score return the top k e g k to the user
IR_ch6,45,introduction to information retrieval ch resources for today s lecture iir http www miislita com information retrieval tutorial cosine similarity tutorial html term weighting and cosine similarity tutorial for seo folk
IR_ch7,1,introduction to information retrieval introduction to information retrieval probabilistic information retrieval christopher manning and pandu nayak
IR_ch7,2,introduction to information retrieval ch from boolean to ranked retrieval why ranked retrieval introduction to the classical probabilistic retrieval model and the probability ranking principle the binary independence model bim relevance feedback briefly the vector space model vsm quick cameo bm model ranking with features bm f if time allows
IR_ch7,3,introduction to information retrieval ch ranked retrieval thus far our queries have all been boolean documents either match or don t can be good for expert users with precise understanding of their needs and the collection can also be good for applications applications can easily consume s of results not good for the majority of users most users incapable of writing boolean queries or they are but they think it s too much work most users don t want to wade through s of results this is particularly true of web search
IR_ch7,4,introduction to information retrieval ch problem with boolean search feast or famine boolean queries often result in either too few or too many s results query standard user dlink hits query standard user dlink no card found hits it takes a lot of skill to come up with a query that produces a manageable number of hits and gives too few or gives too many suggested solution rank documents by goodness a sort of clever soft and
IR_ch7,5,introduction to information retrieval why probabilities in ir understanding user query information need of user need is representation uncertain how to match uncertain guess of document whether document documents representation has relevant content in traditional ir systems matching between each document and query is attempted in a semantically imprecise space of index terms probabilities provide a principled foundation for uncertain reasoning can we use probabilities to quantify our search uncertainties
IR_ch7,6,introduction to information retrieval probabilistic ir topics classical probabilistic retrieval model probability ranking principle etc binary independence model naïve bayes text cat okapi bm bayesian networks for text retrieval language model approach to ir iir ch an important development in s ir probabilistic methods are one of the oldest but also one of the currently hot topics in ir traditionally neat ideas but didn t win on performance it seems to be different now
IR_ch7,7,introduction to information retrieval who are these people karen spärck jones karen spärck jones stephen robertson keith van rijsbergen
IR_ch7,8,introduction to information retrieval the document ranking problem we have a collection of documents user issues a query a list of documents needs to be returned ranking method is the core of modern ir systems in what order do we present documents to the user we want the best document to be first second best second etc idea rank by probability of relevance of the document w r t information need p r document query i
IR_ch7,9,introduction to information retrieval the probability ranking principle prp if a reference retrieval system s response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data s s s robertson w s cooper m e maron van rijsbergen manning schütze
IR_ch7,10,introduction to information retrieval for events a and b bayes rule prior odds p p a a b b p p a b ç p a b b p a p a å b x p p a b b a p a b p p b x a a p x p a recall a few probability basics posterior p a p a o a p a p a
IR_ch7,11,introduction to information retrieval the probability ranking principle prp let x represent a document in the collection let r represent relevance of a document w r t given fixed query and let r represent relevant and r not relevant p p r r x x p p x x r r p p x p x p r r need to find p r x probability that a document x is relevant p r p r prior probability of retrieving a relevant or non relevant document at random p x r p x r probability that if a relevant not relevant document is retrieved it is x p r x p r x
IR_ch7,12,introduction to information retrieval probabilistic retrieval strategy first estimate how each term contributes to relevance how do other things like term frequency and document length influence your judgments about document relevance not at all in bim a more nuanced answer is given by bm combine to find document relevance probability order documents by decreasing probability theorem using the prp is optimal in that it minimizes the loss bayes risk under loss provable if all probabilities correct etc e g ripley
IR_ch7,13,introduction to information retrieval binary independence model traditionally used in conjunction with prp binary boolean documents are represented as binary incidence vectors of terms cf iir chapter iff term i is present in document x independence terms occur in documents independently different documents can be modeled as the same vector x x i x x n
IR_ch7,14,introduction to information retrieval binary independence model queries binary term incidence vectors given query q for each document d need to compute p r q d replace with computing p r q x where x is binary term incidence vector representing d interested only in ranking will use odds and bayes rule p r q p x r q p r q x p x q o r q x p r q p x r q p r q x p x q
IR_ch7,15,introduction to information retrieval binary independence model using independence assumption n p x r q õ o r q x o r q i p x r q i i p p x x r r q q õ i n p p x x i i r r q q p r q x p r q p x r q o r q x p r q x p r q p x r q constant for a needs estimation given query
IR_ch7,16,introduction to information retrieval binary independence model since x is either or i p x r q p x r q õ õ o r q x o r q i i p x r q p x r q x i x i i i let p i p x i r q r p x r q i i p r assume for all terms not occurring in the query q i i i o r q x o r q õ i n p p x x i i r r q q p p õ õ o r q x o r q i i r r x i x i i i q q i i
IR_ch7,17,introduction to information retrieval document relevant r not relevant r term present x p r i i i term absent x p r i i i
IR_ch7,18,introduction to information retrieval binary independence model all matching terms non matching query terms æ ö p r p p õ õ õ o r q x o r q i ç i i i r p r r è ø x i x i i x i i i i q q q i i i p r p õ õ o r q x o r q i i i r p r x q i i q i i i i all matching terms all query terms o r q x o r q x õ i q i p r i i õ xq i i p r i i
IR_ch7,19,introduction to information retrieval binary independence model constant for each query only quantity to be estimated for rankings o r q x o r q x i q i r p i i p r i i q i p r i i retrieval status value p r p r rsv log i i log i i r p r p x q i i x q i i i i i i
IR_ch7,20,introduction to information retrieval binary independence model robertson spärck jones all boils down to computing rsv r r s s v v l x o i g q i x i c i q i r p i c i i l p o r i i g r p i i x i q i p r i i l o g r p i i p r i i the c are log odds ratios of contingency table a few slides back i they function as the term weights in this model so how do we compute c s from our data i
IR_ch7,21,introduction to information retrieval graphical model for bim bernoulli nb r binary x i variables i î q x i t f i ¹
IR_ch7,22,introduction to information retrieval binary independence model estimating rsv coefficients in theory for each term i look at this table of document counts d x x t o i i o c t a u l m e n t s p i r e l e s s s v s s a s n t n o n n r n n n e l s s s e v a s n t t o n t n n a l n n s estimates for now r i n s assume no zero terms s s s c k n n s s log remember i n s n n s s smoothing c i l o g p r i i p r i i
IR_ch7,23,introduction to information retrieval estimation key challenge if non relevant documents are approximated by the whole collection then r prob of occurrence i in non relevant documents for query is n n and inverse document frequency idf spärck jones a key still important term weighting concept l o g r i r i l o g n n n s s s l o g n n n l o g n n i d f
IR_ch7,24,introduction to information retrieval sec collection vs document frequency collection frequency of t is the total number of occurrences of t in the collection incl multiples document frequency is number of docs t is in example word collection frequency document frequency insurance try which word is a better search term and should get a higher weight
IR_ch7,25,introduction to information retrieval estimation key challenge p probability of occurrence in relevant i documents cannot be approximated as easily p can be estimated in various ways i from relevant documents if you know some relevance weighting can be used in a feedback loop constant croft and harper combination match then just get idf weighting of terms with p i proportional to prob of occurrence in collection greiff sigir argues for df n i r s v x å i q i l o g n n i
IR_ch7,26,introduction to information retrieval probabilistic relevance feedback guess a preliminary probabilistic description of r documents use it to retrieve a set of documents interact with the user to refine the description learn some definite members with r and r re estimate p and r on the basis of these i i if i appears in v within set of documents v p v v i i i or can combine new information with original guess use bayesian prior v p p i i κ is i v prior weight repeat thus generating a succession of approximations to relevant documents
IR_ch7,27,introduction to information retrieval pseudo relevance feedback iteratively auto estimate p and r i i assume that p is constant over all x in query and r i i i as before p even odds for any given doc i determine guess of relevant document set v is fixed size set of highest ranked documents on this model we need to improve our guesses for p and r so i i use distribution of x in docs in v let v be set of i i documents containing x i p v v i i assume if not retrieved then not relevant r n v n v i i i go to until converges then return ranking
IR_ch7,28,introduction to information retrieval prp and bim it is possible to reasonably approximate probabilities but either require partial relevance information or need to make do with somewhat inferior term weights requires restrictive assumptions relevance of each document is independent of others really it s bad to keep on returning duplicates term independence terms not in query don t affect the outcome boolean representation of documents queries boolean notion of relevance some of these assumptions can be removed
IR_ch7,29,introduction to information retrieval removing term independence in general index terms aren t independent hong kong dependencies can be complex van rijsbergen proposed simple model of dependencies as a tree each term dependent on one other exactly friedman and goldszmidt s tree augmented naive bayes aaai in s estimation problems held back success of this model
IR_ch7,30,introduction to information retrieval term frequency and the vsm right in the first lecture we said that a page should rank higher if it mentions a word more perhaps modulated by things like page length why not in bim much of early ir was designed for titles or abstracts and not for modern full text search we now want a model with term frequency in it we ll mainly look at a probabilistic model bm first a quick summary of vector space model
IR_ch7,31,introduction to information retrieval summary vector space ranking ch represent the query as a weighted term frequency inverse document frequency tf idf vector represent each document as a weighted tf idf vector compute the cosine similarity score for the query vector and each document vector rank documents with respect to the query by score return the top k e g k to the user
IR_ch7,32,introduction to information retrieval
IR_ch7,33,introduction to information retrieval sec cosine similarity
IR_ch7,34,introduction to information retrieval sec tf idf weighting has many variants
IR_ch7,35,introduction to information retrieval bm
IR_ch7,36,introduction to information retrieval okapi bm robertson et al trec city u bm best match they had a bunch of tries developed in the context of the okapi system started to be increasingly adopted by other teams during the trec competitions it works well goal be sensitive to term frequency and document length while not adding too many parameters robertson and zaragoza spärck jones et al
IR_ch7,37,introduction to information retrieval generative model for documents words are drawn independently from the vocabulary using a multinomial distribution the draft is that each team is given a position in the draft basic given draft team the each nfl annual team of draft design nfl football is that football
IR_ch7,38,introduction to information retrieval generative model for documents distribution of term frequencies tf follows a binomial distribution approximated by a poisson the draft is that each team is given a position in the draft draft
IR_ch7,39,introduction to information retrieval poisson distribution the poisson distribution models the probability of k the number of events occurring in a fixed interval of time space with known average rate λ cf t independent of the last event examples number of cars arriving at a toll booth per minute number of typos on a page p k l k k e l
IR_ch7,40,introduction to information retrieval poisson distribution if t is large and p is small we can approximate a binomial distribution with a poisson where λ tp k l l p k e k mean variance λ tp example p t chance of occurrence is binomial poisson already close p e e p æçè ö ø
IR_ch7,41,introduction to information retrieval poisson model assume that term frequencies in a document tf i follow a poisson distribution fixed interval implies fixed document length think roughly constant sized document abstracts will fix later
IR_ch7,42,introduction to information retrieval poisson distributions
IR_ch7,43,introduction to information retrieval one poisson model flaw is a reasonable fit for general words is a poor fit for topic specific words get higher p k than predicted too often documents containing k occurrences of word λ freq word expected based conditions cathexis comic harter a probabilistic approach to automatic keyword indexing jasist
IR_ch7,44,introduction to information retrieval eliteness aboutness model term frequencies using eliteness what is eliteness hidden variable for each document term pair denoted as e for term i i represents aboutness a term is elite in a document if in some sense the document is about the concept denoted by the term eliteness is binary term occurrences depend only on eliteness but eliteness depends on relevance
IR_ch7,45,introduction to information retrieval elite terms text from the wikipedia page on the nfl draft showing elite terms the national football league draft is an annual event in which the national football league nfl teams select eligible college football players it serves as the league s most common source of player recruitment the basic design of the draft is that each team is given a position in the draft order in reverse order relative to its record
IR_ch7,46,introduction to information retrieval graphical model with eliteness r binary e i variables frequencies tf i not binary i î q
IR_ch7,47,introduction to information retrieval retrieval status value similar to the bim derivation we have where and using eliteness we have r s v e l i t e i î å q t f i c e i l i t e t f i p tf tf r p tf tf e elite p e elite r i i i i i i p tf tf e elite p e elite r i i i i c e i l i t e t f i l o g p p t t f f i i t f i r r p p t t f f i i t f i r r
IR_ch7,48,introduction to information retrieval poisson model the problems with the poisson model suggests fitting two poisson distributions in the poisson model the distribution is different depending on whether the term is elite or not where π is probability that document is elite for term but unfortunately we don t know π λ μ p t f i k i r
IR_ch7,49,introduction to information retrieval elite c tf let s get an idea graphing for i i different parameter values of the poisson
IR_ch7,50,introduction to information retrieval qualitative properties c elite i increases monotonically with tf i but asymptotically approaches a maximum value as not true for simple scaling of tf with the asymptotic limit being c e i l i t e t f i weight of bim c eliteness i feature t f i
IR_ch7,51,introduction to information retrieval approximating the saturation function estimating parameters for the poisson model is not easy so approximate it with a simple parametric curve that has the same qualitative properties tf k tf
IR_ch7,52,introduction to information retrieval saturation function for high values of k increments in tf continue to i contribute significantly to the score contributions tail off quickly for low values of k
IR_ch7,53,introduction to information retrieval early versions of bm version using the saturation function version bim simplification to idf k factor doesn t change ranking but makes term score when tf i similar to tf idf but term scores are bounded c b i m c b i v m t v f i t f i l o g c d b i n f i m i k t k k f i t f i t f t i f i
IR_ch7,54,introduction to information retrieval document length normalization longer documents are likely to have larger tf values i why might documents be longer verbosity suggests observed tf too high i larger scope suggests observed tf may be right i a real document collection probably has both effects so should apply some kind of partial normalization
IR_ch7,55,introduction to information retrieval document length normalization document length avdl average document length over collection length normalization component b full document length normalization b no document length normalization b æçè d l b å i î v t f b i a d v l d l ö ø b
IR_ch7,56,introduction to information retrieval document length normalization
IR_ch7,57,introduction to information retrieval okapi bm normalize tf using document length bm ranking function t f i t b f i n k tf c bm tf log i i i df k tf i i l o g d n f i k b k b a t f d v i l d l t f i å rsv bm cbm tf i i iîq
IR_ch7,58,introduction to information retrieval okapi bm k controls term frequency scaling k is binary model k large is raw term frequency b controls document length normalization b is no length normalization b is relative frequency fully scale by document length typically k is set around and b around iir sec discusses incorporating query term weighting and pseudo relevance feedback r s v b m å i î q l o g d n f i k b k b a t f d v i l d l t f i
IR_ch7,59,introduction to information retrieval why is bm better than vsm tf idf suppose your query is machine learning suppose you have documents with term counts doc learning machine doc learning machine tf idf log tf log n df doc doc bm k doc doc
IR_ch7,60,introduction to information retrieval ranking with features textual features zones title author abstract body anchors proximity non textual features file type file age page rank
IR_ch7,61,introduction to information retrieval ranking with zones straightforward idea apply your favorite ranking function bm to each zone separately combine zone scores using a weighted linear combination but that seems to imply that the eliteness properties of different zones are different and independent of each other which seems unreasonable
IR_ch7,62,introduction to information retrieval ranking with zones alternate idea assume eliteness is a term document property shared across zones but the relationship between eliteness and term frequencies are zone dependent e g denser use of elite topic words in title consequence first combine evidence across zones for each term then combine evidence across terms
IR_ch7,63,introduction to information retrieval bm f with zones calculate a weighted variant of total term frequency and a weighted variant of document length z z å å average dl tf v tf dl v len avdl i z zi z z across all z z documents where v is zone weight z tf is term frequency in zone z zi len is length of zone z z z is the number of zones
IR_ch7,64,introduction to information retrieval simple bm f with zones simple interpretation zone z is replicated v times z but we may want zone specific parameters k b idf r s v s i m p l e b m f å i î q l o g d n f i k b k b a t d v f i l d l t f i
IR_ch7,65,introduction to information retrieval bm f empirically zone specific length normalization i e zone specific b has been found to be useful z tf å tf v zi i z b z z b z æçè b z b z a l v e l n e z n z ö ø b z n k tf rsv bm f å log i df k tf iîq i i see robertson and zaragoza
IR_ch7,66,introduction to information retrieval ranking with non textual features assumptions usual independence assumption independent of each other and of the textual features allows us to factor out in bim style derivation relevance information is query independent usually true for features like page rank age type allows us to keep all non textual features in the bim style derivation where we drop non query terms p p f f j j f f j j r r
IR_ch7,67,introduction to information retrieval ranking with non textual features f å å rsv c tf lv f i i j j j where iîq j p f f r j j v f log j j p f f r j j and l is an artificially added free parameter to account for rescalings in the approximations care must be taken in selecting v depending on f j j e g explains why works well j f log l f j j j l f j j l j e x p f j l j bm rsv log pagerank
IR_ch7,68,introduction to information retrieval resources s e robertson and k spärck jones relevance weighting of search terms journal of the american society for information sciences c j van rijsbergen information retrieval nd ed london butterworths chapter http www dcs gla ac uk keith preface html k spärck jones s walker and s e robertson a probabilistic model of information retrieval development and comparative experiments part information processing and management s e robertson and h zaragoza the probabilistic relevance framework bm and beyond foundations and trends in information retrieval
IR_ch8,1,introduction to information retrieval introduction to information retrieval evaluation chris manning and pandu nayak cs information retrieval and web search
IR_ch8,2,introduction to information retrieval situation thanks to your stellar performance in cs you quickly rise to vp of search at internet retail giant nozama com your boss brings in her nephew sergey who claims to have built a better search engine for nozama do you laugh derisively and send him to rival tramlaw labs counsel sergey to go to stanford and take cs try a few queries on his engine and say not bad
IR_ch8,3,introduction to information retrieval sec what could you ask sergey how fast does it index number of documents hour incremental indexing nozama adds k products day how fast does it search latency and cpu needs for nozama s million products does it recommend related products this is all good but it says nothing about the quality of sergey s search you want nozama s users to be happy with the search experience
IR_ch8,4,introduction to information retrieval how do you tell if users are happy search returns products relevant to users how do you assess this at scale search results get clicked a lot misleading titles summaries can cause users to click users buy after using the search engine or users spend a lot of after using the search engine repeat visitors buyers do users leave soon after searching do they come back within a week month
IR_ch8,5,introduction to information retrieval sec happiness elusive to measure most common proxy relevance of search results pioneered by cyril cleverdon in the cranfield experiments but how do you measure relevance
IR_ch8,6,introduction to information retrieval sec measuring relevance three elements a benchmark document collection a benchmark suite of queries an assessment of either relevant or nonrelevant for each query and each document
IR_ch8,7,introduction to information retrieval so you want to measure the quality of a new search algorithm benchmark documents nozama s products benchmark query suite more on this judgments of document relevance for each query relevance million nozama com products judgment sample queries
IR_ch8,8,introduction to information retrieval relevance judgments binary relevant vs non relevant in the simplest case more nuanced relevance levels also used what are some issues already million times k takes us into the range of a quarter trillion judgments if each judgment took a human seconds we d still need seconds or nearly million if you pay people per hour to assess k new products per day
IR_ch8,9,introduction to information retrieval crowd source relevance judgments present query document pairs to low cost labor on online crowd sourcing platforms hope that this is cheaper than hiring qualified assessors lots of literature on using crowd sourcing for such tasks you get fairly good signal but the variance in the resulting judgments is quite high
IR_ch8,10,introduction to information retrieval sec what else still need test queries must be germane to docs available must be representative of actual user needs random query terms from the documents are not a good idea sample from query logs if available classically non web low query rates not enough query logs experts hand craft user needs
IR_ch8,11,introduction to information retrieval sec early public test collections th c typical trec recent datasets s of million web pages gov clueweb
IR_ch8,12,introduction to information retrieval now we have the basics of a benchmark let s review some evaluation measures precision recall dcg
IR_ch8,13,introduction to information retrieval sec evaluating an ir system note user need is translated into a query relevance is assessed relative to the user need not the query e g information need my swimming pool bottom is becoming black and needs to be cleaned query pool cleaner assess whether the doc addresses the underlying need not whether it has these words
IR_ch8,14,introduction to information retrieval sec unranked retrieval evaluation precision and recall recap from iir video binary assessments precision fraction of retrieved docs that are relevant p relevant retrieved recall fraction of relevant docs that are retrieved p retrieved relevant relevant nonrelevant retrieved tp fp not retrieved fn tn precision p tp tp fp recall r tp tp fn
IR_ch8,15,introduction to information retrieval rank based measures binary relevance precision k p k mean average precision map mean reciprocal rank mrr multiple levels of relevance normalized discounted cumulative gain ndcg
IR_ch8,16,introduction to information retrieval precision k set a rank threshold k compute relevant in top k ignores documents ranked lower than k ex prec of prec of prec of in similar fashion we have recall k
IR_ch8,17,introduction to information retrieval sec a precision recall curve lots more detail on this in the canvas video n o i s i c e r p recall
IR_ch8,18,introduction to information retrieval mean average precision consider rank position of each relevant doc k k k r compute precision k for each k k k r average precision average of p k ex has avgprec of map is average precision across multiple queries rankings
IR_ch8,19,introduction to information retrieval average precision
IR_ch8,20,introduction to information retrieval map
IR_ch8,21,introduction to information retrieval mean average precision if a relevant document never gets retrieved we assume the precision corresponding to that relevant doc to be zero map is macro averaging each query counts equally now perhaps most commonly used measure in research papers good for web search map assumes user is interested in finding many relevant documents for each query map requires many relevance judgments in text collection
IR_ch8,22,introduction to information retrieval beyond binary relevance
IR_ch8,23,introduction to information retrieval fair fair good
IR_ch8,24,introduction to information retrieval discounted cumulative gain popular measure for evaluating web search and related tasks two assumptions highly relevant documents are more useful than marginally relevant documents the lower the ranked position of a relevant document the less useful it is for the user since it is less likely to be examined
IR_ch8,25,introduction to information retrieval discounted cumulative gain uses graded relevance as a measure of usefulness or gain from examining a document gain is accumulated starting at the top of the ranking and may be reduced or discounted at lower ranks typical discount is log rank with base the discount at rank is and at rank it is
IR_ch8,26,introduction to information retrieval summarize a ranking dcg what if relevance judgments are in a scale of r r cumulative gain cg at rank n let the ratings of the n documents be r r r n in ranked order cg r r r n discounted cumulative gain dcg at rank n dcg r r log r log r log n n we may use any base for the logarithm
IR_ch8,27,introduction to information retrieval discounted cumulative gain dcg is the total gain accumulated at a particular rank p alternative formulation used by some web search companies emphasis on retrieving highly relevant documents
IR_ch8,28,introduction to information retrieval dcg example ranked documents judged on relevance scale discounted gain dcg
IR_ch8,29,introduction to information retrieval ndcg for summarizing rankings normalized discounted cumulative gain ndcg at rank n normalize dcg at rank n by the dcg value at rank n of the ideal ranking the ideal ranking would first return the documents with the highest relevance level then the next highest relevance level etc normalization useful for contrasting queries with varying numbers of relevant results ndcg is now quite popular in evaluating web search
IR_ch8,30,introduction to information retrieval ndcg example ground truth ranking function ranking function i document document document r r r order i order i order i d d d d d d d d d d d d ndcg ndcg ndcg gt rf rf dcg gt log log log d d c c g g r r f f l o l o g g l o l o g g l o l o g g documents d d d d maxdcg dcg gt
IR_ch8,31,introduction to information retrieval what if the results are not in a list suppose there s only one relevant document scenarios known item search navigational queries looking for a fact search duration rank of the answer measures a user s effort
IR_ch8,32,introduction to information retrieval mean reciprocal rank consider rank position k of first relevant doc could be only clicked doc reciprocal rank score mrr is the mean rr across multiple queries k
IR_ch8,33,introduction to information retrieval human judgments are expensive inconsistent between raters over time decay in value as documents query mix evolves not always representative of real users rating vis à vis query don t know underlying need may not understand meaning of terms etc so what alternatives do we have
IR_ch8,34,introduction to information retrieval using user clicks
IR_ch8,35,introduction to information retrieval taken with slight adaptation from fan guo and chao liu s cikm tutorial statistical user behavior models for web search click log analysis search results for cikm in of clicks received
IR_ch8,36,introduction to information retrieval user behavior adapt ranking to user clicks of clicks received
IR_ch8,37,introduction to information retrieval what do clicks tell us tools needed for non trivial cases of clicks received strong position bias so absolute click rates unreliable
IR_ch8,38,introduction to information retrieval eye tracking user study
IR_ch8,39,introduction to information retrieval click position bias higher positions receive more user attention eye e g a t fixation and clicks than n e c lower positions r e p this is true even in the normal position extreme setting where the order of positions is e reversed g a t n e c r e p clicks are informative but biased reversed impression joachims
IR_ch8,40,introduction to information retrieval relative vs absolute ratings user s click sequence hard to conclude result result probably can conclude result result
IR_ch8,41,introduction to information retrieval evaluating pairwise relative ratings pairs of the form doca better than docb for a query doesn t mean that doca relevant to query now rather than assess a rank ordering wrt per doc relevance assessments assess in terms of conformance with historical pairwise preferences recorded from user clicks but don t learn and test on the same ranking algorithm i e if you learn historical clicks from nozama and compare sergey vs nozama on this history
IR_ch8,42,introduction to information retrieval comparing two rankings via clicks joachims query support vector machines ranking a ranking b kernel machines kernel machines svm light svms lucent svm demo intro to svms royal holl svm archives of svm svm software svm light svm tutorial svm software
IR_ch8,43,introduction to information retrieval interleave the two rankings kernel machines kernel machines svms this interleaving svm light starts with b intro to svms lucent svm demo archives of svm royal holl svm svm light
IR_ch8,44,introduction to information retrieval remove duplicate results kernel machines kernel machines svms svm light intro to svms lucent svm demo archives of svm royal holl svm svm light
IR_ch8,45,introduction to information retrieval count user clicks kernel machines a b kernel machines clicks svms ranking a svm light a ranking b intro to svms lucent svm demo a archives of svm royal holl svm svm light
IR_ch8,46,introduction to information retrieval interleaved ranking present interleaved ranking to users start randomly with ranking a or ranking b to even out presentation bias count clicks on results from a versus results from b better ranking will on average get more clicks
IR_ch8,47,introduction to information retrieval sec a b testing at web search engines purpose test a single innovation prerequisite you have a large search engine up and running have most users use old system divert a small proportion of traffic e g to an experiment to evaluate an innovation interleaved experiment full page experiment
IR_ch8,48,introduction to information retrieval facts entities what happens to clicks
IR_ch8,49,introduction to information retrieval recap benchmarks consist of document collection query set assessment methodology assessment methodology can use raters user clicks or a combination these get quantized into a goodness measure precision ndcg etc different engines algorithms compared on a benchmark together with a goodness measure
IR_ch8,50,introduction to information retrieval user behavior user behavior is an intriguing source of relevance data users make somewhat informed choices when they interact with search engines potentially a lot of data available in search logs but there are significant caveats user behavior data can be very noisy interpreting user behavior can be tricky spam can be a significant problem not all queries will have user behavior
IR_ch8,51,introduction to information retrieval incorporating user behavior into ranking algorithm incorporate user behavior features into a ranking function like bm f but requires an understanding of user behavior features so that appropriate v functions are used j incorporate user behavior features into learned ranking function either of these ways of incorporating user behavior signals improve ranking
NLP_ch1,1,introduction to nlp what is natural language processing
NLP_ch1,2,dan jurafsky ques on answering ibm s watson won jeopardy on february william wilkinson s an account of the principalities of wallachia and moldovia bram stoker inspired this author s most famous novel
NLP_ch1,3,dan jurafsky informa on extrac on event curriculum mtg date jan subject curriculum mee ng start am date january end am where gates to dan jurafsky hi dan we ve now scheduled the curriculum meeing it will be in gates tomorrow from chris create new calendar entry
NLP_ch1,4,dan jurafsky informa on extrac on sen ment analysis awributes zoom affordability size and weight flash ease of use size and weight nice and compact to carry since the camera is small and light i won t need to carry around those heavy bulky professional cameras either the camera feels flimsy is plasic and very light in weight you have to be very delicate in the handling of this camera
NLP_ch1,5,dan jurafsky machine transla on helping human translators fully automaic enter source text 这 不过 是 一 个 时间 的 问题 translaion from stanford s phrasal this is only a mawer of ime
NLP_ch1,6,dan jurafsky language technology making good progress seniment analysis sill really hard mostly solved best roast chicken in san francisco quesion answering qa the waiter ignored us for minutes q how effecive is ibuprofen in reducing spam detecion coreference resoluion fever in paients with acute febrile illness let s go to agra carter told mubarak he shouldn t run again paraphrase buy v agra word sense disambiguaion wsd xyz acquired abc yesterday abc has been taken over by xyz part of speech pos tagging i need new baweries for my mouse adj adj noun verb adv summarizaion colorless green ideas sleep furiously parsing the dow jones is up economy is the s p jumped i can see alcatraz from the window good housing prices rose named enity recogniion ner machine translaion mt person org loc 第 届上海国际电影节开幕 d ialog where is ciizen kane playing in sf einstein met with un officials in princeton the th shanghai internaional film fesival castro theatre at do informaion extracion ie you want a icket party you re invited to our dinner may party friday may at add
NLP_ch1,7,dan jurafsky ambiguity makes nlp hard crash blossoms violinist linked to jal crash blossoms teacher strikes idle kids red tape holds up new bridges hospitals are sued by foot doctors juvenile court to try shooing defendant local high school dropouts cut in half
NLP_ch1,8,dan jurafsky ambiguity is pervasive new york times headline may fed raises interest rates fed raises interest rates fed raises interest rates
NLP_ch1,9,dan jurafsky in video quizzes most lectures will include a liwle quiz just to check basic understanding simple muliple choice you can retake them if you get them wrong
NLP_ch1,10,dan jurafsky why else is natural language understanding difficult non standard english segmenta on issues idioms great job jusinbieber were dark horse soo proud of what youve the new york new haven railroad get cold feet accomplished u taught us the new york new haven railroad lose face neversaynever you yourself should never give up either throw in the towel world knowledge tricky en ty names neologisms unfriend mary and sue are sisters where is a bug s life playing retweet mary and sue are mothers let it be was recorded bromance a mutaion on the for gene but that s what makes it fun
NLP_ch1,11,dan jurafsky making progress on this problem the task is difficult what tools do we need knowledge about language knowledge about the world a way to combine knowledge sources how we generally do this probabilisic models built from language data p maison house high p l avocat général the general avocado low luckily rough text features can oven do half the job
NLP_ch1,12,dan jurafsky this class teaches key theory and methods for staisical nlp viterbi naïve bayes maxent classifiers n gram language modeling staisical parsing inverted index y idf vector models of meaning for pracical robust real world applicaions informaion extracion spelling correcion informaion retrieval seniment analysis
NLP_ch1,13,dan jurafsky skills you ll need simple linear algebra vectors matrices basic probability theory java or python programming weekly programming assignments
NLP_ch1,14,introduction to nlp what is natural language processing
NLP_ch10,1,relation extraction what is rela on extrac on
NLP_ch10,2,dan jurafsky extrac ng rela ons from text company report interna onal business machines corpora on ibm or the company was incorporated in the state of new york on june as the compu ng tabula ng recording co c t r extracted complex rela on company founding company ibm loca on new york date june original name compu ng tabula ng recording co but we will focus on the simpler task of extrac ng rela on triples founding year ibm founding loca on ibm new york
NLP_ch10,3,dan jurafsky extrac ng rela on triples from text the leland stanford junior university commonly referred to as stanford university or stanford is an american private research university located in stanford california near palo alto california leland stanford founded the university in stanford eq leland stanford junior university stanford loc in california stanford is a research university stanford loc near palo alto stanford founded in stanford founder leland stanford
NLP_ch10,4,dan jurafsky why rela on extrac on create new structured knowledge bases useful for any app augment current knowledge bases adding words to wordnet thesaurus facts to freebase or dbpedia support ques on answering the granddaughter of which actor starred in the movie e t acted in x e t is a y actor granddaughter of x y but which rela ons should we extract
NLP_ch10,5,dan jurafsky automated content extrac on ace relations from relation extraction task person general part physical social affiliation whole subsidiary lasting family near citizen personal geographical resident located ethnicity org location business religion origin org affiliation artifact investor founder student alum ownership user owner inventor employment manufacturer membership sports affiliation
NLP_ch10,6,dan jurafsky automated content extrac on ace physical located per gpe he was in tennessee part whole subsidiary org org xyz the parent company of abc person social family per per john s wife yoko org aff founder per org steve jobs co founder of apple
NLP_ch10,7,dan jurafsky umls unified medical language system en ty types rela ons injury disrupts physiological func on bodily loca on loca on of biologic func on anatomical structure part of organism pharmacologic substance causes pathological func on pharmacologic substance treats pathologic func on
NLP_ch10,8,dan jurafsky extrac ng umls rela ons from a sentence doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type diabetes ê echocardiography doppler diagnoses acquired stenosis
NLP_ch10,9,dan jurafsky databases of wikipedia rela ons wikipedia infobox rela ons extracted from infobox stanford state california stanford moao die lub der freiheit weht
NLP_ch10,10,dan jurafsky rela on databases that draw from wikipedia resource descrip on framework rdf triples subject predicate object golden gate park location san francisco dbpedia golden_gate_park dbpedia owl loca on dbpedia san_francisco dbpedia billion rdf triples from english wikipedia frequent freebase rela ons people person na onality loca on loca on contains people person profession people person place of birth biology organism_higher_classifica on film film genre
NLP_ch10,11,dan jurafsky ontological rela ons examples from the wordnet thesaurus is a hypernym subsump on between classes giraffe is a ruminant is a ungulate is a mammal is a vertebrate is a animal instance of rela on between individual and class san francisco instance of city
NLP_ch10,12,dan jurafsky how to build rela on extractors hand wriaen paaerns supervised machine learning semi supervised and unsupervised bootstrapping using seeds distant supervision unsupervised learning from the web
NLP_ch10,13,relation extraction what is rela on extrac on
NLP_ch10,14,relation extraction using paaerns to extract rela ons
NLP_ch10,15,dan jurafsky rules for extrac ng is a rela on early intui on from hearst agar is a substance prepared from a mixture of red algae such as gelidium for laboratory or industrial use what does gelidium mean how do you know
NLP_ch10,16,dan jurafsky rules for extrac ng is a rela on early intui on from hearst agar is a substance prepared from a mixture of red algae such as gelidium for laboratory or industrial use what does gelidium mean how do you know
NLP_ch10,17,dan jurafsky hearst s paterns for extrac ng is a rela ons hearst automa c acquisi on of hyponyms y such as x x and or x such y as x x or other y x and other y y including x y especially x
NLP_ch10,18,dan jurafsky hearst s paterns for extrac ng is a rela ons hearst patern example occurrences x and other y temples treasuries and other important civic buildings x or other y bruises wounds broken bones or other injuries y such as x the bow lute such as the bambara ndang such y as x such authors as herrick goldsmith and shakespeare y including x common law countries including canada and england y especially x european countries especially france england and spain
NLP_ch10,19,dan jurafsky extrac ng richer rela ons using rules intui on rela ons oben hold between specific en es located in organization location founded person organization cures drug disease start with named en ty tags to help extract rela on
NLP_ch10,20,dan jurafsky named en es aren t quite enough which rela ons hold between en es cure prevent drug cause disease
NLP_ch10,21,dan jurafsky what rela ons hold between en es founder investor member person organization employee president
NLP_ch10,22,dan jurafsky extrac ng richer rela ons using rules and named en es who holds what office in what organiza on person position of org george marshall secretary of state of the united states person named appointed chose etc person prep position truman appointed marshall secretary of state person be named appointed etc prep org position george marshall was named us secretary of state
NLP_ch10,23,dan jurafsky hand built paterns for rela ons plus human patterns tend to be high precision can be tailored to specific domains minus human patterns are often low recall a lot of work to think of all possible patterns don t want to have to do this for every relation we d like better accuracy
NLP_ch10,24,relation extraction using paaerns to extract rela ons
NLP_ch10,25,relation extraction supervised rela on extrac on
NLP_ch10,26,dan jurafsky supervised machine learning for rela ons choose a set of rela ons we d like to extract choose a set of relevant named en es find and label data choose a representa ve corpus label the named en es in the corpus hand label the rela ons between these en es break into training development and test train a classifier on the training set
NLP_ch10,27,dan jurafsky how to do classifica on in supervised rela on extrac on find all pairs of named en es usually in same sentence decide if en es are related if yes classify the rela on why the extra step faster classifica on training by elimina ng most pairs can use dis nct feature sets appropriate for each task
NLP_ch10,28,dan jurafsky automated content extrac on ace sub relations of relations from relation extraction task person general part physical social affiliation whole subsidiary lasting family near citizen personal geographical resident located ethnicity org location business religion origin org affiliation artifact investor founder student alum ownership user owner inventor employment manufacturer membership sports affiliation
NLP_ch10,29,dan jurafsky rela on extrac on classify the rela on between two en es in a sentence american airlines a unit of amr immediately matched the move spokesman tim wagner said employment family nil citizen inventor subsidiary founder
NLP_ch10,30,dan jurafsky word features for rela on extrac on american airlines a unit of amr immediately matched the move spokesman tim wagner said men on men on headwords of m and m and combina on airlines wagner airlines wagner bag of words and bigrams in m and m american airlines tim wagner american airlines tim wagner words or bigrams in par cular posi ons leb and right of m m m spokesman m said bag of words or bigrams between the two en es a amr of immediately matched move spokesman the unit
NLP_ch10,31,dan jurafsky named en ty type and men on level features for rela on extrac on american airlines a unit of amr immediately matched the move spokesman tim wagner said men on men on named en ty types m org m person concatena on of the two named en ty types org person en ty level of m and m name nominal pronoun m name it or he would be pronoun m name the company would be nominal
NLP_ch10,32,dan jurafsky parse features for rela on extrac on american airlines a unit of amr immediately matched the move spokesman tim wagner said men on men on base syntac c chunk sequence from one to the other np np pp vp np np cons tuent path through the tree from one to the other np é np é s é s ê np dependency path airlines matched wagner said
NLP_ch10,33,dan jurafsky gazeteer and trigger word features for rela on extrac on trigger list for family kinship terms parent wife husband grandparent etc from wordnet gazeaeer lists of useful geo or geopoli cal words country name list other sub en es
NLP_ch10,34,dan jurafsky american airlines a unit of amr immediately matched the move spokesman tim wagner said
NLP_ch10,35,dan jurafsky classifiers for supervised methods now you can use any classifier you like maxent naïve bayes svm train it on the training set tune on the dev set test on the test set
NLP_ch10,36,dan jurafsky evalua on of supervised rela on extrac on compute p r f for each rela on of correctly extracted relations p total of extracted relations pr f p r of correctly extracted relations r total of gold relations
NLP_ch10,37,dan jurafsky summary supervised rela on extrac on can get high accuracies with enough hand labeled training data if test similar enough to training labeling a large training set is expensive supervised models are briale don t generalize well to different genres
NLP_ch10,38,relation extraction supervised rela on extrac on
NLP_ch10,39,relation extraction semi supervised and unsupervised rela on extrac on
NLP_ch10,40,dan jurafsky seed based or bootstrapping approaches to rela on extrac on no training set maybe you have a few seed tuples or a few high precision paaerns can you use those seeds to do something useful bootstrapping use the seeds to directly learn to populate a rela on
NLP_ch10,41,dan jurafsky rela on bootstrapping hearst gather a set of seed pairs that have rela on r iterate find sentences with these pairs look at the context between or around the pair and generalize the context to create paaerns use the paaerns for grep for more pairs
NLP_ch10,42,dan jurafsky bootstrapping mark twain elmira seed tuple grep google for the environments of the seed tuple mark twain is buried in elmira ny x is buried in y the grave of mark twain is in elmira the grave of x is in y elmira is mark twain s final res ng place y is x s final res ng place use those paaerns to grep for new tuples iterate
NLP_ch10,43,dan jurafsky dipre extract author book pairs brin sergei extracting patterns and relations from the world wide web start with seeds author book isaac asimov the robots of dawn david brin star de rising james gleick chaos making a new science charles dickens great expecta ons find instances william shakespeare the comedy of errors the comedy of errors by william shakespeare was the comedy of errors by william shakespeare is the comedy of errors one of william shakespeare s earliest aaempts the comedy of errors one of william shakespeare s most extract paaerns group by middle take longest common prefix suffix x by y x one of y s now iterate finding new seeds that match the paaern
NLP_ch10,44,dan jurafsky snowball e agichtein and l gravano snowball extracting relations from large plain text collections icdl similar itera ve algorithm organiza on loca on of headquarters microsob redmond exxon irving ibm armonk group instances w similar prefix middle suffix extract paaerns but require that x and y be named en es and compute a confidence for each paaern organization s in headquarters location location in based organization
NLP_ch10,45,dan jurafsky distant supervision snow jurafsky ng learning syntac c paaerns for automa c hypernym discovery nips fei wu and daniel s weld autonomously seman fying wikipeida cikm mintz bills snow jurafsky distant supervision for rela on extrac on without labeled data acl combine bootstrapping with supervised learning instead of seeds use a large database to get huge of seed examples create lots of features from all these examples combine in a supervised classifier
NLP_ch10,46,dan jurafsky distant supervision paradigm like supervised classifica on uses a classifier with lots of features supervised by detailed hand created knowledge doesn t require itera vely expanding paaerns like unsupervised classifica on uses very large amounts of unlabeled data not sensi ve to genre issues in training corpus
NLP_ch10,47,dan jurafsky distantly supervised learning of rela on extrac on paterns for each rela on born in edwin hubble marshfield for each tuple in big database albert einstein ulm find sentences in large corpus hubble was born in marshfield einstein born ulm with both en es hubble s birthplace in marshfield extract frequent features per was born in loc parse words etc per born xxxx loc per s birthplace in loc train supervised classifier using p born in f f f f thousands of paaerns
NLP_ch10,48,dan jurafsky unsupervised rela on extrac on m banko m cararella s soderland m broadhead and o etzioni open informa on extrac on from the web ijcai open informa on extrac on extract rela ons from the web with no training data no list of rela ons use parsed data to train a trustworthy tuple classifier single pass extract all rela ons between nps keep if trustworthy assessor ranks rela ons based on text redundancy fci specializes in sobware development tesla invented coil transformer
NLP_ch10,49,dan jurafsky evalua on of semi supervised and unsupervised rela on extrac on since it extracts totally new rela ons from the web there is no gold set of correct instances of rela ons can t compute precision don t know which ones are correct can t compute recall don t know which ones were missed instead we can approximate precision only draw a random sample of rela ons from output check precision manually of correctly extracted relations in the sample ˆ p total of extracted relations in the sample can also compute precision at different levels of recall precision for top new rela ons top new rela ons top in each case taking a random sample of that set but no way to evaluate recall
NLP_ch10,50,relation extraction semi supervised and unsupervised rela on extrac on
NLP_ch11,1,maxent models and discrimina ve es ma on the maximum entropy model presenta on
NLP_ch11,2,christopher manning maximum entropy models an equivalent approach lots of distribu ons out there most of them very spiked specific overfit we want a distribu on which is uniform except in specific ways we require uniformity means high entropy we can search for distribu ons which have proper es we desire but also have high entropy ignorance is preferable to error and he is less remote from the truth who believes nothing than he who believes what is wrong thomas jefferson
NLP_ch11,3,christopher manning maximum entropy entropy the uncertainty of a distribu on quan fying uncertainty surprise event x h probability p x surprise log p x p heads entropy expected surprise over p a coin flip is most uncertain for a fair coin h p e log p log p p x x p x x
NLP_ch11,4,christopher manning maxent examples i what do we want from a distribu on minimize commitment maximize entropy resemble some reference distribu on data solu on maximize entropy h subject to feature based constraints unconstrained max at p c e f e f x i p i pˆ i x f i adding constraints features lowers maximum entropy raises maximum likelihood of data constraint that brings the distribu on further from uniform p brings the distribu on closer to data heads
NLP_ch11,5,christopher manning e maxent examples ii x log x h p p p p p h t h t h
NLP_ch11,6,christopher manning maxent examples iii let s say we have the following event space nn nns nnp nnps vbz vbd and the following empirical data maximize h e e e e e e want probabili es e nn nns nnp nnps vbz vbd
NLP_ch11,7,christopher manning maxent examples iv too uniform n are more common than v so we add the feature f nn nns nnp nnps n with e f n nn nns nnp nnps vbz vbd and proper nouns are more frequent than common nouns so we add f nnp p nnps with e f p we could keep refining the models e g by adding a feature to dis nguish singular vs plural nouns or verb types
NLP_ch11,8,christopher manning convexity f w x w f x w i i i i i i i i f wx w f x convex non convex convexity guarantees a single global maximum because any higher points are greedily reachable
NLP_ch11,9,christopher manning convexity ii constrained h p x log x is convex x log x is convex x log x is convex sum of convex func ons is convex the feasible region of constrained h is a linear subspace which is convex the constrained entropy surface is therefore convex the maximum likelihood exponen al model dual formula on is also convex
NLP_ch11,10,maxent models and discrimina ve es ma on the maximum entropy model presenta on
NLP_ch11,11,feature overlap feature interac on how overlapping features work in maxent models
NLP_ch11,12,christopher manning feature overlap maxent models handle overlapping features well unlike a nb model there is no double coun ng a a a a a a b b b empirical b b b a a all a a b a a a a a a b b b b b b b a a a a a a b b λ b λʼ λʼʼ a a a b b λ b λʼ λʼʼ a a a
NLP_ch11,13,christopher manning example named en ty feature overlap feature weights grace is correlated with feature type feature pers loc person but does not add previous word at much evidence on top of current word grace already knowing prefix beginning bigram g features current pos tag nnp prev and cur tags in nnp local context previous state other prev cur next current signature xx state other prev state cur sig o xx prev cur next sig x xx xx word at grace road p state p cur sig o x xx tag in nnp nnp sig x xx xx total
NLP_ch11,14,christopher manning feature interac on maxent models handle overlapping features well but do not automatically model feature interactions a a a a a a b b b empirical b b b a a all a b b a a a a a a b b b b b b b a a a a a a b b λ b λ λ λ a a b b b b λ b λ a a
NLP_ch11,15,christopher manning feature interac on if you want interaction terms you have to add them a a a a a a b b b empirical b b b a a a b ab b a a a a a a b b b b b b b a disjunctive feature would also have done it alone a a a a b b b b
NLP_ch11,16,christopher manning quiz question suppose we have a feature maxent model built over observed data as shown what is the constructed model s probability distribution over the four possible outcomes features expectations probabilities empirical a a a a a a b b b b b b
NLP_ch11,17,christopher manning feature interac on for loglinear logis c regression models in sta s cs it is standard to do a greedy stepwise search over the space of all possible interac on terms this combinatorial space is exponen al in size but that s okay as most sta s cs models only have features in nlp our models commonly use hundreds of thousands of ʼ features so that s not okay commonly interac on terms are added by hand based on linguis c intui ons
NLP_ch11,18,christopher manning example ner interac on previous state and current signature feature weights have interactions e g p pers c xx feature type feature pers loc indicates c pers much more strongly than c xx and p pers independently previous word at current word grace this feature type allows the model to capture this interaction beginning bigram g current pos tag nnp prev and cur tags in nnp local context previous state other current signature xx prev cur next prev state cur sig o xx state other prev cur next sig x xx xx word at grace road p state p cur sig o x xx tag in nnp nnp sig x xx xx total
NLP_ch11,19,feature overlap feature interac on how overlapping features work in maxent models
NLP_ch11,20,condi onal maxent models for classifica on the rela onship between condi onal and joint maxent exponen al models
NLP_ch11,21,christopher manning classifica on p x what do these joint models of have to do with conditional p c d models x c d x think of the space as a complex c is generally small e g topic classes d is generally huge e g space of documents p c d we can in principle build models over c c d this will involve calculating expectations of features over c d e f p c d f c d d i i c d c d x generally impractical can t enumerate efficiently
NLP_ch11,22,christopher manning classifica on ii d may be huge or infinite but only a few d occur in our data what if we add one feature for each d and constrain its c expecta on to match our empirical data ˆ d d p d p d d now most entries of p c d will be zero we can therefore use the much easier sum e f p c d f c d i i c d c d p c d f c d ˆ i c d c d p d
NLP_ch11,23,christopher manning classifica on iii but if we ve constrained the d marginals ˆ d d p d p d then the only thing that can vary is the condi onal distribu ons p c d p c d p d ˆ p c d p d
NLP_ch11,24,christopher manning classifica on iv this is the connec on between joint and condi onal maxent exponen al models condi onal models can be thought of as joint models with marginal constraints maximizing joint likelihood and condi onal likelihood of the data in this model are equivalent
NLP_ch11,25,condi onal maxent models for classifica on the rela onship between condi onal and joint maxent exponen al models
NLP_ch11,26,smoothing priors regulariza on for maxent models
NLP_ch11,27,christopher manning smoothing issues of scale lots of features nlp maxent models can have well over a million features even storing a single array of parameter values can have a substan al memory cost lots of sparsity overfirng very easy we need smoothing many features seen in training will never occur again at test me op miza on problems feature weights can be infinite and itera ve solvers can take a long me to get to those infini es
NLP_ch11,28,christopher manning smoothing issues assume the following empirical distribution heads tails h t features heads tails we ll have the following model distribution e h e t p p heads e h e t tails e h e t really only one degree of freedom λ λ λ h t e he t e e p p heads e he t e te t e e tails e e λ
NLP_ch11,29,christopher manning smoothing issues the data likelihood in this model is log p h t hlog p t log p heads tails log p h t h t h log e log p log p log p λ λ λ heads tails heads tails heads tails
NLP_ch11,30,christopher manning smoothing early stopping in the case there were two problems the op mal value of λwas which is a long trip for an op miza on procedure the learned distribu on is just as spiked as the empirical one no smoothing λ one way to solve both issues is to just stop the op miza on heads tails early ater a few itera ons the value of λ will be finite but presumably big input the op miza on won t take forever clearly heads tails commonly used in early maxent work output
NLP_ch11,31,christopher manning smoothing priors map what if we had a prior expecta on that parameter values wouldn t be very large we could then balance evidence sugges ng large parameters or infinite against our prior the evidence would never totally defeat the prior and parameters would be smoothed and kept finite we can do this explicitly by changing the op miza on objec ve to maximum posterior likelihood log p c d log p log p c d posterior prior evidence
NLP_ch11,32,christopher manning smoothing priors σ gaussian or quadra c or l priors σ intui on parameters shouldn t be large formaliza on prior expecta on that each parameter will σ be distributed according to a gaussian with mean µ and variance σ µ they don t even p exp i i i capitalize my i i name anymore penalizes parameters for driting to far from their mean prior value usually µ σ works surprisingly well
NLP_ch11,33,christopher manning smoothing priors if we use gaussian priors trade off some expecta on matching for smaller parameters when mul ple features can be recruited to explain a data point the more common ones generally receive more weight accuracy generally goes up σ change the objec ve σ log p c d log p c d logp σ µ log p c d p c d i i k c d c d i i change the deriva ve log p c d actual f c predicted f µ i i i i i
NLP_ch11,34,christopher manning smoothing priors if we use gaussian priors trade off some expecta on matching for smaller parameters when mul ple features can be recruited to explain a data point the more common ones generally receive more weight accuracy generally goes up σ change the objec ve σ log p c d log p c d logp σ log p c d p c d i k c d c d i i change the deriva ve taking prior log p c d actual f c predicted f mean as i i i i
NLP_ch11,35,christopher manning example ner smoothing feature weights because of smoothing the more feature type feature pers loc common prefix and single tag previous word at features have larger weights even current word grace though en re word and tag pair beginning bigram g features are more specific current pos tag nnp prev and cur tags in nnp local context previous state other prev cur next current signature xx state other prev state cur sig o xx prev cur next sig x xx xx word at grace road p state p cur sig o x xx tag in nnp nnp sig x xx xx total
NLP_ch11,36,christopher manning example pos tagging from toutanova et al overall unknown accuracy word acc without smoothing with smoothing smoothing helps sotens distribu ons pushes weight onto more explanatory features allows many features to be dumped safely into the mix speeds up convergence if both are allowed to converge
NLP_ch11,37,christopher manning smoothing regulariza on talking of priors and map es ma on is bayesian language in frequen st sta s cs people will instead talk about using regulariza on and in par cular a gaussian prior is l regulariza on the choice of names makes no difference to the math
NLP_ch11,38,christopher manning smoothing virtual data another op on smooth the data not the parameters example heads tails heads tails equivalent to adding two extra data points similar to add one smoothing for genera ve models hard to know what ar ficial data to create
NLP_ch11,39,christopher manning smoothing count cutoffs in nlp features with low empirical counts are oten dropped very weak and indirect smoothing method equivalent to locking their weight to be zero equivalent to assigning them gaussian priors with mean zero and variance zero dropping low counts does remove the features which were most in need of smoothing and speeds up the es ma on by reducing model size but count cutoffs generally hurt accuracy in the presence of proper smoothing we recommend don t use count cutoffs unless absolutely necessary for memory usage reasons
NLP_ch11,40,smoothing priors regulariza on for maxent models
NLP_ch12,1,sta s cal natural language parsing two views of syntac c structure
NLP_ch12,2,christopher manning two views of linguis c structure cons tuency phrase structure phrase structure organizes words into nested cons tuents how do we know what is a cons tuent not that linguists don t argue about some cases distribu on a cons tuent behaves as a unit that can appear in different places john talked to the children about drugs john talked about drugs to the children john talked drugs to the children about subs tu on expansion pro forms i sat on the box right on top of the box there coordina on regular internal structure no intrusion fragments seman cs
NLP_ch12,3,christopher manning two views of linguis c structure cons tuency phrase structure phrase structure organizes words into nested cons tuents how do we know what is a cons tuent not that linguists don t argue about some cases distribu on a cons tuent behaves as a unit that can appear in different places john talked to the children about drugs john talked about drugs to the children john talked drugs to the children about subs tu on expansion pro forms i sat on the box right on top of the box there coordina on regular internal structure no intrusion fragments seman cs
NLP_ch12,4,christopher manning
NLP_ch12,5,christopher manning headed phrase structure vp vb np nn adjp jj advp rb sbar q s sinv sq np vp plus minor phrase types qp quan fier phrase in np conjp mul word construc ons as well as intj interjec ons etc
NLP_ch12,6,christopher manning two views of linguis c structure dependency structure dependency structure shows which words depend on modify or are arguments of which other words put boy tortoise on the the rug the boy put the tortoise on the rug the
NLP_ch12,7,christopher manning two views of linguis c structure dependency structure dependency structure shows which words depend on modify or are arguments of which other words the boy put the tortoise on the rug
NLP_ch12,8,sta s cal natural language parsing two views of syntac c structure
NLP_ch12,9,sta s cal natural language parsing parsing the rise of data and sta s cs
NLP_ch12,10,christopher manning pre classical nlp parsing wrote symbolic grammar cfg or o en richer and lexicon s np vp nn interest np dt nn nns rates np nn nns nns raises np nnp vbp interest vp v np vbz rates used grammar proof systems to prove parses from words this scaled very badly and didn t give coverage for sentence fed raises interest rates in effort to control infla on minimal grammar parses simple rule grammar parses real size broad coverage grammar millions of parses
NLP_ch12,11,christopher manning classical nlp parsing the problem and its solu on categorical constraints can be added to grammars to limit unlikely weird parses for sentences but the ajempt make the grammars not robust in tradi onal systems commonly of sentences in even an edited text would have no parse a less constrained grammar can parse more sentences but simple sentences end up with ever more parses with no way to choose between them we need mechanisms that allow us to find the most likely parse s for a sentence sta s cal parsing lets us work with very loose grammars that admit millions of parses for sentences but s ll quickly find the best parse s
NLP_ch12,12,christopher manning the rise of annotated data the penn treebank marcus et al computational linguistics s np sbj dt the nn move vp vbd followed np np dt a nn round pp in of np np jj similar nns increases pp in by np jj other nns lenders pp in against np nnp arizona jj real nn estate nns loans s adv np sbj none vp vbg reflec ng np np dt a vbg con nuing nn decline pp loc in in np dt that nn market
NLP_ch12,13,christopher manning the rise of annotated data star ng off building a treebank seems a lot slower and less useful than building a grammar but a treebank gives us many things reusability of the labor many parsers pos taggers etc valuable resource for linguis cs broad coverage frequencies and distribu onal informa on a way to evaluate systems
NLP_ch12,14,christopher manning sta s cal parsing applica ons sta s cal parsers are now robust and widely used in larger nlp applica ons high precision ques on answering pasca and harabagiu sigir improving biological named en ty finding finkel et al jnlpba syntac cally based sentence compression lin and wilbur extrac ng opinions about products bloom et al naacl improved interac on in computer games gorniak and roy helping linguists find data resnik et al bls source sentence analysis for machine transla on xu et al rela on extrac on systems fundel et al bioinforma cs
NLP_ch12,15,sta s cal natural language parsing parsing the rise of data and sta s cs
NLP_ch12,16,sta s cal natural language parsing an exponen al number of ajachments
NLP_ch12,17,christopher manning akachment ambigui es a key parsing decision is how we ajach various cons tuents pps adverbial or par cipial phrases infini ves coordina ons etc catalan numbers c n n n n an exponentially growing series which arises in many tree like contexts e g the number of possible triangulations of a polygon with n sides turns up in triangulation of probabilistic graphical models
NLP_ch12,18,christopher manning akachment ambigui es a key parsing decision is how we ajach various cons tuents pps adverbial or par cipial phrases infini ves coordina ons etc catalan numbers c n n n n an exponentially growing series which arises in many tree like contexts e g the number of possible triangulations of a polygon with n sides turns up in triangulation of probabilistic graphical models
NLP_ch12,19,christopher manning quiz question how many distinct parses does the following sentence have due to pp attachment ambiguities a pp can attach to any preceding v or n within the verb phrase subject only to the parse still being a tree this is equivalent to there being no crossing dependencies where if d is a dependent of d and d is a dependent of d then the line d d begins at d under the line from d to d john wrote the book with a pen in the room
NLP_ch12,20,christopher manning two problems to solve repeated work
NLP_ch12,21,christopher manning two problems to solve repeated work
NLP_ch12,22,christopher manning two problems to solve choosing the correct parse how do we work out the correct ajachment she saw the man with a telescope is the problem ai complete yes but words are good predictors of ajachment even absent full understanding moscow sent more than soldiers into afghanistan sydney water breached an agreement with nsw health our sta s cal parsers will try to exploit such sta s cs
NLP_ch12,23,sta s cal natural language parsing an exponen al number of ajachments
NLP_ch13,1,cfgs and pcfgs probabilis c context free grammars
NLP_ch13,2,christopher manning a phrase structure grammar s np vp n people vp v np n fish vp v np pp n tanks np np np n rods np np pp v people np n v fish np e v tanks pp p np p with people fish tanks people fish with rods
NLP_ch13,3,christopher manning phrase structure grammars context free grammars cfgs g t n s r t is a set of terminal symbols n is a set of nonterminal symbols s is the start symbol s n r is a set of rules produc ons of the form x γ x n and γ n t a grammar g generates a language l
NLP_ch13,4,christopher manning phrase structure grammars in nlp g t c n s l r t is a set of terminal symbols c is a set of preterminal symbols n is a set of nonterminal symbols s is the start symbol s n l is the lexicon a set of items of the form x x x p and x t r is the grammar a set of items of the form x γ x n and γ n c by usual conven on s is the start symbol but in sta s cal nlp we usually have an extra node at the top root top we usually write e for an empty sequence rather than nothing
NLP_ch13,5,christopher manning a phrase structure grammar s np vp n people vp v np n fish vp v np pp n tanks np np np n rods np np pp v people np n v fish np e v tanks pp p np p with people fish tanks people fish with rods
NLP_ch13,6,christopher manning probabilis c or stochas c context free grammars pcfgs g t n s r p t is a set of terminal symbols n is a set of nonterminal symbols s is the start symbol s n r is a set of rules produc ons of the form x γ p is a probability func on p r x n p x x r a grammar g generates a language model l p t
NLP_ch13,7,christopher manning a pcfg s np vp n people vp v np n fish vp v np pp n tanks np np np n rods np np pp v people np n v fish pp p np v tanks p with with empty np removed so less ambiguous
NLP_ch13,8,christopher manning the probability of trees and strings p t the probability of a tree t is the product of the probabilities of the rules used to generate it p s the probability of the string s is the sum of the probabilities of the trees which have that string as their yield p s σ p s t where t is a parse of s j σ p t j
NLP_ch13,9,christopher manning
NLP_ch13,10,christopher manning
NLP_ch13,11,christopher manning tree and string probabili es s people fish tanks with rods p t verb attach p t noun attach p s p t p t
NLP_ch13,12,christopher manning
NLP_ch13,13,christopher manning
NLP_ch13,14,cfgs and pcfgs probabilis c context free grammars
NLP_ch13,15,grammar transforms restric ng the grammar form for efficient parsing
NLP_ch13,16,christopher manning chomsky normal form all rules are of the form x y z or x w x y z n and w t a transforma on to this form doesn t change the weak genera ve capacity of a cfg that is it recognizes the same language but maybe with different trees emp es and unaries are removed recursively n ary rules are divided by introducing new nonterminals n
NLP_ch13,17,christopher manning a phrase structure grammar s np vp n people vp v np n fish vp v np pp n tanks np np np n rods np np pp v people np n v fish np e v tanks pp p np p with
NLP_ch13,18,christopher manning chomsky normal form steps s np vp n people s vp n fish vp v np n tanks vp v n rods vp v np pp v people vp v pp np np np v fish np np v tanks np np pp p with np pp np n pp p np pp p
NLP_ch13,19,christopher manning chomsky normal form steps s np vp n people vp v np n fish s v np vp v n tanks s v n rods vp v np pp v people s v np pp vp v pp v fish s v pp v tanks np np np np np p with np np pp np pp np n pp p np pp p
NLP_ch13,20,christopher manning chomsky normal form steps s np vp n people vp v np n fish s v np vp v n tanks vp v np pp n rods s v np pp v people vp v pp s v pp s people np np np v fish np np np np pp s fish np pp v tanks np n s tanks pp p np pp p p with
NLP_ch13,21,christopher manning chomsky normal form steps s np vp n people vp v np n fish s v np n tanks vp v np pp n rods s v np pp v people vp v pp s people s v pp vp people np np np v fish np np s fish np np pp vp fish np pp v tanks np n s tanks pp p np vp tanks pp p p with
NLP_ch13,22,christopher manning chomsky normal form steps s np vp np people vp v np np fish s v np np tanks vp v np pp np rods s v np pp v people vp v pp s people s v pp vp people np np np v fish np np pp s fish np p np vp fish pp p np v tanks s tanks vp tanks p with pp with
NLP_ch13,23,christopher manning chomsky normal form steps s np vp np people vp v np np fish s v np np tanks vp v vp_v np rods vp_v np pp v people s v s_v s people s_v np pp vp people vp v pp v fish s v pp s fish np np np vp fish np np pp v tanks np p np s tanks pp p np vp tanks p with pp with
NLP_ch13,24,christopher manning a phrase structure grammar s np vp n people vp v np n fish vp v np pp n tanks np np np n rods np np pp v people np n v fish np e v tanks pp p np p with
NLP_ch13,25,christopher manning chomsky normal form steps s np vp np people vp v np np fish s v np np tanks vp v vp_v np rods vp_v np pp v people s v s_v s people s_v np pp vp people vp v pp v fish s v pp s fish np np np vp fish np np pp v tanks np p np s tanks pp p np vp tanks p with pp with
NLP_ch13,26,christopher manning chomsky normal form you should think of this as a transforma on for efficient parsing with some extra book keeping in symbol names you can even reconstruct the same trees with a detransform in prac ce full chomsky normal form is a pain reconstruc ng n aries is easy reconstruc ng unaries emp es is trickier binariza on is crucial for cubic me cfg parsing the rest isn t necessary it just makes the algorithms cleaner and a bit quicker
NLP_ch13,27,christopher manning an example before binarization root s vp np v np pp n p np n n people fish tanks with rods
NLP_ch13,28,christopher manning after binarization root s vp np vp_v v np pp n n p np n people fish tanks with rods
NLP_ch13,29,christopher manning treebank empties and unaries root root root root root s hln s s s np subj vp np vp vp none vb none vb vb vb e e atone atone atone atone atone high low ptb tree nofunctags noempties nounaries
NLP_ch13,30,christopher manning unary rules alchemy in the land of treebanks
NLP_ch13,31,christopher manning same span reachability noempties top sq x rrc nx lst adjp advp frag intj np conjp pp prn qp s nac sbar ucp vp whnp sinv prt sbarq whadjp whpp whadvp
NLP_ch13,32,grammar transforms restric ng the grammar form for efficient parsing
NLP_ch13,33,cky parsing exact polynomial me parsing of p cfgs
NLP_ch13,34,christopher manning cons tuency parsing pcfg rule prob θ i s np vp θ s np np np θ vp np np n fish θ n people θ n n v n v fish θ fish people fish tanks
NLP_ch13,35,christopher manning cocke kasami younger cky cons tuency parsing fish people fish tanks
NLP_ch13,36,christopher manning viterbi max scores np nn nns i np s vp np nnp nns i vp np np np v v n n people fish
NLP_ch13,37,christopher manning viterbi max scores s np vp s vp vp v np vp v vp np vp v vp_v np v vp v pp v n vp_v np pp n np np np np np pp np n people fish pp p np
NLP_ch13,38,christopher manning extended cky parsing unaries can be incorporated into the algorithm messy but doesn t increase algorithmic complexity emp es can be incorporated use fenceposts doesn t increase complexity essen ally like unaries binariza on is vital without binariza on you don t get parsing cubic in the length of the sentence and in the number of nonterminals in the grammar binariza on may be an explicit transforma on or implicit in how the parser works early style doted rules but it s always there
NLP_ch13,39,christopher manning the cky algorithm extended to unaries function cky words grammar returns most_probable_parse prob score new double words words nonterms back new pair words words nonterms for i i words i for a in nonterms if a words i in grammar score i i a p a words i handle unaries boolean added true while added added false for a b in nonterms if score i i b a b in grammar prob p a b score i i b if prob score i i a score i i a prob back i i a b added true
NLP_ch13,40,christopher manning the cky algorithm extended to unaries for span to words for begin to words span end begin span for split begin to end for a b c in nonterms prob score begin split b score split end c p a bc if prob score begin end a score begin end a prob back begin end a new triple split b c handle unaries boolean added true while added added false for a b in nonterms prob p a b score begin end b if prob score begin end a score begin end a prob back begin end a b added true return buildtree score back
NLP_ch13,41,christopher manning quiz question pp in np nns nns np nns np np nns pp vp vb pp vp vb np pp nns in vb what nns constituents with what probability can you make runs down
NLP_ch13,42,cky parsing exact polynomial me parsing of p cfgs
NLP_ch13,43,cky parsing a worked example
NLP_ch13,44,christopher manning the grammar binary no epsilons s np vp n people s vp n fish vp v np n tanks vp v n rods vp v vp_v v people vp v pp vp_v np pp v fish np np np v tanks np np pp p with np n pp p np
NLP_ch13,45,fish people fish tanks score score score score score score score score score score
NLP_ch13,46,fish people fish tanks s np vp s vp vp v np vp v vp v vp_v vp v pp vp_v np pp np np np np np pp np n pp p np n people n fish n tanks for i i words i n rods for a in nonterms v people if a words i in grammar score i i a p a words i v fish v tanks p with
NLP_ch13,47,fish people fish tanks s np vp n fish s vp v fish vp v np vp v vp v vp_v vp v pp n people vp_v np pp v people np np np np np pp np n n fish pp p np v fish n people n fish handle unaries boolean added true n tanks while added n tanks n rods added false v tanks for a b in nonterms v people if score i i b a b in grammar prob p a b score i i b v fish if prob score i i a v tanks score i i a prob back i i a b p with added true
NLP_ch13,48,fish people fish tanks s np vp n fish s vp v fish vp v np np n vp v vp v vp v vp_v s vp vp v pp n people vp_v np pp v people np np np np n vp v np np pp s vp np n n fish pp p np v fish np n n people vp v n fish s vp n tanks n tanks n rods prob score begin split b score split end c p a bc v tanks if prob score begin end a v people score begin end a prob np n back begin end a new triple split b c vp v v fish s vp v tanks p with
NLP_ch13,49,fish people fish tanks s np vp n fish np np np s vp v fish vp v np vp v np np n vp v vp v s np vp vp v vp_v s vp vp v pp n people np np np vp_v np pp v peo ple vp v np np np np n p n vp v np np pp s np vp s vp np n n fish np np np pp p np v fish vp v np np n n people vp v handle unaries s np vp n fish boolean added true s vp while added n tanks added false n tanks for a b in nonterms n rods v tanks prob p a b score begin end b v people if prob score begin end a np n score begin end a prob vp v v fish back begin end a b s vp v tanks added true p with
NLP_ch13,50,fish people fish tanks s np vp n fish np np np s vp v fish vp v np vp v np np n vp v vp v s vp vp v vp_v s vp vp v pp n people np np np vp_v np pp v peo ple vp v np np np np n p n vp v np np pp s np vp s vp np n n fish np np np pp p np v fish vp v np np n n people vp v s vp n fish s vp n tanks n tanks n rods for split begin to end v tanks for a b c in nonterms v people prob score begin split b score split end c p a bc np n if prob score begin end a vp v v fish score begin end a prob s vp v tanks back begin end a new triple split b c p with
NLP_ch13,51,fish people fish tanks s np vp n fish np np np np np np s vp v fish vp v np vp v np vp v np np n vp v vp v s vp s np vp vp v vp_v s vp vp v pp n people np np np vp_v np pp v peo ple vp v np np np np n p n vp v np np pp s np vp s vp np n n fish np np np pp p np v fish vp v np np n n people vp v s vp n fish s vp n tanks n tanks n rods for split begin to end v tanks for a b c in nonterms v people prob score begin split b score split end c p a bc np n if prob score begin end a vp v v fish score begin end a prob s vp v tanks back begin end a new triple split b c p with
NLP_ch13,52,fish people fish tanks s np vp n fish np np np np np np s vp v fish vp v np vp v np vp v np np n vp v vp v s vp s np vp vp v vp_v s vp vp v pp n people np np np np np np vp_v np pp v peo ple vp v np vp v np np np np n p n vp v np np pp s np vp s np vp s vp np n n fish np np np pp p np v fish v p v np np n n people vp v s vp n fish s vp n tanks n tanks n rods for split begin to end v tanks for a b c in nonterms v people prob score begin split b score split end c p a bc np n if prob score begin end a vp v v fish score begin end a prob s vp v tanks back begin end a new triple split b c p with
NLP_ch13,53,fish people fish tanks s np vp n fish np np np np np np np np np s vp v fish vp v np vp v np vp v np vp v np np n vp v vp v s vp s np vp s np vp vp v vp_v s vp vp v pp n people np np np n p np np vp_v np pp v peo ple vp v np vp v np np np np n p n vp v np np pp s np vp s np vp s vp np n n fish np np np pp p np v fish v p v np np n n people vp v s vp n fish s vp n tanks n tanks n rods v tanks v people np n vp v v fish s vp v tanks p with call buildtree score back to get the best parse
NLP_ch13,54,cky parsing a worked example
NLP_ch13,55,cons tuency parser evalua on
NLP_ch13,56,christopher manning evalua ng cons tuency parsing
NLP_ch13,57,christopher manning evalua ng cons tuency parsing gold standard brackets s np vp vp np pp np np candidate brackets s np vp vp np pp np labeled precision labeled recall lp lr f tagging accuracy
NLP_ch13,58,christopher manning how good are pcfgs penn wsj parsing accuracy about lp lr f robust usually admit everything but with low probability partial solution for grammar ambiguity a pcfg gives some idea of the plausibility of a parse but not so good because the independence assumptions are too strong give a probabilistic language model but in the simple case it performs worse than a trigram model the problem seems to be that pcfgs lack the lexicalization of a trigram model
NLP_ch13,59,cons tuency parser evalua on
NLP_ch14,1,lexicaliza on of pcfgs introduc on christopher manning
NLP_ch14,2,christopher manning head lexicaliza on of pcfgs magerman collins charniak the head word of a phrase gives a good representa on of the phrase s structure and meaning puts the proper es of words back into a pcfg
NLP_ch14,3,christopher manning head lexicaliza on of pcfgs magerman collins charniak the head word of a phrase gives a good representa on of the phrase s structure and meaning puts the proper es of words back into a pcfg
NLP_ch14,4,christopher manning head lexicaliza on of pcfgs magerman collins charniak the head word of a phrase gives a good representa on of the phrase s structure and meaning puts the proper es of words back into a pcfg
NLP_ch14,5,christopher manning head lexicaliza on of pcfgs magerman collins charniak word to word affini es are useful for certain ambigui es pp akachment is now partly captured in a local pcfg rule think about what useful informa on isn t captured vp vp np pp np pp announce rates for january announce rates in january also useful for coordina on scope verb complement pakerns
NLP_ch14,6,christopher manning lexicalized parsing was seen as the parsing breakthrough of the late s eugene charniak jhu workshop to do beker it is necessary to condi on probabili es on the actual words of the sentence this makes the probabili es much ghter p vp v np np p vp v np np said p vp v np np gave michael collins colt tutorial lexicalized probabilis c context free grammars perform vastly beker than pcfgs vs accuracy
NLP_ch14,7,lexicaliza on of pcfgs introduc on christopher manning
NLP_ch14,8,lexicaliza on of pcfgs the model of charniak
NLP_ch14,9,christopher manning charniak a very straighgorward model of a lexicalized pcfg probabilis c condi oning is top down like a regular pcfg but actual parsing is bokom up somewhat like the cky algorithm we saw
NLP_ch14,10,christopher manning charniak example
NLP_ch14,11,christopher manning lexicaliza on models argument selec on by sharpening rule expansion probabili es the probability of different verbal complement frames i e subcategoriza ons depends on the verb local tree come take think want vp v vp v np vp v pp vp v sbar vp v s vp v np s vp v prt np vp v prt pp monolexical probabilities
NLP_ch14,12,christopher manning lexicaliza on sharpens probabili es predic ng heads bilexical probabili es p prices n plural p prices n plural np p prices n plural np s p prices n plural np s v past p prices n plural np s v past fell
NLP_ch14,13,christopher manning charniak linear interpola on shrinkage
NLP_ch14,14,christopher manning charniak shrinkage example
NLP_ch14,15,lexicaliza on of pcfgs the model of charniak
NLP_ch14,16,christopher manning sparseness the penn treebank the penn treebank million words of parsed english wsj has been a key resource because of the widespread reliance on supervised learning but million words is like nothing constituents but only whadjp of which only aren t how much or how many but there is an infinite space of these how clever original incompetent at risk assessment and evaluation most of the probabilities that you would like to compute you can t compute
NLP_ch14,17,christopher manning quiz question classify each of the italic red phrases as a whnp whadjp whadvp whpp that explains why she is succeeding which student scored highest on the assignment nobody knows how deep the recession will be during which class did the slide projection not work whose iphone was stolen
NLP_ch14,18,christopher manning sparseness the penn treebank many parse preferences depend on bilexical statistics likelihoods of relationships between pairs of words compound nouns pp attachments extremely sparse even on topics central to the wsj stocks plummeted occurrences stocks stabilized occurrence stocks skyrocketed occurrences stocks discussed occurrences there has been only modest success in augmenting the penn treebank with extra unannotated materials or using semantic classes given a reasonable amount of annotated training data cf charniak charniak but mcclosky et al doing self training and koo and collins semantic classes are rather more successful
NLP_ch14,19,pcfg independence assump ons
NLP_ch14,20,christopher manning pcfgs and independence the symbols in a pcfg define independence assump ons s s np vp np np vp np dt nn at any node the material inside that node is independent of the material outside that node given the label of that node any informa on that sta s cally connects behavior inside and outside a node must flow through that node s label
NLP_ch14,21,christopher manning non independence i the independence assump ons of a pcfg are oqen too strong all nps nps under s nps under vp np pp dt nn prp np pp dt nn prp np pp dt nn prp example the expansion of an np is highly dependent on the parent of the np i e subjects vs objects
NLP_ch14,22,christopher manning non independence ii symptoms of overly strong assump ons rewrites get used where they don t belong in the ptb this construction is for possessives
NLP_ch14,23,christopher manning refining the grammar symbols we can relax independence assump ons by encoding dependencies into the pcfg symbols by state splitng parent annota on marking johnson possessive nps too much state splitng sparseness no smoothing used what are the most useful features to encode
NLP_ch14,24,pcfg independence assump ons
NLP_ch14,25,christopher manning annotations annotations split the grammar categories into sub categories conditioning on history vs annotating p np s prp is a lot like p np prp s p np pos nnp pos isn t history conditioning feature grammars vs annotation can think of a symbol like np np pos as np parent np pos after parsing with an annotated grammar the annotations are then stripped for evaluation
NLP_ch14,26,the return of unlexicalized pcfgs
NLP_ch14,27,christopher manning accurate unlexicalized parsing klein and manning what do we mean by an unlexicalized pcfg grammar rules are not systema cally specified down to the level of lexical items np stocks is not allowed np s cc is fine closed vs open class words long tradi on in linguis cs of using func on words as features or markers for selec on vb have sbar if whether different to the bilexical idea of seman c heads open class selec on is really a proxy for seman cs thesis most of what you need for accurate parsing and much of what lexicalized pcfgs actually capture isn t lexical selec on between content words but just basic gramma cal features like verb form finiteness presence of a verbal auxiliary etc
NLP_ch14,28,christopher manning experimental approach corpus penn treebank wsj iterate on small dev set training sections development section first files test section size number of symbols in grammar passive complete symbols np np s ac ve incomplete symbols np_np_cc from binariza on we state split as sparingly as possible highest accuracy with fewest symbols error driven manual hill climb one annota on at a me
NLP_ch14,29,christopher manning horizontal markovization horizontal markovization merges states v inf v inf horizontal markov order horizontal markov order slobmys
NLP_ch14,30,christopher manning ver cal markoviza on order order ver cal markov order rewrites depend on past k ancestor nodes i e parent annota on v v v v vertical markov order vertical markov order slobmys model f size v h v k
NLP_ch14,31,christopher manning unary splits problem unary rewrites are used to transmute categories so a high probability rule can be used solu on mark unary rewrite sites annotation f size with u base k unary k
NLP_ch14,32,christopher manning tag splits problem treebank tags are too coarse example sbar senten al complemen zers that whether if subordina ng conjunc ons while a er and true preposi ons in of to are all tagged in annotation f size par al solu on previous k subdivide the in tag split in k
NLP_ch14,33,christopher manning yield splits problem some mes the behavior of a category depends on something inside its future yield examples possessive nps finite vs infinite vps lexical heads solu on annotate future annotation f size elements into nodes tag splits k poss np k split vp k
NLP_ch14,34,christopher manning distance recursion splits problem vanilla pcfgs cannot np v dis nguish akachment vp heights np solu on mark a property of pp higher or lower sites v contains a verb is non recursive annotation f size base nps cf collins previous k right recursive nps base np k dominates v k right rec np k
NLP_ch14,35,christopher manning a fully annotated tree
NLP_ch14,36,christopher manning final test set results parser lp lr f magerman collins klein manning charniak collins beats first genera on lexicalized parsers
NLP_ch14,37,the return of unlexicalized pcfgs
NLP_ch14,38,latent variable pcfgs extending the idea to induced syntac co seman c classes
NLP_ch14,39,christopher manning learning latent annota ons petrov and klein outside can you automa cally find good symbols brackets are known base categories are known x induce subcategories clever split merge category refinement x x x x x x he was right em algorithm like forward backward for hmms but constrained by tree inside
NLP_ch14,40,christopher manning pos tag splits commonest words effec vely a seman c class based model proper nouns nnp nnp oct nov sept nnp john robert james nnp j e l nnp bush noriega peters nnp new san wall nnp york francisco street personal pronouns prp prp it he i prp it he they prp it them him
NLP_ch14,41,christopher manning pn pv pp pvda s pjda rabs pq pnhw nrp xn vnis trp pphw qs pjnoc garf can pcu pvdahw jtni qrabs crr pjdahw x toor tsl number of phrasal subcategories
NLP_ch14,42,christopher manning the latest parsing results english ptb wsj train test f f words all words parser klein manning unlexicalized matsuzaki et al simple em latent states charniak genera ve lexicalized maxent inspired petrov and klein naacl charniak johnson discrimina ve reranker fossum knight combining cons tuent parsers
NLP_ch14,43,latent variable pcfgs extending the idea to induced syntac co seman c classes
NLP_ch15,1,dependency parsing introduc on
NLP_ch15,2,christopher manning dependency grammar and dependency structure dependency syntax postulates that syntac c structure consists of lexical items linked by binary asymmetric rela ons arrows called dependencies submitted nsubjpass auxpass prep the arrows are bills were by commonly typed prep pobj on with the name of brownback pobj nn appos gramma cal ports senator republican rela ons subject cc conj prep preposi onal object and immigration of apposi on etc pobj kansas
NLP_ch15,3,christopher manning dependency grammar and dependency structure dependency syntax postulates that syntac c structure consists of lexical items linked by binary asymmetric rela ons arrows called dependencies submitted the arrow connects a nsubjpass auxpass prep head governor bills were by superior regent with a prep pobj dependent modifier on brownback inferior subordinate pobj nn appos ports senator republican usually dependencies cc conj prep form a tree connected and immigration of acyclic single head pobj kansas
NLP_ch15,4,christopher manning rela on between phrase structure and dependency structure a dependency grammar has a no on of a head officially cfgs don t but modern linguis c theory and all modern sta s cal parsers charniak collins stanford do via hand wriken phrasal head rules the head of a noun phrase is a noun number adj the head of a verb phrase is a verb modal the head rules can be used to extract a dependency parse from a cfg parse the closure of dependencies give cons tuency from a dependency tree but the dependents of a word must be at the same level i e flat there can be no vp
NLP_ch15,5,christopher manning methods of dependency parsing dynamic programming like in the cky algorithm you can do it similarly to lexicalized pcfg parsing an o n algorithm eisner gives a clever algorithm that reduces the complexity to o n by producing parse items with heads at the ends rather than in the middle graph algorithms you create a maximum spanning tree for a sentence mcdonald et al s mstparser scores dependencies independently using a ml classifier he uses mira for online learning but it could be maxent constraint sa sfac on edges are eliminated that don t sa sfy hard constraints karlsson etc determinis c parsing greedy choice of akachments guided by machine learning classifiers maltparser nivre et al discussed in the next segment
NLP_ch15,6,christopher manning dependency condi oning preferences what are the sources of informa on for dependency parsing bilexical affini es issues the is plausible dependency distance mostly with nearby words intervening material dependencies rarely span intervening verbs or punctua on valency of heads how many dependents on which side are usual for a head root discussion of the outstanding issues was completed
NLP_ch15,7,christopher manning quiz question consider this sentence retail sales drop in april cools afternoon market trading which word are these words a dependent of sales april afternoon trading
NLP_ch15,8,dependency parsing introduc on
NLP_ch15,9,greedy transi on based parsing maltparser
NLP_ch15,10,christopher manning maltparser nivre et al a simple form of greedy discrimina ve dependency parser the parser does a sequence of bokom up ac ons roughly like shiq or reduce in a shiq reduce parser but the reduce ac ons are specialized to create dependencies with head on leq or right the parser has a stack σ wriken with top to the right which starts with the root symbol a buffer β wriken with top to the leq which starts with the input sentence a set of dependency arcs a which starts off empty a set of ac ons
NLP_ch15,11,christopher manning basic transi on based dependency parser start σ root β w w a n shiq σ w β a σ w β a i i leq arc σ w w β a σ w β a r w w r i j j j i right arc σ w w β a σ w β a r w w r i j i i j finish β notes unlike the regular presenta on of the cfg reduce step dependencies combine one thing from each of stack and buffer
NLP_ch15,12,christopher manning ac ons arc eager dependency parser start σ root β w w a n leq arc σ w w β a σ w β a r w w r i j j j i precondi on r w w a w root k i i right arc σ w w β a σ w w β a r w w r i j i j i j reduce σ w β a σ β a i precondi on r w w a k i shiq σ w β a σ w β a i i finish β this is the common arc eager variant a head can immediately take a right dependent before its dependents are found
NLP_ch15,13,christopher manning leq arc σ w w β a σ w β a r w w r i j j j i precondi on w r w a w root k i i right arc σ w w β a σ w w β a r w w example r i j i j i j reduce σ w β a σ β a i precondi on w r w a k i shiq σ w β a σ w β a i i happy children like to play with their friends root happy children shiq root happy children like la root children like amod children happy a amod shiq root children like to a la root like to a nsubj like children a nsubj ra root like to play a root root like a root shiq root like to play with a la root like play with a aux play to a aux ra root like play with their a xcomp like play a xcomp
NLP_ch15,14,christopher manning leq arc σ w w β a σ w β a r w w r i j j j i precondi on w r w a w root k i i right arc σ w w β a σ w w β a r w w example r i j i j i j reduce σ w β a σ β a i precondi on w r w a k i shiq σ w β a σ w β a i i happy children like to play with their friends ra root like play with their a xcomp like play a xcomp ra root like play with their friends a prep play with a prep shiq root like play with their friends a la root like play with friends a poss friends their a poss ra root like play with friends a pobj with friends a pobj reduce root like play with a reduce root like play a reduce root like a ra root like a punc like a punc you terminate as soon as the buffer is empty dependencies a
NLP_ch15,15,christopher manning maltparser nivre et al we have leq to explain how we choose the next ac on each ac on is predicted by a discrimina ve classifier oqen svm could be maxent classifier over each legal move max of untyped choices max of r when typed features top of stack word pos first in buffer word pos etc there is no search in the simplest and usual form but you could do some kind of beam search if you wish the model s accuracy is slightly below the best lpcfgs evaluated on dependencies but it provides close to state of the art parsing performance it provides very fast linear me parsing
NLP_ch15,16,christopher manning evalua on of dependency parsing labeled dependency accuracy acc correct deps of deps uas root she saw the video lecture las gold parsed she nsubj she nsubj saw root saw root the det the det video nn video nsubj lecture dobj lecture ccomp
NLP_ch15,17,christopher manning representa ve performance numbers the conll x shared task provides evalua on numbers for various dependency parsing approaches over languages malt las scores from depending greatly on language treebank here we give a few uas numbers for english to allow some comparison to cons tuency parsing parser uas sagae and lavie ensemble of dependency parsers charniak genera ve cons tuency collins genera ve cons tuency mcdonald and pereira mst graph based dependency yamada and matsumoto transi on based dependency
NLP_ch15,18,christopher manning projec vity dependencies from a cfg tree using heads must be projec ve there must not be any crossing dependency arcs when the words are laid out in their linear order with all arcs above the words but dependency theory normally does allow non projec ve structures to account for displaced cons tuents you can t easily get the seman cs of certain construc ons right without these nonprojec ve dependencies who did bill buy the coffee from yesterday
NLP_ch15,19,christopher manning handling non projec vity the arc eager algorithm we presented only builds projec ve dependency trees possible direc ons to head just declare defeat on nonprojec ve arcs use a dependency formalism which only admits projec ve representa ons a cfg doesn t represent such structures use a postprocessor to a projec ve dependency parsing algorithm to iden fy and resolve nonprojec ve links add extra types of transi ons that can model at least most non projec ve structures move to a parsing mechanism that does not use or require any constraints on projec vity e g the graph based mstparser
NLP_ch15,20,greedy transi on based parsing maltparser
NLP_ch15,21,dependencies encode rela onal structure rela on extrac on with stanford dependencies
NLP_ch15,22,christopher manning dependency paths iden fy rela ons like protein interac on erkan et al emnlp fundel et al demonstrated nsubj ccomp results interacts prep_with compl det that advmod sasa nsubj conj_and conj_and the kaic rythmically kaia kaib kaic nsubj interacts prep_with sasa kaic nsubj interacts prep_with sasa conj_and kaia kaic nsubj interacts prep_with sasa conj_and kaib
NLP_ch15,23,christopher manning stanford dependencies de marneffe et al lrec the basic dependency representa on is projec ve it can be generated by postprocessing headed phrase structure parses penn treebank syntax it can also be generated directly by dependency parsers such as maltparser or the easy first parser jumped nsubj prep boy over pobj det amod the little the det fence
NLP_ch15,24,christopher manning graph modifica on to facilitate seman c analysis bell based in la makes and distributes electronic and computer products conj makes distributes nsubj cc dobj and bell products partmod amod based electronic prep cc conj in and computer pobj la
NLP_ch15,25,christopher manning graph modifica on to facilitate seman c analysis bell based in la makes and distributes electronic and computer products nsubj conj_and makes distributes nsubj dobj bell products partmod amod amod based electronic conj_and prep_in computer la
NLP_ch15,26,christopher manning bionlp rela on extrac on shared tasks björne et al dependency distance linear distance
NLP_ch15,27,dependencies encode rela onal structure rela on extrac on with stanford dependencies
NLP_ch16,1,word meaning and similarity word senses and word rela ons
NLP_ch16,2,dan jurafsky reminder lemma and wordform a lemma or cita on form same stem part of speech rough seman cs a wordform the inflected word as it appears in text wordform lemma banks bank sung sing duermes dormir
NLP_ch16,3,dan jurafsky lemmas have senses one lemma bank can have many meanings a bank can hold the investments in a custodial sense account as agriculture burgeons on the east bank the sense river will shrink even more sense or word sense a discrete representa on of an aspect of a word s meaning the lemma bank here has two senses
NLP_ch16,4,dan jurafsky homonymy homonyms words that share a form but have unrelated dis nct meanings bank financial ins tu on bank sloping land bat club for hinng a ball bat nocturnal flying mammal homographs bank bank bat bat homophones write and right piece and peace
NLP_ch16,5,dan jurafsky homonymy causes problems for nlp applica ons informa on retrieval bat care machine transla on bat murciélago animal or bate for baseball text to speech bass stringed instrument vs bass fish
NLP_ch16,6,dan jurafsky polysemy the bank was constructed in out of local red brick i withdrew the money from the bank are those the same sense sense a financial ins tu on sense the building belonging to a financial ins tu on a polysemous word has related meanings most non rare words have mul ple meanings
NLP_ch16,7,dan jurafsky metonymy or systema c polysemy a systema c rela onship between senses lots of types of polysemy are systema c school university hospital all can mean the ins tu on or the building a systema c rela onship building organiza on other such kinds of systema c polysemy author jane austen wrote emma works of author i love jane austen tree plums have beautiful blossoms fruit i ate a preserved plum
NLP_ch16,8,dan jurafsky how do we know when a word has more than one sense the zeugma test two senses of serve which flights serve breakfast does lufthansa serve philadelphia does lu hansa serve breakfast and san jose since this conjunc on sounds weird we say that these are two different senses of serve
NLP_ch16,9,dan jurafsky synonyms word that have the same meaning in some or all contexts filbert hazelnut couch sofa big large automobile car vomit throw up water h two lexemes are synonyms if they can be subs tuted for each other in all situa ons if so they have the same proposi onal meaning
NLP_ch16,10,dan jurafsky synonyms but there are few or no examples of perfect synonymy even if many aspects of meaning are iden cal s ll may not preserve the acceptability based on no ons of politeness slang register genre etc example water h big large brave courageous
NLP_ch16,11,dan jurafsky synonymy is a rela on between senses rather than words consider the words big and large are they synonyms how big is that plane would i be flying on a large or small plane how about here miss nelson became a kind of big sister to benjamin miss nelson became a kind of large sister to benjamin why big has a sense that means being older or grown up large lacks this sense
NLP_ch16,12,dan jurafsky antonyms senses that are opposites with respect to one feature of meaning otherwise they are very similar dark light short long fast slow rise fall hot cold up down in out more formally antonyms can define a binary opposi on or be at opposite ends of a scale long short fast slow be reversives rise fall up down
NLP_ch16,13,dan jurafsky hyponymy and hypernymy one sense is a hyponym of another if the first sense is more specific deno ng a subclass of the other car is a hyponym of vehicle mango is a hyponym of fruit conversely hypernym superordinate hyper is super vehicle is a hypernym of car fruit is a hypernym of mango superordinate hyper vehicle fruit furniture subordinate hyponym car mango chair
NLP_ch16,14,dan jurafsky hyponymy more formally extensional the class denoted by the superordinate extensionally includes the class denoted by the hyponym entailment a sense a is a hyponym of sense b if being an a entails being a b hyponymy is usually transi ve a hypo b and b hypo c entails a hypo c another name the is a hierarchy a is a b or a isa b b subsumes a
NLP_ch16,15,dan jurafsky hyponyms and instances wordnet has both classes and instances an instance is an individual a proper noun that is a unique en ty san francisco is an instance of city but city is a class city is a hyponym of municipality location
NLP_ch16,16,word meaning and similarity word senses and word rela ons
NLP_ch16,17,word meaning and similarity wordnet and other online thesauri
NLP_ch16,18,dan jurafsky applica ons of thesauri and ontologies informa on extrac on informa on retrieval ques on answering bioinforma cs and medical informa cs machine transla on
NLP_ch16,19,dan jurafsky wordnet a hierarchically organized lexical database on line thesaurus aspects of a dic onary some other languages available or under development arabic finnish german portuguese category unique strings noun verb adjec ve adverb
NLP_ch16,20,dan jurafsky senses of bass in wordnet
NLP_ch16,21,dan jurafsky how is sense defined in wordnet the synset synonym set the set of near synonyms instan ates a sense or concept with a gloss example chump as a noun with the gloss a person who is gullible and easy to take advantage of this sense of chump is shared by words chump fool gull mark patsy fall guy sucker soft touch mug each of these senses have this same gloss not every sense sense of gull is the aqua c bird
NLP_ch16,22,dan jurafsky wordnet hypernym hierarchy for bass
NLP_ch16,23,dan jurafsky wordnet noun rela ons
NLP_ch16,24,dan jurafsky wordnet where it is hnp wordnetweb princeton edu perl webwn libraries python wordnet from nltk hnp www nltk org home java jwnl extjwnl on sourceforge
NLP_ch16,25,dan jurafsky mesh medical subject headings thesaurus from the na onal library of medicine mesh medical subject headings entry terms that correspond to biomedical headings hemoglobins synset entry terms eryhem ferrous hemoglobin hemoglobin defini on the oxygen carrying proteins of erythrocytes they are found in all vertebrates and some invertebrates the number of globin subunits in the hemoglobin quaternary structure differs between species structures range from monomeric to a variety of mul meric arrangements
NLP_ch16,26,dan jurafsky the mesh hierarchy a
NLP_ch16,27,dan jurafsky uses of the mesh ontology provide synonyms entry terms e g glucose and dextrose provide hypernyms from the hierarchy e g glucose isa monosaccharide indexing in medline pubmed database nlm s bibliographic database million journal ar cles each ar cle hand assigned mesh terms
NLP_ch16,28,word meaning and similarity wordnet and other online thesauri
NLP_ch16,29,word meaning and similarity word similarity thesaurus methods
NLP_ch16,30,dan jurafsky word similarity synonymy a binary rela on two words are either synonymous or not similarity or distance a looser metric two words are more similar if they share more features of meaning similarity is properly a rela on between senses the word bank is not similar to the word slope bank is similar to fund bank is similar to slope but we ll compute similarity over both words and senses
NLP_ch16,31,dan jurafsky why word similarity informa on retrieval ques on answering machine transla on natural language genera on language modeling automa c essay grading plagiarism detec on document clustering
NLP_ch16,32,dan jurafsky word similarity and word relatedness we o en dis nguish word similarity from word relatedness similar words near synonyms related words can be related any way car bicycle similar car gasoline related not similar
NLP_ch16,33,dan jurafsky two classes of similarity algorithms thesaurus based algorithms are words nearby in hypernym hierarchy do words have similar glosses defini ons distribu onal algorithms do words have similar distribu onal contexts
NLP_ch16,34,dan jurafsky path based similarity two concepts senses synsets are similar if they are near each other in the thesaurus hierarchy have a short path between them concepts have path to themselves
NLP_ch16,35,dan jurafsky refinements to path based similarity pathlen c c number of edges in the shortest path in the hypernym graph between sense nodes c and c ranges from to iden ty simpath c c pathlen c c wordsim w w max sim c c c senses w c senses w
NLP_ch16,36,dan jurafsky example path based similarity simpath c c pathlen c c simpath nickel coin simpath fund budget simpath nickel currency simpath nickel money simpath coinage richter scale
NLP_ch16,37,dan jurafsky problem with basic path based similarity assumes each link represents a uniform distance but nickel to money seems to us to be closer than nickel to standard nodes high in the hierarchy are very abstract we instead want a metric that represents the cost of each edge independently words connected only through abstract nodes are less similar
NLP_ch16,38,dan jurafsky informa on content similarity metrics resnik using informa on content to evaluate seman c similarity in a taxonomy ijcai let s define p c as the probability that a randomly selected word in a corpus is an instance of concept c formally there is a dis nct random variable ranging over words associated with each concept in the hierarchy for a given concept each observed noun is either a member of that concept with probability p c not a member of that concept with probability p c all words are members of the root node en ty p root the lower a node in hierarchy the lower its probability
NLP_ch16,39,dan jurafsky en ty informa on content similarity geological forma on train by coun ng in a corpus natural eleva on cave shore each instance of hill counts toward frequency hill ridge grono coast of natural eleva on geological forma on en ty etc let words c be the set of all words that are children of node c words geo forma on hill ridge grono coast cave shore natural eleva on words natural eleva on hill ridge count w w words c p c n
NLP_ch16,40,dan jurafsky informa on content similarity wordnet hierarchy augmented with probabili es p c d lin an informa on theore c defini on of similarity icml
NLP_ch16,41,dan jurafsky informa on content defini ons informa on content ic c log p c most informa ve subsumer lowest common subsumer lcs c c the most informa ve lowest node in the hierarchy subsuming both c and c
NLP_ch16,42,using informa on content for similarity dan jurafsky the resnik method philip resnik using informa on content to evaluate seman c similarity in a taxonomy ijcai philip resnik seman c similarity in a taxonomy an informa on based measure and its applica on to problems of ambiguity in natural language jair the similarity between two words is related to their common informa on the more two words have in common the more similar they are resnik measure common informa on as the informa on content of the most informa ve lowest subsumer mis lcs of the two nodes sim c c log p lcs c c resnik
NLP_ch16,43,dan jurafsky dekang lin method dekang lin an informa on theore c defini on of similarity icml intui on similarity between a and b is not just what they have in common the more differences between a and b the less similar they are commonality the more a and b have in common the more similar they are difference the more differences between a and b the less similar commonality ic common a b difference ic descrip on a b ic common a b
NLP_ch16,44,dan jurafsky dekang lin similarity theorem the similarity between a and b is measured by the ra o between the amount of informa on needed to state the commonality of a and b and the informa on needed to fully describe what a and b are ic common a b sim a b lin ic description a b lin altering resnik defines ic common a b as x informa on of the lcs logp lcs c c sim c c lin logp c logp c
NLP_ch16,45,dan jurafsky lin similarity func on logp lcs c c sim a b lin logp c logp c logp geological formation sim hill coast lin logp hill logp coast ln ln ln
NLP_ch16,46,dan jurafsky the extended lesk algorithm a thesaurus based measure that looks at glosses two concepts are similar if their glosses contain similar words drawing paper paper that is specially prepared for use in dra ing decal the art of transferring designs from specially prepared paper to a wood or glass or metal surface for each n word phrase that s in both glosses add a score of n paper and specially prepared for compute overlap also for other rela ons glosses of hypernyms and hyponyms
NLP_ch16,47,dan jurafsky summary thesaurus based similarity sim c c path pathlen c c logp lcs c c sim c c logp lcs c c sim c c resnik lin logp c logp c sim c c jiangconrath logp c logp c logp lcs c c sim c c overlap gloss r c gloss q c elesk r q rels
NLP_ch16,48,dan jurafsky libraries for compu ng thesaurus based similarity nltk hnp nltk github com api nltk corpus reader html highlight similarity nltk corpus reader wordnetcorpusreader res_similarity wordnet similarity hnp wn similarity sourceforge net web based interface hnp marimba d umn edu cgi bin similarity similarity cgi
NLP_ch16,49,dan jurafsky evalua ng similarity intrinsic evalua on correla on between algorithm and human word similarity ra ngs extrinsic task based end to end evalua on malapropism spelling error detec on wsd essay grading taking toefl mul ple choice vocabulary tests levied is closest in meaning to imposed believed requested correlated
NLP_ch16,50,word meaning and similarity word similarity thesaurus methods
NLP_ch16,51,word meaning and similarity word similarity distribu onal similarity i
NLP_ch16,52,dan jurafsky problems with thesaurus based meaning we don t have a thesaurus for every language even if we do they have problems with recall many words are missing most if not all phrases are missing some connec ons between senses are missing thesauri work less well for verbs adjec ves adjec ves and verbs have less structured hyponymy rela ons
NLP_ch16,53,dan jurafsky distribu onal models of meaning aallssoo ccaalllleedd vveeccttoorr ssppaaccee mmooddeellss ooff mmeeaanniinngg ooffffeerr mmuucchh hhiigghheerr rreeccaallll tthhaann hhaanndd bbuuiilltt tthheessaauurrii aalltthhoouugghh tthheeyy tteenndd ttoo hhaavvee lloowweerr pprreecciissiioonn zellig harris oculist and eye doctor occur in almost the same environments if a and b have almost iden cal environments we say that they are synonyms firth you shall know a word by the company it keeps
NLP_ch16,54,dan jurafsky intui on of distribu onal word similarity nida example a bottle of tesgüino is on the table everybody likes tesgüino tesgüino makes you drunk we make tesgüino out of corn from context words humans can guess tesgüino means an alcoholic beverage like beer intui on for algorithm two words are similar if they have similar word contexts
NLP_ch16,55,dan jurafsky reminder term document matrix each cell count of term t in a document d z t d each document is a count vector in v a column below ℕ a a a bc bd e c d f f
NLP_ch16,56,dan jurafsky reminder term document matrix two documents are similar if their vectors are similar a a a bc bd e c d f f
NLP_ch16,57,dan jurafsky the words in a term document matrix each word is a count vector in d a row below ℕ a a a bc bd e c d f f
NLP_ch16,58,dan jurafsky the words in a term document matrix two words are similar if their vectors are similar a a a bc bd e c d f f
NLP_ch16,59,dan jurafsky the term context matrix instead of using en re documents use smaller contexts paragraph window of words a word is now defined by a vector over counts of context words
NLP_ch16,60,dan jurafsky sample contexts words brown corpus equal amount of sugar a sliced lemon a tablespoonful of apricot preserve or jam a pinch each of clove and nutmeg on board for their enjoyment cau ously she sampled her first pineapple and another fruit whose taste she likened to that of of a recursive type well suited to programming on the digital computer in finding the op mal r stage policy from that of substan ally affect commerce for the purpose of gathering data and informa on necessary for the study authorized in the first sec on of this
NLP_ch16,61,dan jurafsky term context matrix for word similarity two words are similar in meaning if their context vectors are similar
NLP_ch16,62,dan jurafsky should we use raw counts for the term document matrix we used z idf instead of raw term counts for the term context matrix posi ve pointwise mutual informa on ppmi is common
NLP_ch16,63,dan jurafsky pointwise mutual informa on pointwise mutual informa on do events x and y co occur more than if they were independent p x y pmi x y log p x p y pmi between two words church hanks do words x and y co occur more than if they were independent p word word pmi word word log p word p word posi ve pmi between two words niwa nina replace all pmi values less than with zero
NLP_ch16,64,dan jurafsky compu ng ppmi on a term context matrix matrix f with w rows words and c columns contexts f is of mes w occurs in context c ij i j c w f f f ij ij ij p ij w c p i w j c p j w i c f f f ij ij ij i j i j i j p pmi if pmi pmi log ij ppmi ij ij ij ij p p otherwise i j
NLP_ch16,65,dan jurafsky f ij p ij w c f ij i j c w f f p w informa on c data ij ij p w j p c i p w informa on i j n n p c data
NLP_ch16,66,dan jurafsky p pmi log ij ij p p i j pmi informa on data log using full precision
NLP_ch16,67,dan jurafsky weighing pmi pmi is biased toward infrequent events various weigh ng schemes help alleviate this see turney and pantel add one smoothing can also help
NLP_ch16,68,dan jurafsky
NLP_ch16,69,dan jurafsky
NLP_ch16,70,word meaning and similarity word similarity distribu onal similarity i
NLP_ch16,71,word meaning and similarity word similarity distribu onal similarity ii
NLP_ch16,72,dan jurafsky using syntax to define a word s context zellig harris the meaning of en es and the meaning of gramma cal rela ons among them is related to the restric on of combina ons of these en es rela ve to other en es two words are similar if they have similar parse contexts duty and responsibility chris callison burch s example modified by addi onal administra ve assumed adjec ves collec ve congressional cons tu onal objects of verbs assert assign assume anend to avoid become breach
NLP_ch16,73,dan jurafsky co occurrence vectors based on syntac c dependencies dekang lin automa c retrieval and clustering of similar words the contexts c are different dependency rela ons subject of absorb preposi onal object of inside counts for the word cell
NLP_ch16,74,dan jurafsky pmi applied to dependency rela ons hindle don noun classification from predicate argument structure acl oobbjjeecctt ooff ddrriinnkk ccoouunntt ppmmii itte a aliqnuytidh ing wwiinnee taenay thing liitq uid drink it more common than drink wine but wine is a bener drinkable thing than it
NLP_ch16,75,sec dan jurafsky reminder cosine for compu ng similarity dot product unit vectors n v w v w v w i i cos v w i v w v w n n v w i i i i v is the ppmi value for word v in context i i w is the ppmi value for word w in context i i cos v w is the cosine similarity of v and w
NLP_ch16,76,dan jurafsky cosine as a similarity metric vectors point in opposite direc ons vectors point in same direc ons vectors are orthogonal raw frequency or ppmi are non nega ve so cosine range
NLP_ch16,77,dan jurafsky large data computer apricot n v w digital v w v w i i cos v w i v w v w n n v w informa on i i i i which pair of words is more similar cosine apricot informa on cosine digital informa on cosine apricot digital
NLP_ch16,78,dan jurafsky other possible similarity measures
NLP_ch16,79,dan jurafsky evalua ng similarity the same as for thesaurus based intrinsic evalua on correla on between algorithm and human word similarity ra ngs extrinsic task based end to end evalua on spelling error detec on wsd essay grading taking toefl mul ple choice vocabulary tests levied is closest in meaning to which of these imposed believed requested correlated
NLP_ch16,80,word meaning and similarity word similarity distribu onal similarity ii
NLP_ch17,1,question answering what is ques on answering
NLP_ch17,2,dan jurafsky ques on answering one of the oldest nlp tasks punched card systems in simmons klein mcconlogue indexing and dependency logic for answering english ques ons american documenta on what do worms eat worms eat grass horses with worms eat grass worms horses worms eat with eat eat grass worms grass what birds eat worms grass is eaten by worms birds worms eat eat worms grass
NLP_ch17,3,dan jurafsky ques on answering ibm s watson won jeopardy on february william wilkinson s an account of the principalities of wallachia and moldovia bram stoker inspired this author s most famous novel
NLP_ch17,4,dan jurafsky apple s siri
NLP_ch17,5,dan jurafsky wolfram alpha
NLP_ch17,6,dan jurafsky types of ques ons in modern systems factoid ques ons who wrote the universal declara on of human rights how many calories are there in two slices of apple pie what is the average age of the onset of au sm where is apple computer based complex narra ve ques ons in children with an acute febrile illness what is the efficacy of acetaminophen in reducing fever what do scholars think about jefferson s posi on on dealing with pirates
NLP_ch17,7,dan jurafsky commercial systems mainly factoid ques ons where is the louvre museum located in paris france what s the abbrevia on for limited l p partnership what are the names of odin s ravens huginn and muninn what currency is used in china the yuan what kind of nuts are used in marzipan almonds what instrument does max roach play drums what is the telephone number for stanford university
NLP_ch17,8,dan jurafsky paradigms for qa ir based approaches trec ibm watson google knowledge based and hybrid approaches ibm watson apple siri wolfram alpha true knowledge evi
NLP_ch17,9,dan jurafsky many ques ons can already be answered by web search a
NLP_ch17,10,dan jurafsky ir based ques on answering a
NLP_ch17,11,dan jurafsky ir based factoid qa document document document document document document indexing answer passage question retrieval processing docume query document d d o n d c ot n u c ot m u c m u e m e e passage answer dnotcume formulation retrieval rnetlevant retrieval passages processing nt question docs answer type detection
NLP_ch17,12,dan jurafsky ir based factoid qa question processing detect ques on type answer type focus rela ons formulate queries to send to a search engine passage retrieval retrieve ranked documents break into suitable passages and rerank answer processing extract candidate answers rank candidates using evidence from the text and external sources
NLP_ch17,13,dan jurafsky knowledge based approaches siri build a seman c representa on of the query times dates loca ons en es numeric quan es map from this seman cs to query structured data or resources geospa al databases ontologies wikipedia infoboxes dbpedia wordnet yago restaurant review sources and reserva on services scien fic databases
NLP_ch17,14,dan jurafsky hybrid approaches ibm watson build a shallow seman c representa on of the query generate answer candidates using ir methods augmented with ontologies and semi structured data score each candidate using richer knowledge sources geospa al databases temporal reasoning taxonomical classifica on
NLP_ch17,15,question answering what is ques on answering
NLP_ch17,16,question answering answer types and query formula on
NLP_ch17,17,dan jurafsky factoid q a document document document document document document indexing answer passage question retrieval processing docume query document d d o n d c ot n u c ot m u c m u e m e e passage answer dnotcume formulation retrieval rnetlevant retrieval passages processing nt question docs answer type detection
NLP_ch17,18,dan jurafsky ques on processing things to extract from the ques on answer type detec on decide the named en ty type person place of the answer query formula on choose query keywords for the ir system ques on type classifica on is this a defini on ques on a math ques on a list ques on focus detec on find the ques on words that are replaced by the answer rela on extrac on find rela ons between en es in the ques on
NLP_ch17,19,dan jurafsky question processing they re the two states you could be reentering if you re crossing florida s northern border answer type us state query two states border florida north focus the two states rela ons borders florida x north
NLP_ch17,20,dan jurafsky answer type detec on named en es who founded virgin airlines person what canadian city has the largest popula on city
NLP_ch17,21,dan jurafsky answer type taxonomy xin li dan roth learning ques on classifiers coling coarse classes abbeviation entity description human location numeric finer classes location city country mountain human group individual tle descrip on entity animal body color currency
NLP_ch17,22,dan jurafsky part of li roth s answer type taxonomy city state country reason expression location definition abbreviation abbreviation description individual food entity human title numeric currency group animal date money percent distance size
NLP_ch17,23,dan jurafsky answer types
NLP_ch17,24,dan jurafsky more answer types
NLP_ch17,25,dan jurafsky answer types in jeopardy ferrucci et al building watson an overview of the deepqa project ai magazine fall answer types in jeopardy ques on sample the most frequent answer types cover of data the most frequent jeopardy answer types he country city man film state she author group here company president capital star novel character woman river island king song part series sport singer actor play team show actress animal presiden al composer musical na on book tle leader game
NLP_ch17,26,dan jurafsky answer type detec on hand wrioen rules machine learning hybrids
NLP_ch17,27,dan jurafsky answer type detec on regular expression based rules can get some cases who is was are were person person year year other rules use the ques on headword the headword of the first noun phrase ater the wh word which city in china has the largest number of foreign financial companies what is the state flower of california
NLP_ch17,28,dan jurafsky answer type detec on most oten we treat the problem as machine learning classifica on define a taxonomy of ques on types annotate training data for each ques on type train classifiers for each ques on class using a rich set of features features include those hand wrioen rules
NLP_ch17,29,dan jurafsky features for answer type detec on ques on words and phrases part of speech tags parse features headwords named en es seman cally related words
NLP_ch17,30,dan jurafsky factoid q a document document document document document document indexing answer passage question retrieval processing docume query document d d o n d c ot n u c ot m u c m u e m e e passage answer dnotcume formulation retrieval rnetlevant retrieval passages processing nt question docs answer type detection
NLP_ch17,31,dan jurafsky keyword selec on algorithm dan moldovan sanda harabagiu marius paca rada mihalcea richard goodrum roxana girju and vasile rus proceedings of trec select all non stop words in quota ons select all nnp words in recognized named en es select all complex nominals with their adjec val modifiers select all other complex nominals select all nouns with their adjec val modifiers select all other nouns select all verbs select all adverbs select the qfw word skipped in all previous steps select all other words
NLP_ch17,32,dan jurafsky choosing keywords from the query slide from mihai surdeanu who coined the term cyberspace in his novel neuromancer cyberspace neuromancer term novel coined
NLP_ch17,33,question answering answer types and query formula on
NLP_ch17,34,question answering passage retrieval and answer extrac on
NLP_ch17,35,dan jurafsky factoid q a document document document document document document indexing answer passage question retrieval processing docume query document d d o n d c ot n u c ot m u c m u e m e e passage answer dnotcume formulation retrieval rnetlevant retrieval passages processing nt question docs answer type detection
NLP_ch17,36,dan jurafsky passage retrieval step ir engine retrieves documents using query terms step segment the documents into shorter units something like paragraphs step passage ranking use answer type to help rerank passages
NLP_ch17,37,dan jurafsky features for passage ranking either in rule based classifiers or with supervised machine learning number of named en es of the right type in passage number of query words in passage number of ques on n grams also in passage proximity of query keywords to each other in passage longest sequence of ques on words rank of the document containing passage
NLP_ch17,38,dan jurafsky factoid q a document document document document document document indexing answer passage question retrieval processing docume query document d d o n d c ot n u c ot m u c m u e m e e passage answer dnotcume formulation retrieval rnetlevant retrieval passages processing nt question docs answer type detection
NLP_ch17,39,dan jurafsky answer extrac on run an answer type named en ty tagger on the passages each answer type requires a named en ty tagger that detects it if answer type is city tagger has to tag city can be full ner simple regular expressions or hybrid return the string with the right type who is the prime minister of india person manmohan singh prime minister of india had told left leaders that the deal would not be renegotiated how tall is mt everest length the official height of mount everest is feet
NLP_ch17,40,dan jurafsky ranking candidate answers but what if there are mul ple candidate answers q who was queen victoria s second son answer type person passage the marie biscuit is named ater marie alexandrovna the daughter of czar alexander ii of russia and wife of alfred the second son of queen victoria and prince albert
NLP_ch17,41,dan jurafsky ranking candidate answers but what if there are mul ple candidate answers q who was queen victoria s second son answer type person passage the marie biscuit is named ater marie alexandrovna the daughter of czar alexander ii of russia and wife of alfred the second son of queen victoria and prince albert
NLP_ch17,42,dan jurafsky use machine learning features for ranking candidate answers answer type match candidate contains a phrase with the correct answer type pazern match regular expression paoern matches the candidate ques on keywords of ques on keywords in the candidate keyword distance distance in words between the candidate and query keywords novelty factor a word in the candidate is not in the query apposi on features the candidate is an apposi ve to ques on terms punctua on loca on the candidate is immediately followed by a comma period quota on marks semicolon or exclama on mark sequences of ques on terms the length of the longest sequence of ques on terms that occurs in the candidate answer
NLP_ch17,43,dan jurafsky candidate answer scoring in ibm watson each candidate answer gets scores from components from unstructured text semi structured text triple stores logical form parse match between ques on and candidate passage source reliability geospa al loca on california is southwest of montana temporal rela onships taxonomic classifica on
NLP_ch17,44,dan jurafsky common evalua on metrics accuracy does answer match gold labeled answer mean reciprocal rank for each query return a ranked list of m candidate answers query score is rank of the first correct answer n if first answer is correct else if second answer is correct ½ rank i else if third answer is correct ⅓ etc mrr i n score is if none of the m answers are correct take the mean over all n queries
NLP_ch17,45,question answering passage retrieval and answer extrac on
NLP_ch17,46,question answering using knowledge in qa
NLP_ch17,47,dan jurafsky rela on extrac on answers databases of rela ons born in emma goldman june author of cao xue qin dream of the red chamber draw from wikipedia infoboxes dbpedia freebase etc ques ons extrac ng rela ons in ques ons whose granddaughter starred in e t acted in x e t granddaughter of x y
NLP_ch17,48,dan jurafsky temporal reasoning rela on databases and obituaries biographical dic onaries etc ibm watson in he took a job as a tax collector in andalusia candidates thoreau is a bad answer born in cervantes is possible was alive in
NLP_ch17,49,dan jurafsky geospa al knowledge containment direc onality borders beijing is a good answer for asian city california is southwest of montana geonames org
NLP_ch17,50,dan jurafsky context and conversa on in virtual assistants like siri coreference helps resolve ambigui es u book a table at il fornaio at with my mom u also send her an email reminder clarifica on ques ons u chicago pizza s did you mean pizza restaurants in chicago or chicago style pizza
NLP_ch17,51,question answering using knowledge in qa
NLP_ch18,1,question answering summarization in question answering
NLP_ch18,2,dan jurafsky text summariza on goal produce an abridged version of a text that contains informa on that is important or relevant to a user summariza on applica ons outlines or abstracts of any document ar cle etc summaries of email threads ac on items from a mee ng simplifying text by compressing sentences
NLP_ch18,3,dan jurafsky what to summarize single vs mul ple documents single document summariza on given a single document produce abstract outline headline mul ple document summariza on given a group of documents produce a gist of the content a series of news stories on the same event a set of web pages about some topic or ques on
NLP_ch18,4,dan jurafsky query focused summariza on generic summariza on generic summariza on summarize the content of a document query focused summariza on summarize a document with respect to an informa on need expressed in a user query a kind of complex ques on answering answer a ques on by summarizing a document that has the informa on to construct the answer
NLP_ch18,5,dan jurafsky summariza on for ques on answering snippets create snippets summarizing a web page for a query google characters about words plus tle and link
NLP_ch18,6,dan jurafsky summariza on for ques on answering mul ple documents create answers to complex ques ons summarizing mul ple documents instead of giving a snippet for each document create a cohesive answer that combines informa on from each document
NLP_ch18,7,dan jurafsky extrac ve summariza on abstrac ve summariza on extrac ve summariza on create the summary from phrases or sentences in the source document s abstrac ve summariza on express the ideas in the source documents using at least in part different words
NLP_ch18,8,dan jurafsky simple baseline take the first sentence
NLP_ch18,9,question answering summarization in question answering
NLP_ch18,10,question answering generating snippets and other single document answers
NLP_ch18,11,dan jurafsky snippets query focused summaries
NLP_ch18,12,dan jurafsky summariza on three stages content selec on choose sentences to extract from the document informa on ordering choose an order to place them in the summary sentence realiza on clean up the sentences all sentences extracted from documents sentences sentence summary document information realization sentence sentence segmentation extraction ordering sentence simplification content selection
NLP_ch18,13,dan jurafsky basic summariza on algorithm content selec on choose sentences to extract from the document informa on ordering just use document order sentence realiza on keep original sentences all sentences extracted from documents sentences sentence summary document information realization sentence sentence segmentation extraction ordering sentence simplification content selection
NLP_ch18,14,dan jurafsky unsupervised content selec on h p luhn the automa c crea on of literature abstracts ibm journal of research and development intui on da ng back to luhn choose sentences that have salient or informa ve words two approaches to defining salient words z idf weigh each word w in document j by z idf i weight w tf idf i ij i topic signature choose a smaller set of salient words mutual informa on log likelihood ra o llr dunning lin and hovy if log w weight w i i otherwise
NLP_ch18,15,dan jurafsky topic signature based content selec on with queries conroy schlesinger and o leary choose words that are informa ve either by log likelihood ra o llr or by appearing in the query if log w i could learn more weight w if w question complex weights i i otherwise weigh a sentence or window by weight of its words weight s weight w s w s
NLP_ch18,16,dan jurafsky supervised content selec on given problems a labeled training set of good hard to get labeled training summaries for each document data align alignment difficult performance not beeer than the sentences in the document unsupervised algorithms with sentences in the summary so in prac ce extract features unsupervised content posi on first sentence selec on is more common length of sentence word informa veness cue phrases cohesion train a binary classifier put sentence in summary yes or no
NLP_ch18,17,question answering generating snippets and other single document answers
NLP_ch18,18,question answering evalua ng summaries rouge
NLP_ch18,19,dan jurafsky rouge recall oriented understudy for gis ng evalua on lin and hovy intrinsic metric for automa cally evalua ng summaries based on bleu a metric used for machine transla on not as good as human evalua on did this answer the user s ques on but much more convenient given a document d and an automa c summary x have n humans produce a set of reference summaries of d run system giving automa c summary x what percentage of the bigrams from the reference summaries appear in x min count i x count i s rouge s refsummaries bigrams i s count i s s refsummaries bigrams i s
NLP_ch18,20,dan jurafsky a rouge example q what is water spinach human water spinach is a green leafy vegetable grown in the tropics human water spinach is a semi aqua c tropical plant grown as a vegetable human water spinach is a commonly eaten leaf vegetable of asia system answer water spinach is a leaf vegetable commonly eaten in tropical areas of asia rouge
NLP_ch18,21,question answering evalua ng summaries rouge
NLP_ch18,22,question answering complex questions summarizing multiple documents
NLP_ch18,23,dan jurafsky defini on ques ons q what is water spinach a water spinach ipomoea aqua ca is a semi aqua c leafy green plant with long hollow stems and spear or heart shaped leaves widely grown throughout asia as a leaf vegetable the leaves and stems are onen eaten s r fried flavored with salt or in soups other common names include morning glory vegetable kangkong malay rau muong viet ong choi cant and kong xin cai mand it is not related to spinach but is closely related to sweet potato and convolvulus
NLP_ch18,24,dan jurafsky medical ques ons demner fushman and lin q in children with an acute febrile illness what is the efficacy of single medica on therapy with acetaminophen or ibuprofen in reducing fever a ibuprofen provided greater temperature decrement and longer dura on of an pyresis than acetaminophen when the two drugs were administered in approximately equal doses pubmedid evidence strength a
NLP_ch18,25,dan jurafsky other complex ques ons modified from the duc compe on hoa trang dang how is compost made and used for gardening including different types of compost their uses origins and benefits what causes train wrecks and what can be done to prevent them where have poachers endangered wildlife what wildlife has been endangered and what steps have been taken to prevent poaching what has been the human toll in death or injury of tropical storms in recent years
NLP_ch18,26,dan jurafsky answering harder ques ons query focused mul document summariza on the boeom up snippet method find a set of relevant documents extract informa ve sentences from the documents order and modify the sentences into an answer the top down informa on extrac on method build specific answerers for different ques on types defini on ques ons biography ques ons certain medical ques ons
NLP_ch18,27,dan jurafsky query focused mul document summariza on a query document document document all sentences extracted document plus simplified versions sentences document input docs all sentences from documents sentence sentence sentence extraction segmentation simplification llr mmr content selection sentence information summary realization ordering
NLP_ch18,28,dan jurafsky simplifying sentences zajic et al conroy et al vanderwende et al simplest method parse sentences use rules to decide which modifiers to prune more recently a wide variety of machine learning methods apposi ves rajam an ar st who was living at the me in philadelphia found the inspira on in the back of city magazines auribu on clauses rebels agreed to talks with government officials interna onal observers said tuesday pps the commercial fishing restric ons in washington will not be lined unless the salmon popula on without named en es increases pp to a sustainable number ini al adverbials for example on the other hand as a maeer of fact at this point
NLP_ch18,29,dan jurafsky maximal marginal relevance mmr jaime carbonell and jade goldstein the use of mmr diversity based reranking for reordering documents and producing summaries sigir an itera ve method for content selec on from mul ple documents itera vely greedily choose the best sentence to insert in the summary answer so far relevant maximally relevant to the user s query high cosine similarity to the query novel minimally redundant with the summary answer so far low cosine similarity to the summary sˆ max sim s q max sim s s mmr s d s s stop when desired length
NLP_ch18,30,dan jurafsky llr mmr choosing informa ve yet non redundant sentences one of many ways to combine the intui ons of llr and mmr score each sentence based on llr including query words include the sentence with highest score in the summary itera vely add into the summary high scoring sentences that are not redundant with summary so far
NLP_ch18,31,dan jurafsky informa on ordering chronological ordering order sentences by the date of the document for summarizing news barzilay elhadad and mckeown coherence choose orderings that make neighboring sentences similar by cosine choose orderings in which neighboring sentences discuss the same en ty barzilay and lapata topical ordering learn the ordering of topics in the source documents
NLP_ch18,32,dan jurafsky domain specific answering the informa on extrac on method a good biography of a person contains a person s birth death fame factor educa on na onality and so on a good defini on contains genus or hypernym the hajj is a type of ritual a medical answer about a drug s use contains the problem the medical condi on the interven on the drug or procedure and the outcome the result of the study
NLP_ch18,33,dan jurafsky informa on that should be in the answer for kinds of ques ons
NLP_ch18,34,dan jurafsky architecture for complex ques on answering defini on ques ons s blair goldensohn k mckeown and a schlaikjer answering defini on ques ons a hybrid approach the hajj or pilgrimage to makkah mecca is the central duty of islam more than two million muslims are expected to take the hajj this year muslims must perform what is the hajj the hajj at least once in their lifetime if physically and financially able the hajj is a ndocs len milestone event in a muslim s life the annual hajj begins in the twelfth month of the islamic year which is lunar not solar so that hajj and ramadan fall sometimes definition in summer sometimes in winter the hajj is a week long pilgrimage that begins in the th month of the islamic lunar calendar another ceremony which was not creation document connected with the rites of the ka ba before the rise of islam is the hajj the retrieval annual pilgrimage to arafat about two miles east of mecca toward mina sentence clusters importance ordering genus species sentences web documents predicate the hajj or pilgrimage to makkah mecca is the central duty of islam data driven total the hajj is a milestone event in a muslim s life sentences identification the hajj is one of five pillars that make up the foundation of islam analysis non specific definitional sentences
NLP_ch18,35,question answering answering questions by summarizing multiple documents
NLP_ch2,1,basic text processing regular expressions
NLP_ch2,2,dan jurafsky regular expressions a formal language for specifying text strings how can we search for any of these woodchuck woodchucks woodchuck woodchucks
NLP_ch2,3,dan jurafsky regular expressions disjunc ons le ers inside square brackets pa ern matches ww oodchuck woodchuck woodchuck any digit ranges a z pa ern matches a z an upper case le er drenched blossoms a z a lower case le er my beans were impatient a single digit chapter down the rabbit hole
NLP_ch2,4,dan jurafsky regular expressions nega on in disjunc on negagons ss carat means negagon only when first in pa ern matches a z not an upper case le er oyfn pripetchik ss neither s nor s i have no exquisite reason e neither e nor look here a b the pa ern a carat b look up a b now
NLP_ch2,5,dan jurafsky regular expressions more disjunc on woodchucks is another name for groundhog the pipe for disjuncgon pa ern matches groundhog woodchuck yours mine yours mine a b c abc gg roundhog ww oodchuck photo d fletcher
NLP_ch2,6,dan jurafsky regular expressions pa ern matches colou r opgonal color colour previous char oo h or more of oh ooh oooh ooooh previous char o h or more of oh ooh oooh ooooh previous char stephen c kleene baa baa baaa baaaa baaaaa kleene kleene beg n begin begun begun beg n
NLP_ch2,7,dan jurafsky regular expressions anchors pa ern matches a z palo alto a za z hello the end the end the end
NLP_ch2,8,dan jurafsky example find me all instances of the word the in a text the misses capitalized examples tt he incorrectly returns other or theology a za z tt he a za z
NLP_ch2,9,dan jurafsky errors the process we just went through was based on fixing two kinds of errors matching strings that we should not have matched there then other false posigves type i not matching things that we should have matched the false negagves type ii
NLP_ch2,10,dan jurafsky errors cont in nlp we are always dealing with these kinds of errors reducing the error rate for an applicagon oden involves two antagonisgc efforts increasing accuracy or precision minimizing false posigves increasing coverage or recall minimizing false negagves
NLP_ch2,11,dan jurafsky summary regular expressions play a surprisingly large role sophisgcated sequences of regular expressions are oden the first model for any text processing text for many hard tasks we use machine learning classifiers but regular expressions are used as features in the classifiers can be very useful in capturing generalizagons
NLP_ch2,12,basic text processing regular expressions
NLP_ch2,13,basic text processing word tokenizagon
NLP_ch2,14,dan jurafsky text normaliza on every nlp task needs to do text normalizagon segmengng tokenizing words in running text normalizing word formats segmengng sentences in running text
NLP_ch2,15,dan jurafsky how many words i do uh main mainly business data processing fragments filled pauses seuss s cat in the hat is different from other cats lemma same stem part of speech rough word sense cat and cats same lemma wordform the full inflected surface form cat and cats different wordforms
NLP_ch2,16,dan jurafsky how many words they lay back on the san francisco grass and looked at the stars and their type an element of the vocabulary token an instance of that type in running text how many tokens or types or or
NLP_ch2,17,dan jurafsky how many words n number of tokens church and gale v o n½ v vocabulary set of types v is the size of the vocabulary tokens n types v switchboard phone million thousand conversagons shakespeare thousand google n grams trillion million
NLP_ch2,18,dan jurafsky simple tokeniza on in unix inspired by ken church s unix for poets given a text file output the word tokens and their frequencies tr sc a za z n shakes txt change all non alpha to newlines sort sort in alphabetical order uniq c merge and count each type a aaron abate aaron abates abbess abbess abbot abbey abbot
NLP_ch2,19,dan jurafsky the first step tokenizing tr sc a za z n shakes txt head the sonnets by william shakespeare from fairest creatures we
NLP_ch2,20,dan jurafsky the second step sor ng tr sc a za z n shakes txt sort head a a a a a a a a a
NLP_ch2,21,dan jurafsky more coun ng merging upper and lower case tr a z a z shakes txt tr sc a za z n sort uniq c sorgng the counts tr a z a z shakes txt tr sc a za z n sort uniq c sort n r the i and to of a you what happened here my in d
NLP_ch2,22,dan jurafsky issues in tokeniza on finland s capital finland finlands finland s what re i m isn t what are i am is not hewlett packard hewlett packard state of the art state of the art lowercase lower case lowercase lower case san francisco one token or two m p h phd
NLP_ch2,23,dan jurafsky tokeniza on language issues french l ensemble one token or two l l le want l ensemble to match with un ensemble german noun compounds are not segmented lebensversicherungsgesellscha sangestellter life insurance company employee german informagon retrieval needs compound spli er
NLP_ch2,24,dan jurafsky tokeniza on language issues chinese and japanese no spaces between words 莎拉波娃现在居住在美国东南部的佛罗里达 莎拉波娃 现在 居住 在 美国 东南部 的 佛罗里达 sharapova now lives in us southeastern florida further complicated in japanese with mulgple alphabets intermingled dates amounts in mulgple formats フォーチュン 社は情報不足のため時間あた k 約 万円 katakana hiragana kanji romaji end user can express query engrely in hiragana
NLP_ch2,25,dan jurafsky word tokeniza on in chinese also called word segmenta on chinese words are composed of characters characters are generally syllable and morpheme average word is characters long standard baseline segmentagon algorithm maximum matching also called greedy
NLP_ch2,26,dan jurafsky maximum matching word segmenta on algorithm given a wordlist of chinese and a string start a pointer at the beginning of the string find the longest word in dicgonary that matches the string stargng at pointer move the pointer over the word in string go to
NLP_ch2,27,dan jurafsky max match segmenta on illustra on thecagnthehat the cat in the hat thetabledownthere the table down there theta bled own there doesn t generally work in english but works astonishingly well in chinese 莎拉波娃现在居住在美国东南部的佛罗里达 莎拉波娃 现在 居住 在 美国 东南部 的 佛罗里达 modern probabilisgc segmentagon algorithms even be er
NLP_ch2,28,basic text processing word tokenizagon
NLP_ch2,29,basic text processing word normalizagon and stemming
NLP_ch2,30,dan jurafsky normaliza on need to normalize terms informagon retrieval indexed text query terms must have same form we want to match u s a and usa we implicitly define equivalence classes of terms e g delegng periods in a term alternagve asymmetric expansion enter window search window windows enter windows search windows windows window enter windows search windows potengally more powerful but less efficient
NLP_ch2,31,dan jurafsky case folding applicagons like ir reduce all le ers to lower case since users tend to use lower case possible excepgon upper case in mid sentence e g general motors fed vs fed sail vs sail for sengment analysis mt informagon extracgon case is helpful us versus us is important
NLP_ch2,32,dan jurafsky lemma za on reduce inflecgons or variant forms to base form am are is be car cars car s cars car the boy s cars are different colors the boy car be different color lemmagzagon have to find correct dicgonary headword form machine translagon spanish quiero i want quieres you want same lemma as querer want
NLP_ch2,33,dan jurafsky morphology morphemes the small meaningful units that make up words stems the core meaning bearing units affixes bits and pieces that adhere to stems oden with grammagcal funcgons
NLP_ch2,34,dan jurafsky stemming reduce terms to their stems in informagon retrieval stemming is crude chopping of affixes language dependent e g automate s automagc automagon all reduced to automat for example compressed for exampl compress and and compression are both compress ar both accept accepted as equivalent to as equival to compress compress
NLP_ch2,35,dan jurafsky porter s algorithm the most common english stemmer step a step for long stems sses ss caresses caress ational ate relational relate ies i ponies poni izer ize digitizer digitize ss ss caress caress ator ate operator operate s ø cats cat step b step for longer stems v ing ø walking walk al ø revival reviv sing sing able ø adjustable adjust v ed ø plastered plaster ate ø activate activ
NLP_ch2,36,dan jurafsky viewing morphology in a corpus why only strip ing if there is a vowel v ing ø walking walk sing sing
NLP_ch2,37,dan jurafsky viewing morphology in a corpus why only strip ing if there is a vowel v ing ø walking walk sing sing tr sc a za z n shakes txt grep ing sort uniq c sort nr king being being nothing nothing something king coming bring morning thing having ring living something loving coming being morning going tr sc a za z n shakes txt grep aeiou ing sort uniq c sort nr
NLP_ch2,38,dan jurafsky dealing with complex morphology is some mes necessary some languages requires complex morpheme segmentagon turkish uygarlasgramadiklarimizdanmissinizcasina behaving as if you are among those whom we could not civilize uygar civilized las become gr cause ama not able dik past lar plural imiz p pl dan abl mis past siniz pl casina as if
NLP_ch2,39,basic text processing word normalizagon and stemming
NLP_ch2,40,basic text processing sentence segmentagon and decision trees
NLP_ch2,41,dan jurafsky sentence segmenta on are relagvely unambiguous period is quite ambiguous sentence boundary abbreviagons like inc or dr numbers like or build a binary classifier looks at a decides endofsentence notendofsentence classifiers hand wri en rules regular expressions or machine learning
NLP_ch2,42,dan jurafsky determining if a word is end of sentence a decision tree
NLP_ch2,43,dan jurafsky more sophis cated decision tree features case of word with upper lower cap number case of word ader upper lower cap number numeric features length of word with probability word with occurs at end of s probability word ader occurs at beginning of s
NLP_ch2,44,dan jurafsky implemen ng decision trees a decision tree is just an if then else statement the interesgng research is choosing the features se cid ng up the structure is oden too hard to do by hand hand building only possible for very simple features domains for numeric features it s too hard to pick each threshold instead structure usually learned by machine learning from a training corpus
NLP_ch2,45,dan jurafsky decision trees and other classifiers we can think of the quesgons in a decision tree as features that could be exploited by any kind of classifier logisgc regression svm neural nets etc
NLP_ch2,46,basic text processing sentence segmentagon and decision trees
NLP_ch3,1,minimum edit distance defini on of minimum edit distance
NLP_ch3,2,dan jurafsky how similar are two strings spell correc on computa onal biology the user typed graffe align two sequences of nucleo des which is closest aggctatcacctgacctccaggccgatgccc graf tagctatcacgaccgcggtcgatttgcccgac grab resul ng alignment grail giraffe aggctatcacctgacctccaggccga tgccc tag ctatcac gaccgc ggtcgatttgcccgac also for machine transla on informa on extrac on speech recogni on
NLP_ch3,3,dan jurafsky edit distance the minimum edit distance between two strings is the minimum number of edi ng opera ons inser on dele on subs tu on needed to transform one into the other
NLP_ch3,4,dan jurafsky minimum edit distance two strings and their alignment
NLP_ch3,5,dan jurafsky minimum edit distance if each opera on has cost of distance between these is if subs tu ons cost levenshtein distance between them is
NLP_ch3,6,dan jurafsky alignment in computa onal biology given a sequence of bases aggctatcacctgacctccaggccgatgccc tagctatcacgaccgcggtcgatttgcccgac an alignment aggctatcacctgacctccaggccga tgccc tag ctatcac gaccgc ggtcgatttgcccgac given two sequences align each lexer to a lexer or gap
NLP_ch3,7,dan jurafsky other uses of edit distance in nlp evalua ng machine transla on and speech recogni on r spokesman confirms senior government adviser was shot h spokesman said the senior adviser was shot dead s i d i named en ty extrac on and en ty coreference ibm inc announced today ibm profits stanford president john hennessy announced yesterday for stanford university president john hennessy
NLP_ch3,8,dan jurafsky how to find the min edit distance searching for a path sequence of edits from the start string to the final string ini al state the word we re transforming operators insert delete subs tute goal state the word we re trying to get to path cost what we want to minimize the number of edits
NLP_ch3,9,dan jurafsky minimum edit as search but the space of all edit sequences is huge we can t afford to navigate naïvely lots of dis nct paths wind up at the same state we don t have to keep track of all of them just the shortest path to each of those revisted states
NLP_ch3,10,dan jurafsky defining min edit distance for two strings x of length n y of length m we define d i j the edit distance between x i and y j i e the first i characters of x and the first j characters of y the edit distance between x and y is thus d n m
NLP_ch3,11,minimum edit distance defini on of minimum edit distance
NLP_ch3,12,minimum edit distance compu ng minimum edit distance
NLP_ch3,13,dan jurafsky dynamic programming for minimum edit distance dynamic programming a tabular computa on of d n m solving problems by combining solu ons to subproblems boxom up we compute d i j for small i j and compute larger d i j based on previously computed smaller values i e compute d i j for all i i n and j j m
NLP_ch3,14,dan jurafsky defining min edit distance levenshtein ini aliza on d i i d j j recurrence rela on for each i m for each j n d i j d i j min d i j d i j if x i y j if x i y j termina on d n m is distance
NLP_ch3,15,dan jurafsky the edit distance table n o i t n e t n i e x e c u t i o n
NLP_ch3,16,dan jurafsky the edit distance table n o i t n e t n i e x e c u t i o n
NLP_ch3,17,dan jurafsky edit distance n o i t n e t n i e x e c u t i o n
NLP_ch3,18,dan jurafsky the edit distance table n o i t n e t n i e x e c u t i o n
NLP_ch3,19,minimum edit distance compu ng minimum edit distance
NLP_ch3,20,minimum edit distance backtrace for compu ng alignments
NLP_ch3,21,dan jurafsky compu ng alignments edit distance isn t sufficient we oben need to align each character of the two strings to each other we do this by keeping a backtrace every me we enter a cell remember where we came from when we reach the end trace back the path from the upper right corner to read off the alignment
NLP_ch3,22,dan jurafsky edit distance n o i t n e t n i e x e c u t i o n
NLP_ch3,23,dan jurafsky minedit with backtrace
NLP_ch3,24,dan jurafsky adding backtrace to minimum edit distance base condi ons termina on d i i d j j d n m is distance recurrence rela on for each i m for each j n d i j dele on d i j min d i j inser on d i j if x i y j s ubs tu on if x i y j inser on left ptr i j down dele on diag subs tu on
NLP_ch3,25,dan jurafsky the distance matrix y cid cid cid cid cid cid cid cid cid cid cid cid y m slide adapted from serafim batzoglou x cid cid cid cid cid cid cid cid x n every non decreasing path from to m n corresponds to an alignment of the two sequences an optimal alignment is composed of optimal subalignments
NLP_ch3,26,dan jurafsky result of backtrace two strings and their alignment
NLP_ch3,27,dan jurafsky performance time o nm space o nm backtrace o n m
NLP_ch3,28,minimum edit distance backtrace for compu ng alignments
NLP_ch3,29,minimum edit distance weighted minimum edit distance
NLP_ch3,30,dan jurafsky weighted edit distance why would we add weights to the computa on spell correc on some lexers are more likely to be mistyped than others biology certain kinds of dele ons or inser ons are more likely than others
NLP_ch3,31,dan jurafsky confusion matrix for spelling errors
NLP_ch3,32,dan jurafsky
NLP_ch3,33,dan jurafsky weighted min edit distance ini aliza on d d i d i del x i i n d j d j ins y j j m recurrence rela on d i j del x i d i j min d i j ins y j d i j sub x i y j termina on d n m is distance
NLP_ch3,34,dan jurafsky where did the name dynamic programming come from the s were not good years for mathematical research the secretary of defense had a pathological fear and hatred of the word research i decided therefore to use the word programming i wanted to get across the idea that this was dynamic this was multistage i thought let s take a word that has an absolutely precise meaning namely dynamic it s impossible to use the word dynamic in a pejorative sense try thinking of some combination that will possibly give it a pejorative meaning it s impossible thus i thought dynamic programming was a good name it was something not even a congressman could object to richard bellman eye of the hurricane an autobiography
NLP_ch3,35,minimum edit distance weighted minimum edit distance
NLP_ch3,36,minimum edit distance minimum edit distance in computa onal biology
NLP_ch3,37,dan jurafsky sequence alignment aggctatcacctgacctccaggccgatgccc tagctatcacgaccgcggtcgatttgcccgac aggctatcacctgacctccaggccga tgccc tag ctatcac gaccgc ggtcgatttgcccgac
NLP_ch3,38,dan jurafsky why sequence alignment comparing genes or regions from different species to find important regions determine func on uncover evolu onary forces assembling fragments to sequence dna compare individuals to looking for muta ons
NLP_ch3,39,dan jurafsky alignments in two fields in natural language processing we generally talk about distance minimized and weights in computa onal biology we generally talk about similarity maximized and scores
NLP_ch3,40,dan jurafsky the needleman wunsch algorithm ini aliza on d i i d d j j d recurrence rela on d i j d d i j min d i j d d i j s x i y j termina on d n m is distance
NLP_ch3,41,dan jurafsky the needleman wunsch matrix x cid cid cid cid cid cid cid cid cid cid cid cid x m slide adapted from serafim batzoglou n y cid cid cid cid cid cid cid cid y note that the origin is at the upper leb
NLP_ch3,42,dan jurafsky a variant of the basic algorithm maybe it is ok to have an unlimited of gaps in the beginning and end ctatcacctgacctccaggccgatgccccttccggc gcgagttcatctatcac gaccgc ggtcg if so we don t want to penalize gaps at the ends slide from serafim batzoglou
NLP_ch3,43,dan jurafsky different types of overlaps example overlapping reads from a sequencing project example search for a mouse gene within a human chromosome slide from serafim batzoglou
NLP_ch3,44,dan jurafsky the overlap detec on variant x cid cid cid cid cid cid cid cid cid cid cid cid x changes m ini aliza on for all i j f i f j termina on max f i n i f max opt max f m j j slide from serafim batzoglou y cid cid cid cid cid cid cid cid y n
NLP_ch3,45,dan jurafsky the local alignment problem given two strings x x x m y y y n find substrings x y whose similarity op mal global alignment value is maximum x aaaacccccggggxa y xcccgggaaccaacc slide from serafim batzoglou
NLP_ch3,46,dan jurafsky the smith waterman algorithm idea ignore badly aligning regions modifica ons to needleman wunsch ini aliza on f j f i itera on f i j max f i j d f i j d f i j s x y i j slide from serafim batzoglou
NLP_ch3,47,dan jurafsky the smith waterman algorithm termina on if we want the best local alignment f max f i j opt i j find f and trace back opt if we want all local alignments scoring t for all i j find f i j t and trace back complicated by overlapping local alignments slide from serafim batzoglou
NLP_ch3,48,dan jurafsky local alignment example a t t a t c x atcat y attatc a t let m point for match c d point for del ins sub a t
NLP_ch3,49,dan jurafsky local alignment example a t t a t c x atcat y attatc a t c a t
NLP_ch3,50,dan jurafsky local alignment example a t t a t c x atcat y attatc a t c a t
NLP_ch3,51,dan jurafsky local alignment example a t t a t c x atcat y attatc a t c a t
NLP_ch3,52,minimum edit distance minimum edit distance in computa onal biology
NLP_ch4,1,language modeling introduc on to n grams
NLP_ch4,2,dan jurafsky probabilis c language models today s goal assign a probability to a sentence machine transla on p high winds tonite p large winds tonite spell correc on why the office is about fiieen minuets from my house p about fiieen minutes from p about fiieen minuets from speech recogni on p i saw a van p eyes awe of an summariza on ques on answering etc etc
NLP_ch4,3,dan jurafsky probabilis c language modeling goal compute the probability of a sentence or sequence of words p w p w w w w w w n related task probability of an upcoming word p w w w w w a model that computes either of these p w or p w w w w is called a language model n n be_er the grammar but language model or lm is standard
NLP_ch4,4,dan jurafsky how to compute p w how to compute this joint probability p its water is so transparent that intui on let s rely on the chain rule of probability
NLP_ch4,5,dan jurafsky reminder the chain rule recall the defini on of condi onal probabili es rewri ng more variables p a b c d p a p b a p c a b p d a b c the chain rule in general p x x x x p x p x x p x x x p x x x n n n
NLP_ch4,6,dan jurafsky the chain rule applied to compute joint probability of words in sentence p w w w p w w w w n i i i p its water is so transparent p its p water its p is its water p so its water is p transparent its water is so
NLP_ch4,7,dan jurafsky how to es mate these probabili es could we just count and divide p the its water is so transparent that count its water is so transparent that the count its water is so transparent that no too many possible sentences we ll never see enough data for es ma ng these
NLP_ch4,8,dan jurafsky markov assump on simplifying assump on andrei markov p the its water is so transparent that p the that or maybe p the its water is so transparent that p the transparent that
NLP_ch4,9,dan jurafsky markov assump on p w w w p w w w n i i k i i in other words we approximate each component in the product p w w w w p w w w i i i i k i
NLP_ch4,10,dan jurafsky simplest case unigram model p w w w p w n i i some automa cally generated sentences from a unigram model fifth an of futures the an incorporated a a the inflation most dollars quarter in is mass thrift did eighty said hard m july bullish that or limited the
NLP_ch4,11,dan jurafsky bigram model condi on on the previous word p w w w w p w w i i i i texaco rose one in this issue is pursuing growth in a boiler house said mr gurria mexico s motion control proposal without permission from five hundred fifty five yen outside new car parking lot of the agreement reached this would be a record november
NLP_ch4,12,dan jurafsky n gram models we can extend to trigrams grams grams in general this is an insufficient model of language because language has long distance dependencies the computer which i had just put into the machine room on the fiih floor crashed but we can oien get away with n gram models
NLP_ch4,13,language modeling introduc on to n grams
NLP_ch4,14,language modeling es ma ng n gram probabili es
NLP_ch4,15,dan jurafsky es ma ng bigram probabili es the maximum likelihood es mate count w w p w w i i i i count w i c w w p w w i i i i c w i
NLP_ch4,16,dan jurafsky an example s i am sam s c w w p w w i i s sam i am s i i c w s i do not like green eggs and ham s i
NLP_ch4,17,dan jurafsky more examples berkeley restaurant project sentences can you tell me about any good cantonese restaurants close by mid priced thai food is what i m looking for tell me about chez panisse can you give me a lis ng of the kinds of food that are available i m looking for a good place to eat breakfast when is caffe venezia open during the day
NLP_ch4,18,dan jurafsky raw bigram counts out of sentences
NLP_ch4,19,dan jurafsky raw bigram probabili es normalize by unigrams result
NLP_ch4,20,dan jurafsky bigram es mates of sentence probabili es p s i want english food s p i s p want i p english want p food english p s food
NLP_ch4,21,dan jurafsky what kinds of knowledge p english want p chinese want p to want p eat to p food to p want spend p i s
NLP_ch4,22,dan jurafsky prac cal issues we do everything in log space avoid underflow also adding is faster than mul plying log p p p p log p log p log p log p
NLP_ch4,23,dan jurafsky language modeling toolkits srilm h_p www speech sri com projects srilm
NLP_ch4,24,dan jurafsky google n gram release august
NLP_ch4,25,dan jurafsky google n gram release serve as the incoming serve as the incubator serve as the independent serve as the index serve as the indication serve as the indicator serve as the indicators serve as the indispensable serve as the indispensible serve as the individual http googleresearch blogspot com all our n gram are belong to you html
NLP_ch4,26,dan jurafsky google book n grams h_p ngrams googlelabs com
NLP_ch4,27,language modeling es ma ng n gram probabili es
NLP_ch4,28,language modeling evalua on and perplexity
NLP_ch4,29,dan jurafsky evalua on how good is our model does our language model prefer good sentences to bad ones assign higher probability to real or frequently observed sentences than ungramma cal or rarely observed sentences we train parameters of our model on a training set we test the model s performance on data we haven t seen a test set is an unseen dataset that is different from our training set totally unused an evalua on metric tells us how well our model does on the test set
NLP_ch4,30,dan jurafsky extrinsic evalua on of n gram models best evalua on for comparing models a and b put each model in a task spelling corrector speech recognizer mt system run the task get an accuracy for a and for b how many misspelled words corrected properly how many words translated correctly compare accuracy for a and b
NLP_ch4,31,dan jurafsky difficulty of extrinsic in vivo evalua on of n gram models extrinsic evalua on time consuming can take days or weeks so some mes use intrinsic evalua on perplexity bad approxima on unless the test data looks just like the training data so generally only useful in pilot experiments but is helpful to think about
NLP_ch4,32,dan jurafsky intui on of perplexity mushrooms the shannon game pepperoni how well can we predict the next word anchovies i always order pizza with cheese and ____ the rd president of the us was ____ fried rice i saw a ____ unigrams are terrible at this game why and e a be_er model of a text is one which assigns a higher probability to the word that actually occurs
NLP_ch4,33,dan jurafsky perplexity the best language model is one that best predicts an unseen test set gives the highest p sentence pp w p w w w n perplexity is the inverse probability of n the test set normalized by the number of words n p w w w n chain rule for bigrams minimizing perplexity is the same as maximizing probability
NLP_ch4,34,dan jurafsky the shannon game intui on for perplexity from josh goodman how hard is the task of recognizing digits perplexity how hard is recognizing names at microsoi perplexity if a system has to recognize operator in sales in technical support in names in each perplexity is perplexity is weighted equivalent branching factor
NLP_ch4,35,dan jurafsky perplexity as branching factor let s suppose a sentence consis ng of random digits what is the perplexity of this sentence according to a model that assign p to each digit
NLP_ch4,36,dan jurafsky lower perplexity bexer model training million words test million words wsj n gram unigram bigram trigram order perplexity
NLP_ch4,37,language modeling evalua on and perplexity
NLP_ch4,38,language modeling generaliza on and zeros
NLP_ch4,39,dan jurafsky the shannon visualiza on method choose a random bigram s i s w according to its probability i want now choose a random bigram want to w x according to its probability to eat and so on un l we choose s eat chinese then string the words together chinese food food s i want to eat chinese food
NLP_ch4,40,dan jurafsky approxima ng shakespeare
NLP_ch4,41,dan jurafsky shakespeare as corpus n tokens v shakespeare produced bigram types out of v million possible bigrams so of the possible bigrams were never seen have zero entries in the table quadrigrams worse what s coming out looks like shakespeare because it is shakespeare
NLP_ch4,42,dan jurafsky the wall street journal is not shakespeare no offense
NLP_ch4,43,dan jurafsky the perils of overfi ng n grams only work well for word predic on if the test corpus looks like the training corpus in real life it oien doesn t we need to train robust models that generalize one kind of generaliza on zeros things that don t ever occur in the training set but occur in the test set
NLP_ch4,44,dan jurafsky zeros training set test set denied the allega ons denied the offer denied the reports denied the loan denied the claims denied the request p offer denied the
NLP_ch4,45,dan jurafsky zero probability bigrams bigrams with zero probability mean that we will assign probability to the test set and hence we cannot compute perplexity can t divide by
NLP_ch4,46,language modeling generaliza on and zeros
NLP_ch4,47,language modeling smoothing add one laplace smoothing
NLP_ch4,48,dan jurafsky the intuition of smoothing from dan klein when we have sparse sta s cs p w denied the allega ons reports claims request total steal probability mass to generalize be_er p w denied the allega ons reports claims request other total snoitagella stroper smialc tseuqer kcatta nam emoctuo snoitagella kcatta nam emoctuo snoitagella stroper smialc tseuqer
NLP_ch4,49,dan jurafsky add one es ma on also called laplace smoothing pretend we saw each word one more me than we did just add one to all the counts c w w p w w i i mle i i c w mle es mate i c w w p w w i i add es mate add i i c w v i
NLP_ch4,50,dan jurafsky maximum likelihood es mates the maximum likelihood es mate of some parameter of a model m from a training set t maximizes the likelihood of the training set t given the model m suppose the word bagel occurs mes in a corpus of a million words what is the probability that a random word from some other text will be bagel mle es mate is this may be a bad es mate for some other corpus but it is the es mate that makes it most likely that bagel will occur mes in a million word corpus
NLP_ch4,51,dan jurafsky berkeley restaurant corpus laplace smoothed bigram counts
NLP_ch4,52,dan jurafsky laplace smoothed bigrams
NLP_ch4,53,dan jurafsky reconstituted counts
NLP_ch4,54,dan jurafsky compare with raw bigram counts
NLP_ch4,55,dan jurafsky add es ma on is a blunt instrument so add isn t used for n grams we ll see be_er methods but add is used to smooth other nlp models for text classifica on in domains where the number of zeros isn t so huge
NLP_ch4,56,language modeling smoothing add one laplace smoothing
NLP_ch4,57,language modeling interpola on backoff and web scale lms
NLP_ch4,58,dan jurafsky backoff and interpolation some mes it helps to use less context condi on on less context for contexts you haven t learned much about backoff use trigram if you have good evidence otherwise bigram otherwise unigram interpola on mix unigram bigram trigram interpola on works be_er
NLP_ch4,59,dan jurafsky linear interpola on simple interpola on lambdas condi onal on context
NLP_ch4,60,dan jurafsky how to set the lambdas use a held out corpus held out test training data data data choose λs to maximize the probability of held out data fix the n gram probabili es on the training data then search for λs that give largest probability to held out set log p w w m log p w w n k m i i k i
NLP_ch4,61,dan jurafsky unknown words open versus closed vocabulary tasks if we know all the words in advanced vocabulary v is fixed closed vocabulary task oien we don t know this out of vocabulary oov words open vocabulary task instead create an unknown word token unk training of unk probabili es create a fixed lexicon l of size v at text normaliza on phase any training word not in l changed to unk now we train its probabili es like a normal word at decoding me if text input use unk probabili es for any word not in training
NLP_ch4,62,dan jurafsky huge web scale n grams how to deal with e g google n gram corpus pruning only store n grams with count threshold remove singletons of higher order n grams entropy based pruning efficiency efficient data structures like tries bloom filters approximate language models store words as indexes not strings use huffman coding to fit large numbers of words into two bytes quan ze probabili es bits instead of byte float
NLP_ch4,63,dan jurafsky smoothing for web scale n grams stupid backoff brants et al no discoun ng just use rela ve frequencies count wi i k if count wi s w wi count wi i k i i k i k s w wi otherwise i i k count w s w i i n
NLP_ch4,64,dan jurafsky n gram smoothing summary add smoothing ok for text categoriza on not for language modeling the most commonly used method extended interpolated kneser ney for very large n grams like the web stupid backoff
NLP_ch4,65,dan jurafsky advanced language modeling discrimina ve models choose n gram weights to improve a task not to fit the training set parsing based models caching models recently used words are more likely to appear c w history p w history p w w w cache i i i history these perform very poorly for speech recogni on why
NLP_ch4,66,language modeling interpola on backoff and web scale lms
NLP_ch4,67,language modeling advanced good turing smoothing
NLP_ch4,68,dan jurafsky reminder add laplace smoothing c w w p w w i i add i i c w v i
NLP_ch4,69,dan jurafsky more general formulations add k c w w k p w w i i add k i i c w kv i c w w m i i v p w w add k i i c w m i
NLP_ch4,70,dan jurafsky unigram prior smoothing c w w m i i v p w w add k i i c w m i c w w mp w p w w i i i unigramprior i i c w m i
NLP_ch4,71,dan jurafsky advanced smoothing algorithms intui on used by many smoothing algorithms good turing kneser ney wi_en bell use the count of things we ve seen once to help es mate the count of things we ve never seen
NLP_ch4,72,dan jurafsky nota on n frequency of frequency c c n the count of things we ve seen c mes c sam i am i am sam i do not eat i sam n am do n not n eat
NLP_ch4,73,dan jurafsky good turing smoothing intuition you are fishing a scenario from josh goodman and caught carp perch whitefish trout salmon eel fish how likely is it that next species is trout how likely is it that next species is new i e ca cid ish or bass let s use our es mate of things we saw once to es mate the new things because n assuming so how likely is it that next species is trout must be less than how to es mate
NLP_ch4,74,dan jurafsky good turing calculations n c n p things with zero frequency c c gt n n c unseen bass or catfish seen once trout c c mle p mle p p unseen n n c trout n n gt p trout gt
NLP_ch4,75,dan jurafsky ney et al s good turing intuition h ney u essen and r kneser on the es ma on of small probabili es by leaving one out ieee trans pami held out words
NLP_ch4,76,dan jurafsky ney et al good turing intuition slide from dan klein n intui on from leave one out valida on take each of the c training words out in turn c training sets of size c held out of size n what frac on of held out words are unseen in training n c what frac on of held out words are seen k mes in n training k n c k so in the future we expect k n c of the words to be k those with training count k there are n words with training count k k each should occur with probability k n c n n k k k n or expected count k k n n k n n n n n training held out
NLP_ch4,77,dan jurafsky good turing complications slide from dan klein problem what about the say c for small k n n k k n for large k too jumpy zeros wreck n n es mates simple good turing gale and sampson replace empirical n with a k best fit power law once counts get n unreliable n
NLP_ch4,78,dan jurafsky resulting good turing numbers numbers from church and gale count c good turing c million words of ap newswire c n c c n c
NLP_ch4,79,language modeling advanced good turing smoothing
NLP_ch4,80,language modeling advanced kneser ney smoothing
NLP_ch4,81,dan jurafsky resulting good turing numbers numbers from church and gale count c good turing c million words of ap newswire c n c c n c it sure looks like c c
NLP_ch4,82,dan jurafsky absolute discounting interpolation save ourselves some me and just subtract or some d discounted bigram interpolation weight c w w d p w w i i w p w absolutediscounting i i i c w i unigram maybe keeping a couple extra values of d for counts and but should we really just use the regular unigram p w
NLP_ch4,83,dan jurafsky kneser ney smoothing i be_er es mate for probabili es of lower order unigrams shannon game i can t see without my reading___ fg _ ral _ an _ scs _ ies _ sc _ o _ _ francisco is more common than glasses but francisco always follows san the unigram is useful exactly when we haven t seen this bigram instead of p w how likely is w p w how likely is w to appear as a novel con nua on con nua on for each word count the number of bigram types it completes every bigram type was a novel con nua on the first me it was seen p w w c w w continuation i i
NLP_ch4,84,dan jurafsky kneser ney smoothing ii how many mes does w appear as a novel con nua on p w w c w w continuation i i normalized by the total number of word bigram types w w c w w j j j j w c w w i i p w continuation w w c w w j j j j
NLP_ch4,85,dan jurafsky kneser ney smoothing iii alterna ve metaphor the number of of word types seen to precede w w c w w i i normalized by the of words preceding all words w c w w p w i i continuation w c w w i i w a frequent word francisco occurring in only one context san will have a low con nua on probability
NLP_ch4,86,dan jurafsky kneser ney smoothing iv max c w w d p w w i i w p w kn i i i continuation i c w i λ is a normalizing constant the probability mass we ve discounted d w w c w w i i c w i the number of word types that can follow w i the normalized discount of word types we discounted of times we applied normalized discount
NLP_ch4,87,dan jurafsky kneser ney smoothing recursive formulation i max c w d p w w i kn i n w i p w w i kn i i n i i n kn i i n c w kn i n count for the highest order c kn continuationcount for lower order continuation count number of unique single word contexts for
NLP_ch4,88,language modeling advanced kneser ney smoothing
NLP_ch5,1,spelling correction and the noisy channel the spelling correc on task
NLP_ch5,2,dan jurafsky applica ons for spelling correc on word processing phones web search
NLP_ch5,3,dan jurafsky spelling tasks spelling error detec on spelling error correc on autocorrect hteà the suggest a correc on sugges on lists
NLP_ch5,4,dan jurafsky types of spelling errors non word errors graffe à giraffe real word errors typographical errors three à there cogni ve errors homophones pieceà peace too à two
NLP_ch5,5,dan jurafsky rates of spelling errors web queries wang et al retyping no backspace whitelaw et al english german words corrected retyping on phone sized organizer words uncorrected on organizer soukoreff mackenzie retyping kane and wobbrock gruden et al
NLP_ch5,6,dan jurafsky non word spelling errors non word spelling error detec on any word not in a dic onary is an error the larger the dic onary the be er non word spelling error correc on generate candidates real words that are similar to error choose the one which is best shortest weighted edit distance highest noisy channel probability
NLP_ch5,7,dan jurafsky real word spelling errors for each word w generate candidate set find candidate words with similar pronuncia ons find candidate words with similar spelling include w in candidate set choose best candidate noisy channel classifier
NLP_ch5,8,spelling correction and the noisy channel the spelling correc on task
NLP_ch5,9,spelling correction and the noisy channel the noisy channel model of spelling
NLP_ch5,10,dan jurafsky noisy channel intui on
NLP_ch5,11,dan jurafsky noisy channel we see an observa on x of a misspelled word find the correct word w wˆ argmax p w x w v p x w p w argmax p x w v argmax p x w p w w v
NLP_ch5,12,dan jurafsky history noisy channel for spelling proposed around ibm mays eric fred j damerau and robert l mercer context based spelling correc on informa on processing and management at t bell labs kernighan mark d kenneth w church and william a gale a spelling correc on program based on a noisy channel model proceedings of coling
NLP_ch5,13,dan jurafsky non word spelling error example acress
NLP_ch5,14,dan jurafsky candidate genera on words with similar spelling small edit distance to error words with similar pronuncia on small edit distance of pronuncia on to error
NLP_ch5,15,dan jurafsky damerau levenshtein edit distance minimal edit distance between two strings where edits are inser on dele on subs tu on transposi on of two adjacent le ers
NLP_ch5,16,dan jurafsky words within of acress error candidate correct error type correc on lerer lerer acress actress t dele on acress cress a inser on acress caress ca ac transposi on acress access c r subs tu on acress across o e subs tu on acress acres s inser on acress acres s inser on
NLP_ch5,17,dan jurafsky candidate genera on of errors are within edit distance almost all errors within edit distance also allow inser on of space or hyphen thisidea à this idea inlaw à in law
NLP_ch5,18,dan jurafsky language model use any of the language modeling algorithms we ve learned unigram bigram trigram web scale spelling correc on stupid backoff
NLP_ch5,19,dan jurafsky unigram prior probability counts from words in corpus of contemporary english coca word frequency of word p word actress cress caress access across acres
NLP_ch5,20,dan jurafsky channel model probability error model probability edit probability kernighan church gale misspelled word x x x x x m correct word w w w w w n p x w probability of the edit dele on inser on subs tu on transposi on
NLP_ch5,21,dan jurafsky compu ng error probability confusion matrix del x y count xy typed as x ins x y count x typed as xy sub x y count x typed as y trans x y count xy typed as yx inser on and dele on condi oned on previous character
NLP_ch5,22,dan jurafsky confusion matrix for spelling errors
NLP_ch5,23,dan jurafsky genera ng the confusion matrix peter norvig s list of errors peter norvig s list of counts of single edit errors
NLP_ch5,24,dan jurafsky channel model kernighan church gale del w w i i if deletion count w w i i ins w x i i if insertion count w i p x w sub x w i i if substitution count w i trans w w i i if transposition count w w i i
NLP_ch5,25,dan jurafsky channel model for acress candidate correct error x w p x word correc on lerer lerer actress t c ct cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
NLP_ch5,26,dan jurafsky noisy channel probability for acress candidate correct error x w p x word p word p x w p w correc on lerer lerer actress t c ct cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
NLP_ch5,27,dan jurafsky noisy channel probability for acress candidate correct error x w p x word p word p x w p w correc on lerer lerer actress t c ct cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
NLP_ch5,28,dan jurafsky using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
NLP_ch5,29,dan jurafsky using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
NLP_ch5,30,dan jurafsky evalua on some spelling error test sets wikipedia s list of common english misspelling aspell filtered version of that list birkbeck spelling error corpus peter norvig s list of errors includes wikipedia and birkbeck for training or tes ng
NLP_ch5,31,spelling correction and the noisy channel the noisy channel model of spelling
NLP_ch5,32,spelling correction and the noisy channel real word spelling correc on
NLP_ch5,33,dan jurafsky real word spelling errors leaving in about fifteen minuets to go to her house the design an construction of the system can they lave him my messages the study was conducted mainly be john black of spelling errors are real words kukich
NLP_ch5,34,dan jurafsky solving real world spelling errors for each word in sentence generate candidate set the word itself all single le er edits that are english words words that are homophones choose best candidates noisy channel model task specific classifier
NLP_ch5,35,dan jurafsky noisy channel for real word spell correc on given a sentence w w w w n generate a set of candidates for each word w i candidate w w w w w candidate w w w w w candidate w w w w w n n n n n choose the sequence w that maximizes p w
NLP_ch5,36,dan jurafsky noisy channel for real word spell correc on two of thew to threw tao off thaw too on the two of thaw
NLP_ch5,37,dan jurafsky noisy channel for real word spell correc on two of thew to threw tao off thaw too on the two of thaw
NLP_ch5,38,dan jurafsky simplifica on one error per sentence out of all possible sentences with one word replaced w w w w two off thew w w w w two of the w w w w too of thew choose the sequence w that maximizes p w
NLP_ch5,39,dan jurafsky where to get the probabili es language model unigram bigram etc channel model same as for non word spelling correc on plus need probability for no error p w w
NLP_ch5,40,dan jurafsky probability of no error what is the channel probability for a correctly typed word p the the obviously this depends on the applica on error in words error in words error in words error in words
NLP_ch5,41,dan jurafsky peter norvig s thew example x w x w p x w p w p x w p w thew the ew e thew thew thew thaw e a thew threw h hr thew thwe ew we
NLP_ch5,42,spelling correction and the noisy channel real word spelling correc on
NLP_ch5,43,spelling correction and the noisy channel state of the art systems
NLP_ch5,44,dan jurafsky hci issues in spelling if very confident in correc on autocorrect less confident give the best correc on less confident give a correc on list unconfident just flag as an error
NLP_ch5,45,dan jurafsky state of the art noisy channel we never just mul ply the prior and the error model independence assump onsà probabili es not commensurate instead weigh them wˆ argmax p x w p w w v learn λ from a development test set
NLP_ch5,46,dan jurafsky phone c error model metaphone used in gnu aspell convert misspelling to metaphone pronuncia on drop duplicate adjacent le ers except for c if the word begins with kn gn pn ae wr drop the first le er drop b if aver m and if it is at the end of the word find words whose pronuncia on is edit distance from misspelling s score result list weighted edit distance of candidate to misspelling edit distance of candidate pronuncia on to misspelling pronuncia on
NLP_ch5,47,dan jurafsky improvements to channel model allow richer edits brill and moore entà ant phà f leà al incorporate pronuncia on into channel toutanova and moore
NLP_ch5,48,dan jurafsky channel model factors that could influence p misspelling word the source le er the target le er surrounding le ers the posi on in the word nearby keys on the keyboard homology on the keyboard pronuncia ons likely morpheme transforma ons
NLP_ch5,49,dan jurafsky nearby keys
NLP_ch5,50,dan jurafsky classifier based methods for real word spelling correc on instead of just channel model and language model use many features in a classifier next lecture build a classifier for a specific pair like whether weather cloudy within words ___ to verb ___ or not
NLP_ch5,51,spelling correction and the noisy channel real word spelling correc on
NLP_ch6,1,text classification and naïve bayes the task of text classifica on
NLP_ch6,2,dan jurafsky is this spam
NLP_ch6,3,dan jurafsky who wrote which federalist papers anonymous essays try to convince new york to ra fy u s cons tu on jay madison hamilton authorship of of the lelers in dispute solved by mosteller and wallace using bayesian methods james madison alexander hamilton
NLP_ch6,4,dan jurafsky male or female author by present day vietnam was divided into three parts under french colonial rule the southern region embracing saigon and the mekong delta was the colony of cochin china the central area with its imperial capital at hue was the protectorate of annam clara never failed to be astonished by the extraordinary felicity of her own name she found it hard to trust herself to the mercy of fate which had managed over the years to convert her greatest shame into one of her greatest assets s argamon m koppel j fine a r shimoni gender genre and wri ng style in formal wrilen texts text volume number pp
NLP_ch6,5,dan jurafsky posi ve or nega ve movie review unbelievably disappoin ng full of zany characters and richly applied sa re and some great plot twists this is the greatest screwball comedy ever filmed it was pathe c the worst part about it was the boxing scenes
NLP_ch6,6,dan jurafsky what is the subject of this ar cle medline article mesh subject category hierarchy antogonists and inhibitors blood supply chemistry drug therapy embryology epidemiology
NLP_ch6,7,dan jurafsky text classifica on assigning subject categories topics or genres spam detec on authorship iden fica on age gender iden fica on language iden fica on sen ment analysis
NLP_ch6,8,dan jurafsky text classifica on defini on input a document d a fixed set of classes c c c c j output a predicted class c c
NLP_ch6,9,dan jurafsky classifica on methods hand coded rules rules based on combina ons of words or other features spam black list address or dollars and have been selected accuracy can be high if rules carefully refined by expert but building and maintaining these rules is expensive
NLP_ch6,10,dan jurafsky classifica on methods supervised machine learning input a document d a fixed set of classes c c c c j a training set of m hand labeled documents d c d c m m output a learned classifier γ d à c
NLP_ch6,11,dan jurafsky classifica on methods supervised machine learning any kind of classifier naïve bayes logis c regression support vector machines k nearest neighbors
NLP_ch6,12,text classification and naïve bayes the task of text classifica on
NLP_ch6,13,text classification and naïve bayes naïve bayes i
NLP_ch6,14,dan jurafsky naïve bayes intui on simple naïve classifica on method based on bayes rule relies on very simple representa on of document bag of words
NLP_ch6,15,dan jurafsky the bag of words representa on i love this movie it s sweet but with satirical humor the dialogue is great and the γ adventure scenes are fun it c manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre i would recommend it to just about anyone i ve seen it several times and i m always happy to see it again whenever i have a friend who hasn t seen it yet
NLP_ch6,16,dan jurafsky the bag of words representa on i love this movie it s sweet but with satirical humor the dialogue is great and the γ adventure scenes are fun it c manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre i would recommend it to just about anyone i ve seen it several times and i m always happy to see it again whenever i have a friend who hasn t seen it yet
NLP_ch6,17,dan jurafsky the bag of words representa on using a subset of words x love xxxxxxxxxxxxxxxx sweet xxxxxxx satirical xxxxxxxxxx xxxxxxxxxxx great xxxxxxx γ xxxxxxxxxxxxxxxxxxx fun xxxx c xxxxxxxxxxxxx whimsical xxxx romantic xxxx laughing xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxx recommend xxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xx several xxxxxxxxxxxxxxxxx xxxxx happy xxxxxxxxx again xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxx
NLP_ch6,18,dan jurafsky the bag of words representa on great love γ c recommend laugh happy
NLP_ch6,19,dan jurafsky bag of words for document classifica on test document machine garbage parser nlp planning gui learning collection language learning parser garbage planning label training tag collection temporal translation algorithm training memory reasoning shrinkage translation optimization plan network language region language
NLP_ch6,20,text classification and naïve bayes naïve bayes i
NLP_ch6,21,text classification and naïve bayes formalizing the naïve bayes classifier
NLP_ch6,22,dan jurafsky bayes rule applied to documents and classes for a document d and a class c p d c p c p c d p d
NLP_ch6,23,dan jurafsky naïve bayes classifier i map is maximum a c argmax p c d posteriori most map likely class c c p d c p c argmax bayes rule p d c c argmax p d c p c dropping the denominator c c
NLP_ch6,24,dan jurafsky naïve bayes classifier ii c argmax p d c p c map c c document d argmax p x x x c p c represented as n features c c x xn
NLP_ch6,25,dan jurafsky naïve bayes classifier iv c argmax p x x x c p c map n c c o x n c parameters how often does this class occur could only be es mated if a we can just count the very very large number of relative frequencies in training examples was a corpus available
NLP_ch6,26,dan jurafsky mul nomial naïve bayes independence assump ons p x x x c n bag of words assump on assume posi on doesn t maler condi onal independence assume the feature probabili es p x c are independent given the class c i j p x x c p x c p x c p x c p x c n n
NLP_ch6,27,dan jurafsky mul nomial naïve bayes classifier c argmax p x x x c p c map n c c c argmax p c p x c nb j c c x x
NLP_ch6,28,dan jurafsky applying mul nomial naive bayes classifiers to text classifica on positions all word posi ons in test document c argmax p c p x c nb j i j c c j i positions
NLP_ch6,29,text classification and naïve bayes formalizing the naïve bayes classifier
NLP_ch6,30,text classification and naïve bayes naïve bayes learning
NLP_ch6,31,sec dan jurafsky learning the mul nomial naïve bayes model first alempt maximum likelihood es mates simply use the frequencies in the data doccount c c ˆ j p c j n doc count w c ˆ i j p w c i j count w c j w v
NLP_ch6,32,dan jurafsky parameter es ma on count w c frac on of mes word w appears ˆ i j p w c i i j count w c among all words in documents of topic c j j w v create mega document for topic j by concatena ng all docs in this topic use frequency of w in mega document
NLP_ch6,33,sec dan jurafsky problem with maximum likelihood what if we have seen no training documents with the word fantas c and classified in the topic posi ve thumbs up count fantastic positive ˆ p fantastic positive count w positive w v zero probabili es cannot be condi oned away no maler the other evidence ˆ ˆ c argmax p c p x c map c i i
NLP_ch6,34,dan jurafsky laplace add smoothing for naïve bayes ccoouunntt ww cc ˆˆ pp ww cc ii ii ccoouunntt ww cc ww vv count w c i count w c v w v
NLP_ch6,35,dan jurafsky mul nomial naïve bayes learning from training corpus extract vocabulary calculate p c terms calculate p w c terms j k j for each c in c do text single doc containing all docs j j j for each word w in vocabulary docs all docs with class c k j j n of occurrences of w in text docs k k j p c j n j total documents p w c k k j n vocabulary
NLP_ch6,36,dan jurafsky laplace add smoothing unknown words add one extra word to the vocabulary the unknown word w u count w c ˆ p w c u u count w c v w v count w c v w v
NLP_ch6,37,text classification and naïve bayes naïve bayes learning
NLP_ch6,38,text classification and naïve bayes naïve bayes rela onship to language modeling
NLP_ch6,39,dan jurafsky genera ve model for mul nomial naïve bayes c china x shanghai x and x shenzhen x issue x bonds
NLP_ch6,40,dan jurafsky naïve bayes and language modeling naïve bayes classifiers can use any sort of feature url email address dic onaries network features but if as in the previous slides we use only word features we use all of the words in the text not a subset then naïve bayes has an important similarity to language modeling
NLP_ch6,41,sec dan jurafsky each class a unigram language model assigning each word p word c assigning each sentence p s c π p word c class pos i i love this fun film love this fun film p s pos
NLP_ch6,42,sec dan jurafsky naïve bayes as a language model which class assigns the higher probability to s model pos model neg i i i love this fun film love love this this fun fun p s pos p s neg film film
NLP_ch6,43,text classification and naïve bayes naïve bayes rela onship to language modeling
NLP_ch6,44,text classification and naïve bayes mul nomial naïve bayes a worked example
NLP_ch6,45,dan jurafsky doc words class n ˆ training chinese beijing chinese c p c c n chinese chinese shanghai c chinese macao c count w c ˆ tokyo japan chinese j p w c count c v test chinese chinese chinese tokyo japan priors p c choosing a class p j p c d condi onal probabili es p chinese c p tokyo c p j d p japan c p chinese j p tokyo j p japan j
NLP_ch6,46,dan jurafsky naïve bayes in spam filtering spamassassin features men ons generic viagra online pharmacy men ons millions of dollar dollar nn nnn nnn nn phrase impress girl from starts with many numbers subject is all capitals html has a low ra o of text to image area one hundred percent guaranteed claims you can be removed from the list pres gious non accredited universi es hlp spamassassin apache org tests_ _ _x html
NLP_ch6,47,dan jurafsky summary naive bayes is not so naive very fast low storage requirements robust to irrelevant features irrelevant features cancel each other without affec ng results very good in domains with many equally important features decision trees suffer from fragmentagon in such cases especially if lille data op mal if the independence assump ons hold if assumed independence is correct then it is the bayes op mal classifier for problem a good dependable baseline for text classifica on but we will see other classifiers that give bewer accuracy
NLP_ch6,48,text classification and naïve bayes mul nomial naïve bayes a worked example
NLP_ch6,49,text classification and naïve bayes precision recall and the f measure
NLP_ch6,50,dan jurafsky the by con ngency table correct not correct selected tp fp not selected fn tn
NLP_ch6,51,dan jurafsky precision and recall precision of selected items that are correct recall of correct items that are selected correct not correct selected tp fp not selected fn tn
NLP_ch6,52,dan jurafsky a combined measure f a combined measure that assesses the p r tradeoff is f measure weighted harmonic mean pr f p r p r the harmonic mean is a very conserva ve average see iir people usually use balanced f measure i e with β that is α ½ f pr p r
NLP_ch6,53,text classification and naïve bayes precision recall and the f measure
NLP_ch6,54,text classification and naïve bayes text classifica on evalua on
NLP_ch6,55,sec dan jurafsky more than two classes sets of binary classifiers dealing with any of or mul value classifica on a document can belong to or classes for each class c c build a classifier γ to dis nguish c from all other classes c c c given test doc d evaluate it for membership in each class using each γ c d belongs to any class for which γ returns true c
NLP_ch6,56,sec dan jurafsky more than two classes sets of binary classifiers one of or mul nomial classifica on classes are mutually exclusive each document in exactly one class for each class c c build a classifier γ to dis nguish c from all other classes c c c given test doc d evaluate it for membership in each class using each γ c d belongs to the one class with maximum score
NLP_ch6,57,sec dan jurafsky evalua on classic reuters data set most over used data set docs each types toknens training test ar cles modapte lewis split categories an ar cle can be in more than one category learn binary category dis nc ons average document with at least one category has classes only about out of categories are large trade earn interest acquisitions common categories ship money fx train test wheat grain corn crude
NLP_ch6,58,sec dan jurafsky reuters text categoriza on data set reuters document reuters topics yes lewissplit train cgisplit training set oldid newid date mar date topics d livestock d d hog d topics title american pork congress kicks off tomorrow title dateline chicago march dateline body the american pork congress kicks off tomorrow march in indianapolis with of the nations pork producers from member states determining industry positions on a number of issues according to the national pork producers council nppc delegates to the three day congress will be considering resolutions concerning various issues including the future direction of farm policy and the tax law as it applies to the agriculture sector the delegates will also debate whether to endorse concepts of a national prv pseudorabies virus control and eradication program the nppc said a large trade show in conjunction with the congress will feature the latest in technology in all areas of the industry the nppc added reuter body text reuters
NLP_ch6,59,dan jurafsky confusion matrix c for each pair of classes c c how many documents from c were incorrectly assigned to c c wheat documents incorrectly assigned to poultry docs in test set assigned assigned assigned assigned assigned assigned uk poultry wheat coffee interest trade true uk true poultry true wheat true coffee true interest true trade
NLP_ch6,60,sec dan jurafsky per class evalua on measures c recall ii c frac on of docs in class i classified correctly ij j c precision ii c frac on of docs assigned class i that are ji actually about class i j c ii i accuracy error rate c ij frac on of docs classified correctly j i
NLP_ch6,61,sec dan jurafsky micro vs macro averaging if we have more than one class how do we combine mul ple performance measures into one quan ty macroaveraging compute performance for each class then average microaveraging collect decisions for all classes compute con ngency table evaluate
NLP_ch6,62,sec dan jurafsky micro vs macro averaging example class class micro ave table truth truth truth truth truth truth yes no yes no yes no classifier yes classifier yes classifier yes classifier no classifier no classifier no macroaveraged precision microaveraged precision microaveraged score is dominated by score on common classes
NLP_ch6,63,dan jurafsky development test sets and cross valida on training set development test set test set metric p r f or accuracy unseen test set training set dev test avoid overfi cid ng tuning to the test set more conserva ve es mate of performance training set dev test cross valida on over mul ple splits handle sampling errors from different datasets d e v t e s t training set pool results over each split compute pooled dev set performance test set
NLP_ch6,64,text classification and naïve bayes text classifica on evalua on
NLP_ch6,65,text classification and naïve bayes text classifica on prac cal issues
NLP_ch6,66,sec dan jurafsky the real world gee i m building a text classifier for real now what should i do
NLP_ch6,67,sec dan jurafsky no training data manually written rules if wheat or grain and not whole or bread then categorize as grain need careful cra cid ing human tuning on development data time consuming days per class
NLP_ch6,68,sec dan jurafsky very little data use naïve bayes naïve bayes is a high bias algorithm ng and jordan nips get more labeled data find clever ways to get humans to label data for you try semi supervised training methods bootstrapping em over unlabeled documents
NLP_ch6,69,sec dan jurafsky a reasonable amount of data perfect for all the clever classifiers svm regularized logis c regression you can even use user interpretable decision trees users like to hack management likes quick fixes
NLP_ch6,70,sec dan jurafsky a huge amount of data can achieve high accuracy at a cost svms train me or knn test me can be too slow regularized logis c regression can be somewhat beler so naïve bayes can come back into its own again
NLP_ch6,71,sec dan jurafsky accuracy as a function of data size with enough data classifier may not maler brill and banko on spelling correc on
NLP_ch6,72,dan jurafsky real world systems generally combine automa c classifica on manual review of uncertain difficult new cases
NLP_ch6,73,dan jurafsky underflow preven on log space mul plying lots of probabili es can result in floa ng point underflow since log xy log x log y beler to sum logs of probabili es instead of mul plying probabili es class with highest un normalized log probability score is s ll most probable c argmax log p c log p x c nb j i j c c j i positions model is now just max of sum of weights
NLP_ch6,74,sec dan jurafsky how to tweak performance domain specific features and weights very important in real performance some mes need to collapse terms part numbers chemical formulas but stemming generally doesn t help upweigh ng coun ng a word as if it occurred twice tle words cohen singer first sentence of each paragraph murata in sentences that contain tle words ko et al
NLP_ch6,75,text classification and naïve bayes text classifica on prac cal issues
NLP_ch7,1,sentiment analysis what is sen ment analysis
NLP_ch7,2,dan jurafsky posi ve or nega ve movie review unbelievably disappoin ng full of zany characters and richly applied sa re and some great plot twists this is the greatest screwball comedy ever filmed it was pathe c the worst part about it was the boxing scenes
NLP_ch7,3,dan jurafsky google product search a
NLP_ch7,4,dan jurafsky bing shopping a
NLP_ch7,5,twi er sen ment versus gallup poll of dan jurafsky consumer confidence brendan o connor ramnath balasubramanyan bryan r routledge and noah a smith from tweets to polls linking text sen ment to public opinion time series in icwsm
NLP_ch7,6,dan jurafsky twi er sen ment johan bollen huina mao xiaojun zeng twixer mood predicts the stock market journal of computa onal science j jocs
NLP_ch7,7,dan jurafsky senoj wod calm predicts djia days later at least one current hedge fund uses this algorithm mlac bollen et al
NLP_ch7,8,dan jurafsky target sen ment on twi er twixer sen ment app alec go richa bhayani lei huang twixer sen ment classifica on using distant supervision
NLP_ch7,9,dan jurafsky sen ment analysis has many other names opinion extrac on opinion mining sen ment mining subjec vity analysis
NLP_ch7,10,dan jurafsky why sen ment analysis movie is this review posi ve or nega ve products what do people think about the new iphone public sen ment how is consumer confidence is despair increasing poli cs what do people think about this candidate or issue predic on predict elec on outcomes or market trends from sen ment
NLP_ch7,11,dan jurafsky scherer typology of affec ve states emo on brief organically synchronized evalua on of a major event angry sad joyful fearful ashamed proud elated mood diffuse non caused low intensity long dura on change in subjec ve feeling cheerful gloomy irritable listless depressed buoyant interpersonal stances affec ve stance toward another person in a specific interac on friendly flirta ous distant cold warm suppor ve contemptuous agtudes enduring affec vely colored beliefs disposi ons towards objects or persons liking loving ha ng valuing desiring personality traits stable personality disposi ons and typical behavior tendencies nervous anxious reckless morose hos le jealous
NLP_ch7,12,dan jurafsky scherer typology of affec ve states emo on brief organically synchronized evalua on of a major event angry sad joyful fearful ashamed proud elated mood diffuse non caused low intensity long dura on change in subjec ve feeling cheerful gloomy irritable listless depressed buoyant interpersonal stances affec ve stance toward another person in a specific interac on friendly flirta ous distant cold warm suppor ve contemptuous agtudes enduring affec vely colored beliefs disposi ons towards objects or persons liking loving ha ng valuing desiring personality traits stable personality disposi ons and typical behavior tendencies nervous anxious reckless morose hos le jealous
NLP_ch7,13,dan jurafsky sen ment analysis sen ment analysis is the detec on of agtudes enduring affec vely colored beliefs disposi ons towards objects or persons holder source of aftude target aspect of aftude type of aftude from a set of types like love hate value desire etc or more commonly simple weighted polarity posi ve nega ve neutral together with strength text containing the aftude sentence or en re document
NLP_ch7,14,dan jurafsky sen ment analysis simplest task is the aftude of this text posi ve or nega ve more complex rank the aftude of this text from to advanced detect the target source or complex aftude types
NLP_ch7,15,dan jurafsky sen ment analysis simplest task is the aftude of this text posi ve or nega ve more complex rank the aftude of this text from to advanced detect the target source or complex aftude types
NLP_ch7,16,sentiment analysis what is sen ment analysis
NLP_ch7,17,sentiment analysis a baseline algorithm
NLP_ch7,18,dan jurafsky sentiment classification in movie reviews bo pang lillian lee and shivakumar vaithyanathan thumbs up sen ment classifica on using machine learning techniques emnlp bo pang and lillian lee a sen mental educa on sen ment analysis using subjec vity summariza on based on minimum cuts acl polarity detec on is an imdb movie review posi ve or nega ve data polarity data hxp www cs cornell edu people pabo movie review data
NLP_ch7,19,dan jurafsky imdb data in the pang and lee database when _star wars_ came out some twenty years snake eyes is the most aggrava ng ago the image of traveling throughout the stars kind of movie the kind that shows so has become a commonplace image much poten al then becomes unbelievably disappoin ng when han solo goes light speed the stars change to bright lines going towards the viewer in lines it s not just because this is a brian that converge at an invisible point depalma film and since he s a great director and one who s films are always cool greeted with at least some fanfare _october sky_ offers a much simpler image that of and it s not even because this was a film a single white dot traveling horizontally across the starring nicolas cage and since he gives a night sky brauvara performance this film is hardly worth his talents
NLP_ch7,20,dan jurafsky baseline algorithm adapted from pang and lee tokeniza on feature extrac on classifica on using different classifiers naïve bayes maxent svm
NLP_ch7,21,dan jurafsky sen ment tokeniza on issues deal with html and xml markup twixer mark up names hash tags capitaliza on preserve for poxs emo cons optional hat brow words in all caps eyes o optional nose phone numbers dates ddpp mouth reverse orientation emo cons ddpp mouth o optional nose eyes useful code optional hat brow christopher poxs sen ment tokenizer brendan o connor twixer tokenizer
NLP_ch7,22,dan jurafsky extrac ng features for sen ment classifica on how to handle nega on i didn t like this movie vs i really like this movie which words to use only adjec ves all words all words turns out to work bexer at least on this data
NLP_ch7,23,dan jurafsky nega on das sanjiv and mike chen yahoo for amazon extrac ng market sen ment from stock message boards in proceedings of the asia pacific finance associa on annual conference apfa bo pang lillian lee and shivakumar vaithyanathan thumbs up sentiment classification using machine learning techniques emnlp add not_ to every word between nega on and following punctua on didn t like this movie but i didn t not_like not_this not_movie but i
NLP_ch7,24,dan jurafsky reminder naïve bayes c argmax p c p w c nb j i j c c j i positions count w c ˆ p w c count c v
NLP_ch7,25,dan jurafsky binarized boolean feature mul nomial naïve bayes intui on for sen ment and probably for other text classifica on domains word occurrence may maxer more than word frequency the occurrence of the word fantas c tells us a lot the fact that it occurs mes may not tell us much more boolean mul nomial naïve bayes clips all the word counts in each document at
NLP_ch7,26,dan jurafsky boolean mul nomial naïve bayes learning from training corpus extract vocabulary calculate p c terms calculate p w c terms j k j for each c in c do treemxto ve dsuinpglilcea tdeos cin c eoanctha idnoinc g all docs j j j docs all docs with class c f o r f e o a r c e h a c w h o w rd o r w d t y in p e v w o c in a b d u o l c a r y j j j k retain only a single instance of w n of occurrences of w in text docs k k j p c j n j total documents p w c k k j n vocabulary
NLP_ch7,27,dan jurafsky boolean mul nomial naïve bayes on a test document d first remove all duplicate words from d then compute nb using the same equa on c argmax p c p w c nb j i j c c j i positions
NLP_ch7,28,dan jurafsky normal vs boolean mul nomial nb normal doc words class training chinese beijing chinese c chinese chinese shanghai c chinese macao c tokyo japan chinese j test chinese chinese chinese tokyo japan boolean doc words class training chinese beijing c chinese shanghai c chinese macao c tokyo japan chinese j test chinese tokyo japan
NLP_ch7,29,dan jurafsky binarized boolean feature mul nomial naïve bayes b pang l lee and s vaithyanathan thumbs up sen ment classifica on using machine learning techniques emnlp v metsis i androutsopoulos g paliouras spam filtering with naive bayes which naive bayes ceas third conference on email and an spam k m schneider on word frequency informa on and nega ve evidence in naive bayes text classifica on icanlp jd rennie l shih j teevan tackling the poor assump ons of naive bayes text classifiers icml binary seems to work bexer than full word counts this is not the same as mul variate bernoulli naïve bayes mbnb doesn t work well for sen ment or other text tasks other possibility log freq w
NLP_ch7,30,dan jurafsky cross valida on iteration break up data into folds test training equal posi ve and nega ve inside each fold test training for each fold choose the fold as a training test training temporary test set train on folds compute training test performance on the test fold report average training test performance of the runs
NLP_ch7,31,dan jurafsky other issues in classifica on maxent and svm tend to do bexer than naïve bayes
NLP_ch7,32,dan jurafsky problems what makes reviews hard to classify subtlety perfume review in perfumes the guide if you are reading this because it is your darling fragrance please wear it at home exclusively and tape the windows shut dorothy parker on katherine hepburn she runs the gamut of emo ons from a to b
NLP_ch7,33,dan jurafsky thwarted expecta ons and ordering effects this film should be brilliant it sounds like a great plot the actors are first grade and the suppor ng cast is good as well and stallone is axemp ng to deliver a good performance however it can t hold up well as usual keanu reeves is nothing special but surprisingly the very talented laurence fishbourne is not so good either i was surprised
NLP_ch7,34,sentiment analysis a baseline algorithm
NLP_ch7,35,sentiment analysis sen ment lexicons
NLP_ch7,36,dan jurafsky the general inquirer philip j stone dexter c dunphy marshall s smith daniel m ogilvie the general inquirer a computer approach to content analysis mit press home page hxp www wjh harvard edu inquirer list of categories hxp www wjh harvard edu inquirer homecat htm spreadsheet hxp www wjh harvard edu inquirer inquirerbasic xls categories posi v words and nega v words strong vs weak ac ve vs passive overstated versus understated pleasure pain virtue vice mo va on cogni ve orienta on etc free for research use
NLP_ch7,37,dan jurafsky liwc linguis c inquiry and word count pennebaker j w booth r j francis m e linguis c inquiry and word count liwc aus n tx home page hxp www liwc net words classes affec ve processes nega ve emo on bad weird hate problem tough posi ve emo on love nice sweet cogni ve processes tenta ve maybe perhaps guess inhibi on block constraint pronouns nega on no never quan fiers few many or fee
NLP_ch7,38,dan jurafsky mpqa subjec vity cues lexicon theresa wilson janyce wiebe and paul hoffmann recognizing contextual polarity in phrase level sentiment analysis proc of hlt emnlp riloff and wiebe learning extraction patterns for subjective expressions emnlp home page hxp www cs pix edu mpqa subj_lexicon html words from lemmas posi ve nega ve each word annotated for intensity strong weak gnu gpl
NLP_ch7,39,dan jurafsky bing liu opinion lexicon minqing hu and bing liu mining and summarizing customer reviews acm sigkdd bing liu s page on opinion mining hxp www cs uic edu liub fbs opinion lexicon english rar words posi ve nega ve
NLP_ch7,40,dan jurafsky sen wordnet stefano baccianella andrea esuli and fabrizio sebas ani sentiwordnet an enhanced lexical resource for sen ment analysis and opinion mining lrec home page hxp sen wordnet is cnr it all wordnet synsets automa cally annotated for degrees of posi vity nega vity and neutrality objec veness es mable j may be computed or es mated pos neg obj es mable j deserving of respect or high regard pos neg obj
NLP_ch7,41,dan jurafsky disagreements between polarity lexicons christopher poxs sen ment tutorial opinion general sen wordnet liwc lexicon inquirer mpqa opinion lexicon general inquirer sen wordnet liwc
NLP_ch7,42,dan jurafsky analyzing the polarity of each word in imdb poxs christopher on the nega vity of nega on salt how likely is each word to appear in each sen ment class count bad in star star star etc but can t use raw counts instead likelihood f w c p w c f w c w c make them comparable between words p w c scaled likelihood p w
NLP_ch7,43,dan jurafsky analyzing the polarity of each word in imdb pos good tokens amazing tokens great tokens awesome tokens l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l w c rp rating neg good tokens depress ed ing tokens bad tokens terrible tokens l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l w c rp rating doohilekil delacs doohilekil delacs w p c w p w p c w p poxs christopher on the nega vity of nega on salt
NLP_ch7,44,dan jurafsky other sen ment feature logical nega on poxs christopher on the nega vity of nega on salt is logical nega on no not associated with nega ve sen ment poxs experiment count nega on not n t no never in online reviews regress against the review ra ng
NLP_ch7,45,dan jurafsky po s results more nega on in nega ve sen ment a doohilekil delacs w p c w p
NLP_ch7,46,sentiment analysis sen ment lexicons
NLP_ch7,47,sentiment analysis learning sen ment lexicons
NLP_ch7,48,dan jurafsky semi supervised learning of lexicons use a small amount of informa on a few labeled examples a few hand built paxerns to bootstrap a lexicon
NLP_ch7,49,dan jurafsky hatzivassiloglou and mckeown intui on for iden fying word polarity vasileios hatzivassiloglou and kathleen r mckeown predic ng the seman c orienta on of adjec ves acl adjec ves conjoined by and have same polarity fair and legi mate corrupt and brutal fair and brutal corrupt and legi mate adjec ves conjoined by but do not fair but brutal
NLP_ch7,50,dan jurafsky hatzivassiloglou mckeown step label seed set of adjec ves all in million word wsj corpus posi ve adequate central clever famous intelligent remarkable reputed sensi ve slender thriving nega ve contagious drunken ignorant lanky listless primi ve strident troublesome unresolved unsuspec ng
NLP_ch7,51,dan jurafsky hatzivassiloglou mckeown step expand seed set to conjoined adjec ves nice helpful nice classy
NLP_ch7,52,dan jurafsky hatzivassiloglou mckeown step supervised classifier assigns polarity similarity to each word pair resul ng in graph brutal helpful irrational corrupt nice fair classy
NLP_ch7,53,dan jurafsky hatzivassiloglou mckeown step clustering for par oning the graph into two brutal helpful irrational corrupt nice fair classy
NLP_ch7,54,dan jurafsky output polarity lexicon posi ve bold decisive disturbing generous good honest important large mature pa ent peaceful posi ve proud sound s mula ng straigh cid orward strange talented vigorous wixy nega ve ambiguous cau ous cynical evasive harmful hypocri cal inefficient insecure irra onal irresponsible minor outspoken pleasant reckless risky selfish tedious unsupported vulnerable wasteful
NLP_ch7,55,dan jurafsky output polarity lexicon posi ve bold decisive disturbing generous good honest important large mature pa ent peaceful posi ve proud sound s mula ng straigh cid orward strange talented vigorous wixy nega ve ambiguous cau ous cynical evasive harmful hypocri cal inefficient insecure irra onal irresponsible minor outspoken pleasant reckless risky selfish tedious unsupported vulnerable wasteful
NLP_ch7,56,dan jurafsky turney algorithm turney thumbs up or thumbs down semantic orientation applied to unsupervised classification of reviews extract a phrasal lexicon from reviews learn polarity of each phrase rate a review by the average polarity of its phrases
NLP_ch7,57,dan jurafsky extract two word phrases with adjec ves first word second word third word not extracted jj nn or nns anything rb rbr rbs jj not nn nor nns jj jj not nn or nns nn or nns jj nor nn nor nns rb rbr or rbs vb vbd vbn vbg anything
NLP_ch7,58,dan jurafsky how to measure polarity of a phrase posi ve phrases co occur more with excellent nega ve phrases co occur more with poor but how to measure co occurrence
NLP_ch7,59,dan jurafsky pointwise mutual informa on mutual informa on between random variables x and y p x y i x y p x y log p x p y x y pointwise mutual informa on how much more do events x and y co occur than if they were independent p x y pmi x y log p x p y
NLP_ch7,60,dan jurafsky pointwise mutual informa on pointwise mutual informa on how much more do events x and y co occur than if they were independent p x y pmi x y log p x p y pmi between two words how much more do two words co occur than if they were independent p word word pmi word word log p word p word
NLP_ch7,61,dan jurafsky how to es mate pointwise mutual informa on query search engine altavista p word es mated by hits word n p word word by hits word near word n hits word near word pmi word word log hits word hits word
NLP_ch7,62,dan jurafsky does phrase appear more with poor or excellent polarity phrase pmi phrase excellent pmi phrase poor hits phrase near excellent hits phrase near poor log log hits phrase hits excellent hits phrase hits poor hits phrase near excellent hits phrase hits poor log hits phrase hits excellent hits phrase near poor hits phrase near excellent hits poor log hits phrase near poor hits excellent
NLP_ch7,63,dan jurafsky phrases from a thumbs up review phrase pos tags polarity online service jj nn online experience jj nn direct deposit jj nn local branch jj nn low fees jj nns true service jj nn other bank jj nn inconveniently located jj nn average
NLP_ch7,64,dan jurafsky phrases from a thumbs down review phrase pos tags polarity direct deposits jj nns online web jj nn very handy rb jj virtual monopoly jj nn lesser evil rbr jj other problems jj nns low funds jj nns unethical prac ces jj nns average
NLP_ch7,65,dan jurafsky results of turney algorithm reviews from epinions nega ve posi ve majority class baseline turney algorithm phrases rather than words learns domain specific informa on
NLP_ch7,66,dan jurafsky using wordnet to learn polarity s m kim and e hovy determining the sen ment of opinions coling m hu and b liu mining and summarizing customer reviews in proceedings of kdd wordnet online thesaurus covered in later lecture create posi ve good and nega ve seed words terrible find synonyms and antonyms posi ve set add synonyms of posi ve words well and antonyms of nega ve words nega ve set add synonyms of nega ve words awful and antonyms of posi ve words evil repeat following chains of synonyms filter
NLP_ch7,67,dan jurafsky summary on learning lexicons advantages can be domain specific can be more robust more words intui on start with a seed set of words good poor find other words that have similar polarity using and and but using words that occur nearby in the same document using wordnet synonyms and antonyms use seeds and semi supervised learning to induce lexicons
NLP_ch7,68,sentiment analysis learning sen ment lexicons
NLP_ch7,69,sentiment analysis other sen ment tasks
NLP_ch7,70,dan jurafsky finding sen ment of a sentence important for finding aspects or axributes target of sen ment the food was great but the service was awful
NLP_ch7,71,dan jurafsky finding aspect a ribute target of sen ment m hu and b liu mining and summarizing customer reviews in proceedings of kdd s blair goldensohn k hannan r mcdonald t neylon g reis and j reynar building a sen ment summarizer for local service reviews www workshop frequent phrases rules find all highly frequent phrases across reviews fish tacos filter by rules like occurs right a cid er sen ment word great fish tacos means fish tacos a likely aspect casino casino buffet pool resort beds children s barber haircut job experience kids greek restaurant food wine service appe zer lamb department store selec on department sales shop clothing
NLP_ch7,72,dan jurafsky finding aspect a ribute target of sen ment the aspect name may not be in the sentence for restaurants hotels aspects are well understood supervised classifica on hand label a small corpus of restaurant review sentences with aspect food décor service value none train a classifier to assign an aspect to asentence given this sentence is the aspect food décor service value or none
NLP_ch7,73,dan jurafsky pugng it all together finding sen ment for aspects s blair goldensohn k hannan r mcdonald t neylon g reis and j reynar building a sen ment summarizer for local service reviews www workshop sentences sentences sentences phrases phrases phrases final summary reviews text sentiment aspect aggregator extractor classifier extractor
NLP_ch7,74,dan jurafsky results of blair goldensohn et al method rooms stars comments the room was clean and everything worked fine even the water pressure we went because of the free room and was pleasantly pleased the worst hotel i had ever stayed at service stars comments upon checking out another couple was checking early due to a problem every single hotel staff member treated us great and answered every the food is cold and the service gives new meaning to slow dining stars comments our favorite place to stay in biloxi the food is great also the service offer of free buffet for joining the play
NLP_ch7,75,dan jurafsky baseline methods assume classes have equal frequencies if not balanced common in the real world can t use accuracies as an evalua on need to use f scores severe imbalancing also can degrade classifier performance two common solu ons resampling in training random undersampling cost sensi ve learning penalize svm more for misclassifica on of the rare thing
NLP_ch7,76,dan jurafsky how to deal with stars bo pang and lillian lee seeing stars exploiting class relationships for sentiment categorization with respect to rating scales acl map to binary use linear or ordinal regression or specialized models like metric labeling
NLP_ch7,77,dan jurafsky summary on sen ment generally modeled as classifica on or regression task predict a binary or ordinal label features nega on is important using all words in naïve bayes works well for some tasks finding subsets of words may help in other tasks hand built polarity lexicons use seeds and semi supervised learning to induce lexicons
NLP_ch7,78,dan jurafsky scherer typology of affec ve states emo on brief organically synchronized evalua on of a major event angry sad joyful fearful ashamed proud elated mood diffuse non caused low intensity long dura on change in subjec ve feeling cheerful gloomy irritable listless depressed buoyant interpersonal stances affec ve stance toward another person in a specific interac on friendly flirta ous distant cold warm suppor ve contemptuous agtudes enduring affec vely colored beliefs disposi ons towards objects or persons liking loving ha ng valuing desiring personality traits stable personality disposi ons and typical behavior tendencies nervous anxious reckless morose hos le jealous
NLP_ch7,79,dan jurafsky computa onal work on other affec ve states emo on detec ng annoyed callers to dialogue system detec ng confused frustrated versus confident students mood finding trauma zed or depressed writers interpersonal stances detec on of flirta on or friendliness in conversa ons personality traits detec on of extroverts
NLP_ch7,80,dan jurafsky detec on of friendliness ranganath jurafsky mcfarland friendly speakers use collabora ve conversa onal style laughter less use of nega ve emo onal words more sympathy that s too bad i m sorry to hear that more agreement i think so too less hedges kind of sort of a little
NLP_ch7,81,sentiment analysis other sen ment tasks
NLP_ch8,1,maxent models and discrimina ve es ma on genera ve vs discrimina ve models christopher manning
NLP_ch8,2,christopher manning introduc on so far we ve looked at genera ve models language models naive bayes but there is now much use of condi onal or discrimina ve probabilis c models in nlp speech ir and ml generally because they give high accuracy performance they make it easy to incorporate lots of linguis cally important features they allow automa c building of language independent retargetable nlp modules
NLP_ch8,3,christopher manning joint vs condi onal models we have some data d c of paired observa ons d and hidden classes c joint genera ve models place probabili es over p c d both observed data and the hidden stuff gene rate the observed data from hidden stuff all the classic statnlp models n gram models naive bayes classifiers hidden markov models probabilis c context free grammars ibm machine transla on alignment models
NLP_ch8,4,christopher manning joint vs condi onal models discrimina ve condi onal models take the data p c d as given and put a probability over hidden structure given the data logis c regression condi onal loglinear or maximum entropy models condi onal random fields also svms averaged perceptron etc are discrimina ve classifiers but not directly probabilis c
NLP_ch8,5,christopher manning bayes net graphical models bayes net diagrams draw circles for random variables and lines for direct dependencies some variables are observed some are hidden each node is a liyle classifier condi onal probability table based on incoming arcs c c d d d d d d naive bayes logis c regression genera ve discrimina ve
NLP_ch8,6,christopher manning condi onal vs joint likelihood a joint model gives probabili es p d c and tries to maximize this joint likelihood it turns out to be trivial to choose weights just rela ve frequencies a condi onal model gives probabili es p c d it takes the data as given and models only the condi onal probability of the class we seek to maximize condi onal likelihood harder to do as we ll see more closely related to classifica on error
NLP_ch8,7,christopher manning condi onal models work well word sense disambigua on training set even with exactly the same features changing from objective accuracy joint to condi onal joint like es ma on increases cond like performance test set that is we use the same objective accuracy smoothing and the same word class features we just joint like change the numbers cond like parameters klein and manning using senseval data
NLP_ch8,8,maxent models and discrimina ve es ma on genera ve vs discrimina ve models christopher manning
NLP_ch8,9,discrimina ve model features making features from text for discrimina ve nlp models christopher manning
NLP_ch8,10,christopher manning features in these slides and most maxent work features f are elementary pieces of evidence that link aspects of what we observe d with a category c that we want to predict a feature is a func on with a bounded real value f c d ℝ
NLP_ch8,11,christopher manning features in these slides and most maxent work features f are elementary pieces of evidence that link aspects of what we observe d with a category c that we want to predict a feature is a func on with a bounded real value
NLP_ch8,12,christopher manning example features f c d c location w in iscapitalized w f c d c location hasaccentedlatinchar w f c d c drug ends w c location location drug person in arcadia in québec taking zantac saw sue models will assign to each feature a weight a posi ve weight votes that this configura on is likely correct a nega ve weight votes that this configura on is likely incorrect
NLP_ch8,13,christopher manning example features f c d c location w in iscapitalized w f c d c location hasaccentedlatinchar w f c d c drug ends w c location location drug person in arcadia in québec taking zantac saw sue models will assign to each feature a weight a posi ve weight votes that this configura on is likely correct a nega ve weight votes that this configura on is likely incorrect
NLP_ch8,14,christopher manning feature expecta ons we will crucially make use of two expecta ons actual or predicted counts of a feature firing empirical count expecta on of a feature empirical e f f c d i i c d observed c d model expecta on of a feature e f p c d f c d i i c d c d
NLP_ch8,15,christopher manning features in nlp uses usually a feature specifies an indicator func on a yes no boolean matching func on of proper es of the input and a par cular class f c d φ d c c value is or i j they pick out a data subset and suggest a label for it we will say that φ d is a feature of the data d when for each c the conjunc on φ d c c is a feature of the data class j j pair c d
NLP_ch8,16,christopher manning features in nlp uses usually a feature specifies an indicator func on a yes no boolean matching func on of proper es of the input and a par cular class f c d φ d c c value is or i j each feature picks out a data subset and suggests a label for it
NLP_ch8,17,christopher manning feature based models the decision about a data point is based only on the features ac ve at that point data data data business stocks to restructure dt jj nn hit a yearly low bank money debt the previous fall label business label money label nn features features features stocks hit a w restructure w fall t jj yearly low w debt l w previous text word sense pos tagging categorization disambiguation
NLP_ch8,18,christopher manning example text categoriza on zhang and oles features are presence of each word in a document and the document class they do feature selec on to use reliable indicator words tests on classic reuters data set and others naïve bayes f linear regression logis c regression support vector machine paper emphasizes the importance of regulariza on smoothing for successful use of discrimina ve methods not used in much early nlp ir work
NLP_ch8,19,christopher manning other maxent classifier examples you can use a maxent classifier whenever you want to assign data points to one of a number of classes sentence boundary detec on mikheev is a period end of sentence or abbrevia on sen ment analysis pang and lee word unigrams bigrams pos counts pp ayachment ratnaparkhi ayach to verb or noun features of head noun preposi on etc parsing decisions in general ratnaparkhi johnson et al etc
NLP_ch8,20,discrimina ve model features making features from text for discrimina ve nlp models christopher manning
NLP_ch8,21,feature based linear classifiers how to put features into a classifier
NLP_ch8,22,christopher manning feature based linear classifiers linear classifiers at classifica on me linear function from feature sets f to classes c i assign a weight λ to each feature f i i we consider each class for an observed datum d for a pair c d features vote with their weights vote c σλf c d i i person location drug in québec in québec in québec choose the class c which maximizes σλf c d i i
NLP_ch8,23,christopher manning feature based linear classifiers linear classifiers at classifica on me linear function from feature sets f to classes c i assign a weight λ to each feature f i i we consider each class for an observed datum d for a pair c d features vote with their weights vote c σλf c d i i person location drug in québec in québec in québec choose the class c which maximizes σλf c d location i i
NLP_ch8,24,christopher manning feature based linear classifiers there are many ways to chose weights for features perceptron find a currently misclassified example and nudge weights in the direc on of its correct classifica on margin based methods support vector machines
NLP_ch8,25,christopher manning feature based linear classifiers exponen al log linear maxent logis c gibbs models make a probabilis c model from the linear combina on σλf c d i i exp f c d makes votes positive i i p c d i exp f c d normalizes votes i i c i p location in québec e e e e e e p drug in québec e e e e e p person in québec e e e e e the weights are the parameters of the probability model combined via a soft max function
NLP_ch8,26,christopher manning feature based linear classifiers exponen al log linear maxent logis c gibbs models given this model form we will choose parameters λ i that maximize the condi onal likelihood of the data according to this model we construct not only classifica ons but probability distribu ons over classifica ons there are other good ways of discrimina ng classes svms boos ng even perceptrons but these methods are not as trivial to interpret as distribu ons over classes
NLP_ch8,27,christopher manning aside logis c regression maxent models in nlp are essen ally the same as mul class logis c regression models in sta s cs or machine learning if you haven t seen these before don t worry this presenta on is self contained if you have seen these before you might think about the parameteriza on is slightly different in a way that is advantageous for nlp style models with tons of sparse features but sta s cally inelegant the key role of feature func ons in nlp and in this presenta on the features are more general with f also being a func on of the class when might this be useful
NLP_ch8,28,christopher manning quiz question assuming exactly the same set up class decision location person or drug features as before maxent what are p person by goéric p location by goéric p drug by goéric f c d c location w in iscapitalized w f c d c location hasaccentedlatinchar w f c d c drug ends w c exp f c d i i person location drug p c d i exp f c d by goéric by goéric by goéric i i c i
NLP_ch8,29,feature based linear classifiers how to put features into a classifier
NLP_ch8,30,building a maxent model the nuts and bolts
NLP_ch8,31,christopher manning building a maxent model we define features indicator func ons over data points features represent sets of data points which are dis nc ve enough to deserve model parameters words but also word contains number word ends with ing etc we will simply encode each φ feature as a unique string a datum will give rise to a set of strings the ac ve φ features each feature f c d φ d c c gets a real number weight i j we concentrate on φ features but the math uses i indices of f i
NLP_ch8,32,christopher manning building a maxent model features are oven added during model development to target errors oven the easiest thing to think of are features that mark bad combina ons then for any given feature weights we want to be able to calculate data condi onal likelihood deriva ve of the likelihood wrt each feature weight uses expecta ons of each feature according to the model we can then find the op mum feature weights discussed later
NLP_ch8,33,building a maxent model the nuts and bolts
NLP_ch8,34,naive bayes vs maxent models genera ve vs discrimina ve models the problem of overcoun ng evidence christopher manning
NLP_ch8,35,christopher manning text classifica on asia or europe europe training data asia monaco monaco monaco monaco hong monaco hong hong monaco monaco hong kong kong kong kong monaco nb factors predictions nb model p a p e p a m class p m a p e m p m e p a m x m p e m
NLP_ch8,36,christopher manning text classifica on asia or europe europe training data asia monaco monaco monaco monaco hong monaco hong hong monaco monaco hong kong kong kong kong monaco nb factors predictions nb model p a p e p a h k class p h a p k a p e h k p h e pk e p a h k x h x k p e h k
NLP_ch8,37,christopher manning text classifica on asia or europe europe training data asia monaco monaco monaco monaco hong monaco hong hong monaco monaco hong kong kong kong kong monaco nb factors predictions nb model p a p e p a h k m class p m a p e h k m p m e p a h k m p h a p k a h k m p h e pk e p e h k m
NLP_ch8,38,christopher manning naive bayes vs maxent models naive bayes models mul count correlated evidence each feature is mul plied in even when you have mul ple features telling you the same thing maximum entropy models preyy much solve this problem as we will see this is done by weigh ng features so that model expecta ons match the observed empirical expecta ons
NLP_ch8,39,naive bayes vs maxent models genera ve vs discrimina ve models the problem of overcoun ng evidence christopher manning
NLP_ch8,40,maxent models and discrimina ve es ma on maximizing the likelihood
NLP_ch8,41,christopher manning exponen al model likelihood maximum condi onal likelihood models given a model form choose values of parameters to maximize the condi onal likelihood of the data exp f c d i i i log p c d log p c d log exp f c d c d c d c d c d i i c i
NLP_ch8,42,christopher manning the likelihood value the log condi onal likelihood of iid data c d according to maxent model is a func on of the data and the parameters λ log p c d log p c d log p c d c d c d c d c d if there aren t many values of c it s easy to calculate exp f c d i i log p c d log i exp f c d c d c d i i c i
NLP_ch8,43,christopher manning the likelihood value we can separate this into two components log p c d logexp f c d log exp f c d i i i i c d c d i c d c d c i log p c d n m the deriva ve is the difference between the deriva ves of each component
NLP_ch8,44,christopher manning the deriva ve i numerator log exp f c d f c d ci i n i i c d c d i c d c d i i i i f c d i i i c d c d i f c d i c d c d deriva ve of the numerator is the empirical count f c i
NLP_ch8,45,christopher manning the deriva ve ii denominator log exp f c d i i m c d c d c i i i exp f c d i i c i exp f c d c d c d i i i c i exp f c d f c d i i i i i i exp f c d c d c d i i c i c i exp f c d f c d i i i i i i exp f c d c d c d c i i i c i p c d f c d i predicted count f λ i c d c d c
NLP_ch8,46,christopher manning the deriva ve iii log p c d actual count f c predicted count f i i i the op mum parameters are the ones for which each feature s predicted expecta on equals its empirical expecta on the op mum distribu on is always unique but parameters may not be unique always exists if feature counts are from actual data these models are also called maximum entropy models because we find the model having maximum entropy and sa sfying the constraints e f e f j p j p j
NLP_ch8,47,christopher manning finding the op mal parameters we want to choose parameters λ λ λ that maximize the condi onal log likelihood of the training data n cloglik d log p c d i i i to be able to do that we ve worked out how to calculate the func on value and its par al deriva ves its gradient
NLP_ch8,48,christopher manning a likelihood surface
NLP_ch8,49,christopher manning finding the op mal parameters use your favorite numerical op miza on package commonly and in our code you minimize the nega ve of cloglik gradient descent gd stochas c gradient descent sgd itera ve propor onal fi ng methods generalized itera ve scaling gis and improved itera ve scaling iis conjugate gradient cg perhaps with precondi oning quasi newton methods limited memory variable metric lmvm methods in par cular l bfgs
NLP_ch8,50,maxent models and discrimina ve es ma on maximizing the likelihood
NLP_ch9,1,informa on extrac on and named en ty recogni on introducing the tasks ge ng simple structured informa on out of text
NLP_ch9,2,christopher manning informa on extrac on informa on extrac on ie systems find and understand limited relevant parts of texts gather informa on from many pieces of text produce a structured representa on of relevant informa on rela ons in the database sense a k a a knowledge base goals organize informa on so that it is useful to people put informa on in a seman cally precise form that allows further inferences to be made by computer algorithms
NLP_ch9,3,christopher manning informa on extrac on ie ie systems extract clear factual informa on roughly who did what to whom when e g gathering earnings profits board members headquarters etc from company reports the headquarters of bhp billiton limited and the global headquarters of the combined bhp billiton group are located in melbourne australia headquarters bhp biliton limited melbourne australia learn drug gene product interac ons from medical research literature
NLP_ch9,4,christopher manning low level informa on extrac on is now available and i think popular in applica ons like apple or google mail and web indexing owen seems to be based on regular expressions and name lists
NLP_ch9,5,christopher manning low level informa on extrac on
NLP_ch9,6,christopher manning named en ty recogni on ner a very important sub task find and classify names in text for example the decision by the independent mp andrew wilkie to withdraw his support for the minority labor government sounded drama c but it should not further threaten its stability when awer the elec on wilkie rob oakeshoz tony windsor and the greens agreed to support labor they gave just two guarantees confidence and supply
NLP_ch9,7,christopher manning named en ty recogni on ner a very important sub task find and classify names in text for example the decision by the independent mp andrew wilkie to withdraw his support for the minority labor government sounded drama c but it should not further threaten its stability when awer the elec on wilkie rob oakeshoz tony windsor and the greens agreed to support labor they gave just two guarantees confidence and supply
NLP_ch9,8,christopher manning named en ty recogni on ner a very important sub task find and classify names in text for example the decision by the independent mp andrew wilkie to person withdraw his support for the minority labor government date sounded drama c but it should not further threaten its loca on stability when awer the elec on wilkie rob organi oakeshoz tony windsor and the greens agreed to support za on labor they gave just two guarantees confidence and supply
NLP_ch9,9,christopher manning named en ty recogni on ner the uses named en es can be indexed linked off etc sen ment can be azributed to companies or products a lot of ie rela ons are associa ons between named en es for ques on answering answers are owen named en es concretely many web pages tag various en es with links to bio or topic pages etc reuters opencalais evri alchemyapi yahoo s term extrac on apple google microsow smart recognizers for document content
NLP_ch9,10,informa on extrac on and named en ty recogni on introducing the tasks ge ng simple structured informa on out of text
NLP_ch9,11,evalua on of named en ty recogni on the extension of precision recall and the f measure to sequences
NLP_ch9,12,christopher manning the named en ty recogni on task task predict en es in a text foreign org ministry org spokesman o standard shen per evalua on is per en ty guofang per not per token told o reuters org
NLP_ch9,13,christopher manning precision recall f for ie ner recall and precision are straigheorward for tasks like ir and text categoriza on where there is only one grain size documents the measure behaves a bit funnily for ie ner when there are boundary errors which are common first bank of chicago announced earnings this counts as both a fp and a fn selec ng nothing would have been bezer some other metrics e g muc scorer give par al credit according to complex rules
NLP_ch9,14,evalua on of named en ty recogni on the extension of precision recall and the f measure to sequences
NLP_ch9,15,sequence models for named en ty recogni on
NLP_ch9,16,christopher manning the ml sequence model approach to ner training collect a set of representa ve training documents label each token for its en ty class or other o design feature extractors appropriate to the text and classes train a sequence classifier to predict the labels from the data tes ng receive a set of tes ng documents run sequence model inference to label each token appropriately output the recognized en es
NLP_ch9,17,christopher manning encoding classes for sequence labeling io encoding iob encoding fred per b per showed o o sue per b per mengqiu per b per huang per i per s o o new o o pain ng o o
NLP_ch9,18,christopher manning features for sequence labeling words current word essen ally like a learned dic onary previous next word context other kinds of inferred linguis c classifica on part of speech tags label context previous and perhaps next label
NLP_ch9,19,christopher manning features word substrings oxa field drug company cotrimoxazole wethersfield movie place alien fury countdown to invasion person
NLP_ch9,20,christopher manning features word shapes word shapes map words to simplified representation that encodes attributes such as length capitalization numerals greek letters internal punctuation etc varicella zoster xx xxx mrna xxxx cpa xxxd
NLP_ch9,21,sequence models for named en ty recogni on
NLP_ch9,22,maximum entropy sequence models maximum entropy markov models memms or condi onal markov models
NLP_ch9,23,christopher manning sequence problems many problems in nlp have data which is a sequence of characters words phrases lines or sentences we can think of our task as one of labeling each item vbg nn in dt nn in nn b b i i b i b i b b chasing opportunity in an age of upheaval 而 相 对 于 这 些 品 牌 的 价 pos tagging word segmenta on q pers o o o org org a text q a segmen murdoch discusses future of news corp a a ta on named en ty recogni on q a
NLP_ch9,24,christopher manning memm inference in systems for a condi onal markov model cmm a k a a maximum entropy markov model memm the classifier makes a single decision at a me condi oned on evidence from observa ons and previous decisions a larger space of sequences is usually explored via search features decision point local context w w w fell dt nnp vbd t vbd the dow fell t t nnp vbd hasdigit true ratnaparkhi toutanova et al etc
NLP_ch9,25,christopher manning example pos tagging scoring individual labeling decisions is no more complex than standard classifica on decisions we have some assumed labels to use for prior posi ons we use features of those and the observed data which can include current previous and next words to predict the current label features decision point local context w w w fell dt nnp vbd t vbd the dow fell t t nnp vbd hasdigit true ratnaparkhi toutanova et al etc
NLP_ch9,26,christopher manning example pos tagging pos tagging features can include current previous next words in isola on or together previous one two three tags word internal features word types suffixes dashes etc features decision point local context w w w fell dt nnp vbd t vbd the dow fell t t nnp vbd hasdigit true ratnaparkhi toutanova et al etc
NLP_ch9,27,christopher manning inference in systems sequence level sequence model inference sequence data local level classifier type label label local feature llooccaal l optimization extraction data ddaattaa features smoothing features maximum entropy conjugate quadratic models gradient penalties
NLP_ch9,28,christopher manning greedy inference sequence model best sequence inference greedy inference we just start at the left and use our classifier at each position to assign a label the classifier can depend on previous labeling decisions as well as observed data advantages fast no extra memory requirements very easy to implement with rich features including observations to the right it may perform quite well disadvantage greedy we make commit errors we cannot recover from
NLP_ch9,29,christopher manning beam inference sequence model best sequence inference beam inference k at each position keep the top complete sequences extend each sequence in each local way k the extensions compete for the slots at the next position advantages fast beam sizes of are almost as good as exact inference in many cases easy to implement no dynamic programming required disadvantage inexact the globally best sequence can fall off the beam
NLP_ch9,30,christopher manning viterbi inference sequence model best sequence inference viterbi inference dynamic programming or memoization requires small window of state influence e g past two states are relevant advantage exact the global best sequence is returned disadvantage harder to implement long distance state state interactions but beam inference tends not to allow long distance resurrection of sequences anyway
NLP_ch9,31,christopher manning crfs lafferty pereira and mccallum another sequence model conditional random fields crfs a whole sequence conditional model rather than a chaining of local models exp f c d i i p c d i exp f c d i i c i c the space of ʼs is now the space of sequences but if the features f remain local the conditional sequence likelihood can be calculated i exactly using dynamic programming training is slower but crfs avoid causal competition biases these or a variant using a max margin criterion are seen as the state of the art these days but in practice usually work much the same as memms
NLP_ch9,32,maximum entropy sequence models maximum entropy markov models memms or condi onal markov models
